---
title: "Lab 4: Topic Modeling in MOOC-Eds with Python"
author: "LASER Institute"
date: today 
format:
  html:
    toc: true
    toc-depth: 4
    toc-location: right
theme:
  light: simplex
  dark: cyborg
editor: visual
#bibliography: lit/references.bib
---

## 0. INTRODUCTION

This case study walkthrough extends previous research and evaluation work at the Friday Institute for Educational Innovation at North Carolina State University. In addition to many other areas of inquiry, this work was aimed at understanding and improving peer interaction and discussion in the Friday Institute's Massively Open Online Courses for Educators (MOOC-Ed) and Online Professional Learning programs. To learn more about these courses and programs, visit: <https://place.fi.ncsu.edu>

### Walkthrough Focus

Our focus will be on identifying "topics" by examining how words cohere into different latent, or hidden, themes based on patterns of co-occurrence of words within documents. With a bit of tongue-in-cheek, [Meeks and Weingart (2012)](http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/) describe topic modeling as: 

> *...focused on corpora and not individual texts, treating the works themselves as unceremonious 'buckets of words,' and providing seductive but obscure results in the forms of easily interpreted (and manipulated) 'topics'.... To achieve its results, it leverages occult statistical methods like 'dirichlet priors' and 'bayesian models.'*

That being said, [Weingart](http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/) also noted that "a topic model is a"clever and exceptionally versatile little algorithm that can be customized to all sorts of applications" and [Bail (2020)](https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html#running-your-first-topic-model) add that topic modeling can be "a powerful tool for identifying general trends in a corpus that can then be analyzed in a more granular manner using other techniques."

As noted by Krumm and Means (2018), the workflow for topic modelling is not always a linear process and there is often a great deal of iteration that occurs within and between wrangling, exploring, modeling. In this case study, we will primarily explore our data after the modeling process in order to gain some additional insight into the topics generated by our model. Specifically, this walkthrough covers the following concepts and skills:

1.  **Prepare**: Prior to analysis, we'll take a quick look at some of the related MOOC-Ed research and evaluation work to gain some context for our analysis. This should aid in the interpretation of our results and help guide some decisions as we tidy, model, and visualize our data.
2.  **Wrangle**: In section 2, we again preprocess text such as tokenizing, removing stop words and lemmatizing using the `nltk` library. Other than these, there are two more essential steps to preprocess the text for topic modelling, which are creating a dictionary by using `corpora.Dictionary` and a corpus by using `doc2bow()` function, both steps will use `gensim` library.
3.  **Model**: We take a look at the most popular topic modeling algorithms: Latent Dirichlet Allocation (LDA) and using `gensim` library to implement the model. We also use coherence score approach to find the optimal number of topics for the LDA model.
4.  **Explore**: To further explore the results of our topic model, we use `pyLDAvis` for visualizing and exploring the discovered topics such as finding the dominate topics for each document, finding the dominate words for each topic, and finding the dominate documents for each topic.

------------------------------------------------------------------------

## 1. PREPARE

To help us better understand the context, questions, and data sources we'll be using, this section will focus on the following topics:

a.  **Context**. As context for our analysis, we'll review several related papers by my colleagues relevant to the analysis of MOOC-Ed discussion forums.
b.  **Questions.** We'll also examine what insight topic modeling can provide to a question that we asked participants answer in their professional learning teams (PLTs).
c.  **Project Setup.** We will set up a "home' environment for the files and codes for lab 3 by installling and importing the required libraries for the topic modeling walkthrough.

### 1a. Context

#### Participating in a MOOC and Professional Learning Team: How a Blended Approach to Professional Development Makes a Difference

[![Teaching Statistics Through Data Investigations MOOC-Ed](img/tsdi.png "Our world is rich with data sources, and technology makes data more accessible than ever before! To help ensure students are future ready to use data for making informed decisions, many countries around the world have increased the emphasis on statistics and data analysis in school curriculum–from elementary/primary grades through college. This course allows you to learn, along with colleagues from other schools, an investigation cycle to teach statistics and to help students explore data to make evidence-based claims. To learn more about engaging learners in making inferences and claims supported by data and how to emphasize inferential reasoning in teaching statistics through posing different types of investigative questions, enroll in our Teaching Statistics through Inferential Reasoning MOOC-Ed.")](https://place.fi.ncsu.edu/local/catalog/course.php?id=4&ref=1)

Full text: <https://www.learntechlib.org/p/195234/>

**Abstract**

Massive Open Online Courses for Educators (MOOC-Eds) provide opportunities for using research-based learning and teaching practices, along with new technological tools and facilitation approaches for delivering quality online professional development. The Teaching Statistics Through Data Investigations MOOC-Ed was built for preparing teachers in pedagogy for teaching statistics, and it has been offered to participants from around the world. During 2016-2017, professional learning teams (PLTs) were formed from a subset of MOOC-Ed participants. These teams met several times to share and discuss their learning and experiences. This study focused on examining the ways that a blended approach to professional development may result in similar or different patterns of engagement to those who only participate in a large-scale online course. Results show the benefits of a blended learning environment for retention, engagement with course materials, and connectedness within the online community of learners in an online professional development on teaching statistics. The findings suggest the use of self-forming autonomous PLTs for supporting a deeper and more comprehensive experience with self-directed online professional developments such as MOOCs. Other online professional development courses, such as MOOCs, may benefit from purposely suggesting and advertising, and perhaps facilitating, the formation of small face-to-face or virtual PLTs who commit to engage in learning together.

**Data Source & Analysis**

All peer interaction, including peer discussion, take place within discussion forums of MOOC-Eds, which are hosted using the Moodle Learning Management System. To build the dataset you'll be using for this walkthrough, the research team wrote a query for Moodle's MySQL database, which records participants' user-logs of activity in the online forums. This sql query combines separate database tables containing postings and comments including participant IDs, timestamps, discussion text and other attributes or "metadata."

For further description of the forums and data retrieval process, see also the following papers:

-   Kellogg, S., & Edelmann, A. (2015). [Massively Open Online Course for Educators (MOOC‐Ed) network dataset](https://bera-journals.onlinelibrary.wiley.com/doi/pdfdirect/10.1111/bjet.12312). *British journal of educational technology*, *46*(5), 977-983.

-   Ezen-Can, A., Boyer, K. E., Kellogg, S., & Booth, S. (2015, March). [Unsupervised modeling for understanding MOOC discussion forums: a learning analytics approach](https://dl.acm.org/doi/pdf/10.1145/2723576.2723589). In *Proceedings of the fifth international conference on learning analytics and knowledge* (pp. 146-150).

-   Kellogg, S., Booth, S., & Oliver, K. (2014). [A social network perspective on peer supported learning in MOOCs for educators.](https://www.erudit.org/en/journals/irrodl/1900-v1-n1-irrodl04945/1065545ar.pdf) *International Review of Research in Open and Distributed Learning*, *15*(5), 263-289.

**Summary of Key Findings**

The following highlight some key findings related to the discussion forums in the papers cited above:

1.  MOOCs designed specifically for K-12 teachers can provide positive self-directed learning experiences and rich engagement in discussion forums that help form online communities for educators.
2.  Analysis of discussion forum data in TSDI provided a very clear picture of how enthusiastic many PLT members and leaders were to talk to others in the online community. They posed their questions and shared ideas with others about teaching statistics throughout the units, even though they were also meeting synchronously several times with their colleagues in small group PLTs.
3.  Findings on knowledge construction demonstrated that over half of the discussions in both courses moved beyond sharing information and statements of agreement and entered a process of dissonance, negotiation and co-construction of knowledge, but seldom moved beyond this phase in which new knowledge was tested or applied. These findings echo similar research on difficulties in promoting knowledge construction in online settings.
4.  Topic modeling provides more interpretable and cohesive models for discussion forums than other popular unsupervised modeling techniques such as k-means and k-medoids clustering algorithms.

### 1b. Guiding Questions

For the paper, [*Participating in a MOOC and Professional Learning Team: How a Blended Approach to Professional Development Makes a Difference*](https://www.learntechlib.org/p/195234/), the researchers were interested in unpacking how participants who enrolled in the Teaching Statistics through Data Investigations MOOC-Ed might benefit from also being in a smaller group of professionals committed to engaging in the same professional development. The specific research question for this paper was:

> What are the similarities and differences between how PLT members and Non-PLT online participants engage and meet course goals in a MOOC-Ed designed for educators in secondary and collegiate settings?

Dr. Hollylynne Lee and the TSDI team also developed a facilitation guide designed specifically for PLT teams to help groups synthesize the ideas in the course and make plans for how to implement new strategies in their classroom in order to impact students' learning of statistics. One question PLT members were asked to address was:

> What ideas or issues emerged in the discussion forums this past week?

For this walkthrough, we will further examine that question through the use of topic modeling.

One overarching question that Silge and Robinson (2018) identify as a central question to text mining and natural language processing, is:

> How do we to **quantify** what a document or collection of documents is about?

### 1c. Set Up

First, let's load the following libraries that we'll be needing for this walkthrough:

-   [`pandas`](https://pandas.pydata.org): a powerful and widely-used Python library for data manipulation and analysis. It provides data structures and functions needed to work seamlessly with structured data, such as tables and time series.

-   [`nltk`](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.nltk.org/&ved=2ahUKEwid7Pn-97-GAxWKMVkFHSOqBuUQFnoECB4QAQ&usg=AOvVaw1cuWsEY8ZUmW75nSDfXo7m): a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language. It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.

-   [`gensim`](https://pypi.org/project/gensim/): a Python library designed for natural language processing (NLP) tasks such as topic modeling, document indexing, and similarity retrieval, particularly with large text corpora. Gensim provides efficient implementations of popular topic modeling algorithms such as Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA).

-   [`matplotlib`](https://matplotlib.org): a comprehensive library for creating static, animated, and interactive visualizations in Python. It is widely used across various scientific and engineering disciplines to create high-quality graphs, charts, and plots.

-   [`seaborn`](https://seaborn.pydata.org): a Python data visualization library based on `matplotlib`. It provides a high-level interface for drawing attractive and informative statistical graphics.

-   [`pyLDAvis`](https://pypi.org/project/pyLDAvis/): a Python library used for interactive visualization of topic models. It is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.

```{python}
!pip install pandas nltk gensim matplotlib seaborn pyLDAvis
```

```{python}
import pandas as pd
import nltk
import gensim
import matplotlib
import seaborn as sns
import pyLDAvis
```

------------------------------------------------------------------------

## 2. WRANGLE

Data wrangling involves some combination of cleaning, reshaping, transforming, and merging data (Wickham & Grolemund, 2017). We'll preprocess the data as follows to get it ready for topic modelling.

a.  **Import Data**. We'll be working with .csv files and use `read_csv()` function to load data into our working environment.

b.  **Subset Columns and Rows**. We will filter the dataset and only includes the columns and rows of interests.

c.  **Prepare Data for LDA Modelling.**

    **Step 1: Tokenize Text and Remove Stop Words.** Tokenization and stop words removals are often the first steps in preparing the text data for further exploring or modelling, including the LDA topic modelling.

    **Step 2: Lemmatization.** It is another crucial step that helps in normalizing the text data, reducing words to their base or root form. In the case of topic modelling, it enhances the model's ability to identify the underlying topics.

    **Step 3: Create a Dictionary.** The dictionary assigns a unique integer ID to each unique word in the dataset. This ID is used to create a numerical representation of the text, which is important for text mining including LDA topic modelling, as they always operate on numerical representations of the text (e.g., the ID) rather than the raw text itself.

    **Step 4: Create a Corpus.** Build upon the dictionary, the corpus represents the text data with words count besides the IDs. In the form of a corpus, each document is represented as a list of tuples (word_id, word_count), which is necessary for LDA to perform its computations.

### 2a. Import Forum Data

To get started, we need to import, or "read", our data into Python. The function used to import your data will depend on the file format of the data you are trying to import. Let's read our data into our Environment using the `read_csv()` function and assign it to a variable name so we can work with it like any other object in Python.

```{python}
ts_forum_data = pd.read_csv("data/ts_forum_data.csv")
```

To get an overview, we can display the ts_forum_data data frame. You can find that the data frame includes course information (e.g., course_id, course_name), discussion forum information (e.g., forum_id, discussion_id), and more importantly, the post information (e.g., post_id, post_title, post_content).

```{python}
ts_forum_data
```

### 2b. Subset Columns and Rows

The post_content will be the text from which we will discover the topics, and post_id as the identifier could be useful, so those two columns will be the columns of interests, so let's subset column and create a **post_content** data frame. Also, the rows with missing values will not contribute to the topics, so let's drop it out.

```{python}
# Select 'post_id' and 'post_content' columns
post_content = ts_forum_data[['post_id', 'post_content']]

# Rename the columns to 'id' and 'document'
post_content.columns = ['id', 'document']

# Drop rows with missing values in the 'document' column
post_content.dropna(subset=['document'], inplace=True)

# Display the resulting DataFrame
print(post_content)
```

### 2c. Prepare Data for LDA Model

In this session, we will processing text and transforming our data frames into new data structures required for topic modeling, in the case of using `gensim` for LDA topic modelling, the needed data structure is the bag of word (BoW) format which will be the **corpus** data frame that we will end with after this session.

#### Step 1: Tokenize Text & Remove Stop Words

We are already familiar with the tokization process from the last two labs. Tokenization and stop words removals are always the first step for most Natural Language Processing (NLP) models, including LDA. It involves splitting the text into individual words or tokens and remove nonsense ones, which are the basic units for analysis. Let's tokenize and remove stop wordsour forum text and by using the familiar `tokenize()` and `stopwords`:

```{python}
from nltk.tokenize import word_tokenize
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords

# Ensure nltk punkt tokenizer and stopwords are downloaded
nltk.download('punkt')
nltk.download('stopwords')

# Create a tokenizer instance with the regular expression of tweets
tweet_pattern = r'\b\w+\b'
tokenizer = RegexpTokenizer(tweet_pattern)

# Specify stop words
stop_words = set(stopwords.words('english'))

# Tokenize the 'document' column, remove stop words, and create a new column 'word'
post_content['word'] = post_content['document'].apply(
    lambda x: [token.lower() for token in tokenizer.tokenize(str(x)) if token.lower() not in stop_words]
)

# Explode the 'word' column to transform each row into individual words
post_content_exploded = post_content.explode('word')

# Display the resulting DataFrame
print(post_content_exploded)

# Explode the 'word' column to transform each row into individual words
post_content_exploded = post_content.explode('word')

# Display the resulting DataFrame
print(post_content_exploded)
```

Now let's do a quick word count to see some of the most common words used throughout the forums.

```{python}
# Count the occurrences of each word
words_count = post_content_exploded['word'].value_counts().reset_index()
words_count.columns = ['word', 'count']

# Print the word count of the top 20 words
print("Top 20 words and their counts:")
print(words_count.head(20))
```

Terms like "students," "data," and "class" are about what we would have expected from a course teaching statistics. The term "agree" and "time" however, are not so intuitive and worth a quick look as well.

##### Comprehension Check

Use the `str.contains()` function to filter for rows in our `post_content` data frame that contain the term "time" and another term or terms of your choosing. After filtering, select a random sample of 10 posts using the `sample()` function for your terms and answer the following questions:

1.  What, if anything, do these posts have in common?

2.  What topics or themes might be apparent, or do you anticipate emerging, from our topic modeling?

Your output should look something like this:

```{python}
# Filter to get rows where text contains the word 'time'
post_with_time = post_content[post_content['document'].str.contains('time')]

# Select a random sample of 10 rows
time = post_with_time.sample(n=10, random_state=42)

# Print the random sample
print(time)
```

#### Step 2: Lemmatize

Lemmatization is a crucial step in preparing our text data for topic modeling. It reduces words to their base or root form. For example, the lemma of the words "running", "runs", and "ran" is "run". By grouping similar words under one token, it will make the analysis more efficient, and improve the coherence of the topics identified by the model.

To lemmatize the word, we will introduce the `WordNetLemmatizer` class. One important notice here, the lemmatizer expects input that is composed of characters (strings) and can be interpreted as words. If you pass non-string data, such as floats, to the lemmatizer, it may encounter errors because it doesn't know how to interpret these data types as words.

If there is a post content like "A class as large as that 300 is impossibly...", after tokenizing, the '300' will be one of the tokens. Surrounded by quotation marks, the '300' is still string, while after exploding, the 300 will be recognized as integer data type by Python, it is no longer the string.

Therefore, before applying lemmatization or any other text processing technique, it's important to ensure that your data consists of strings.

```{python}
# Convert float values to strings in the 'word' column
post_content_exploded['word'] = post_content_exploded['word'].astype(str)
```

Now we can lemmatize using `WordNetLemmatizer` from `nltk` library.

```{python}
from nltk.stem import WordNetLemmatizer

# Ensure NLTK resources are downloaded
nltk.download('wordnet')

# Initialize the WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

# Apply lemmatization to the 'word' column
post_content_exploded['lemmatized_word'] = post_content_exploded['word'].apply(lambda x: lemmatizer.lemmatize(x))

# Display the resulting DataFrame
print(post_content_exploded)
```

#### Step 3: **Create a Dictionary**

The dictionary maps each word in our dataset to a unique integer ID. This is important because LDA modeling operates on numerical representations of the text (e.g., the ID) rather than the raw text itself. And the dictionary helps to track the entire vocabulary, making it easier to manage and manipulate the text data.

To create the dictionary, we will use the `corpora.Dictionary` from `gensim` library. Notice that the input of `corpora.Dictionary` should be the lists of token list for each document (in our case, the document refers to the post). So let's convert the lemmatized words back to lists for each post, then create a dictionary.

```{python}
from gensim import corpora

# Convert the lemmatized words back to lists for each document
lemmatized_documents = post_content_exploded.groupby('document')['lemmatized_word'].apply(list).tolist()

# Create a Dictionary
dictionary = corpora.Dictionary(lemmatized_documents)

# Print the dictionary
# print(dictionary.token2id)  # This will print the token to id mapping
```

#### Step 4: Create a Corpus

Now we get a dictionary by mapping the words with IDs as one of the numerical representation of text data. The word counts is another important numerical representation which is necessary for LDA to perform its computations.

The corpus combines the IDs and word count together to represent the text data, which is a list of documents, where each document is represented as a list of tuples (word_id, word_count). This format is known as the bag-of-words (BoW) format.

**Bag-of-Words (BoW)**: The BoW simplifies the text data by representing the number of occurrences of each word in the document without considering the word order. This simplification is suitable for topic modeling and lots of other text mining tasks.

To create a corpus, we will apply the `doc2bow()` function on the dictionary that we just created.

```{python}
# Create a corpus: BOW representation of each document
corpus = [dictionary.doc2bow(doc) for doc in lemmatized_documents]

# Print the corpus
# print(corpus) # This will print the id to count mapping
```

LDA requires the input data in the form of a corpus, where each document is represented as a list of (word_id, word_count) tuples. This input format allows LDA to calculate the distribution of topics in each document and the distribution of words in each topic. As we already get the corpus, we are ready to perform the LDA modelling.

------------------------------------------------------------------------

## 3. MODEL

In very simple terms, modeling involves developing a mathematical summary of a dataset. These summaries can help us further explore trends and patterns in our data.

In their book, *Learning Analytics Goes to School,* Krumm and Means (2018) describe two general types of modeling approaches used in the Data-Intensive Research workflow: unsupervised and supervised machine learning. In distinguishing between the two, they note:

> *Unsupervised learning algorithms can be used to understand the structure of one's dataset. Supervised models, on the other hand, help to quantify relationships between features and a known outcome. Known outcomes are also commonly referred to as labels or dependent variables.*

In Section 3 we focus on Topic Modeling, an unsupervised learning approach to automatically identify topics in a collection of documents. In fact, we'll explore the most popular approach to topic modeling - LDA, as well as strategies for identifying the "right" number of topics:

a.  **Fitting a Topic Modeling with LDA**. In this section we learn to use the `LdaModel` function for unsupervised classification of our forum discussions to find natural groupings of words, or topics.
b.  **Choosing K.** Finally, we wrap up Section 3 by learning about diagnostic properties like exclusivity, semantic coherence, and held out likelihood for helping to select an appropriate number of topics.

### 3a. Fitting a Topic Modeling with LDA

Before running our first topic model using the `LdaModel` function, let's learn some basic principles behind LDA and why LDA is of preferred over other automatic classification or clustering approaches.

Unlike simple forms of cluster analysis such as k-means clustering, LDA is a **"mixture" model**, which in our context means that:

1.  **Every [document]{.underline} contains a mixture of topics.** Unlike algorithms like k-means, LDA treats each document as a mixture of topics, which allows documents to "overlap" each other in terms of content, rather than being separated into discrete groups. So in practice, this means that a discussion forum post could have an estimated topic proportion of 70% for Topic 1 (e.g. be mostly about a Topic 1), but also be partly about Topic 2.
2.  **Every [topic]{.underline} contains a mixture of words.** For example, if we specified in our LDA model just 2 topics for our discussion posts, we might find that one topic seems to be about pedagogy while another is about learning. The most common words in the pedagogy topic might be "teacher", "strategies", and "instruction", while the learning topic may be made up of words like "understanding" and "students". However, words can be shared between topics and words like "statistics" or "assessment" might appear in both equally.

Similar to k-means other other simple clustering approaches, however, LDA does require us to specify a value of *k* for the number of topics in our corpus. Selecting *k* is no trivial matter and can greatly impact your results.

Since so far we don't have a have strong rationale about the number of topics that might exist in discussion forums, let's use the `nunique()` function from the `pandas` library to find the number of unique forum names in our course data and run with that:

```{python}
# Find the number of unique values
unique_forum_count = ts_forum_data['forum_name'].nunique()

# Print count of unique values
print(unique_forum_count)
```

Since it looks like there are 22 distinct discussion forums, we'll use that as our value for the `num_topics =` argument of the `LdaModel`. Be patient while this runs, since the default setting of is to perform a large number of iterations.

```{python}
from gensim.models.ldamodel import LdaModel

# Build the LDA model
lda_model = LdaModel(corpus, num_topics=22, id2word=dictionary, passes=15)

# Print the topics
for idx, topic in lda_model.print_topics(-1):
    print(f"Topic: {idx} \nWords: {topic}")
```

Notice that the `passes` parameter specifies the number of times the entire corpus will be used to update the LDA model's parameters and improve the quality of the topic distributions during training. This is part of the process known as "epochs" in other machine learning contexts. If not specified, the default value for `passes` is typically 1, here we choose 15. Increasing the number of passes can lead to a more accurate and stable model because the model has more opportunities to refine its parameters. However, more passes also mean longer training times.

### 3b. Finding optimal K - num_topics

As alluded to earlier, selecting the number of topics for your model is a non-trivial decision and can dramatically impact your results. Bail (2018) notes that

> *The results of topic models should not be over-interpreted unless the researcher has strong theoretical apriori about the number of topics in a given corpus, or if the researcher has carefully validated the results of a topic model using both the quantitative and qualitative techniques described above.*

There are several approaches to estimating a value for K and we'll take a quick look at coherence score approach. The coherence score measures the degree of semantic similarity between high scoring words in the topic. The idea is to find a number of topics that maximizes the coherence score. The `gensim` library provides `CoherenceModel` that we can use to compute the coherence scores.

#### **Coherence Score Approach**

```{python}
# from gensim.models import CoherenceModel
# 
# def compute_coherence_values(dictionary, corpus, texts, start=2, limit=40, step=6):
#     coherence_values = []
#     model_list = []
#     for num_topics in range(start, limit, step):
#         model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=15)
#         model_list.append(model)
#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
#         coherence_values.append(coherencemodel.get_coherence())
#     return model_list, coherence_values
# 
# # Compute coherence scores
# model_list, coherence_values = compute_coherence_values(dictionary, corpus, post_content_exploded['lemmatized_word'], start=2, limit=40, step=2)
# 
# # Plot coherence scores
# x = range(2, 40, 2)
# plt.plot(x, coherence_values)
# plt.xlabel("Num Topics")
# plt.ylabel("Coherence score")
# plt.legend("coherence_values", loc='best')
# plt.show()
```

Notice that the `CoherenceModel` will take a while to run. The output looks like this:

![](img/coherencemodel-output.png)

As a general rule of thumb and overly simplistic heuristic, we're looking for an inflection point in our plot which indicates an optimal number of topics to select for a value of K. Let's select the model with highest coherence value, and print out the topics identified the optimal model.

```{python}
# # Select the model with highest coherence value
# optimal_model = model_list[coherence_values.index(max(coherence_values))]
# optimal_num_topics = x[coherence_values.index(max(coherence_values))]
# print(f"Optimal number of topics: {optimal_num_topics}")
# 
# # Print the topics of the optimal model
# topics = optimal_model.print_topics(num_words=10)
# for topic in topics:
#     print(topic)
```

The optimal number of topics is 4. In the next session, we will explore the lda model with 4 topics.

```{python}
from gensim.models.ldamodel import LdaModel

# Build the optimal LDA model
optimal_model = LdaModel(corpus, num_topics=4, id2word=dictionary, passes=15)

# Print the topics
for idx, topic in lda_model.print_topics(-1):
    print(f"Topic: {idx} \nWords: {topic}")
```

## 4. EXPLORE

Silge and Robinson (2018) note that fitting at topic model is the "easy part." The hard part is making sense of the model results and that the rest of the analysis involves exploring and interpreting the model using a variety of approaches which we'll walkthrough in in this section.

Bail (2018) cautions, however, that:

> *...post-hoc interpretation of topic models is rather dangerous... and can quickly come to resemble the process of "reading tea leaves," or finding meaning in patterns that are in fact quite arbitrary or even random.*

### 4a. Visualizing Topics by pyLDAvis

Visualization is always a good way to explore the modelling results. The `pyLDAvis` is primarily designed for visualizing the results of LDA topic modeling, it provides an interactive dashboard that helps users interpret the topics discovered by LDA.

Let's use to `pyLDAvis` to create one.

```{python}
import pyLDAvis
import pyLDAvis.gensim

# Prepare the visualization
vis = pyLDAvis.gensim.prepare(optimal_model, corpus, dictionary)

# Display the visualization
pyLDAvis.display(vis)
```

### 4b. Finding the Dominant Topic for Each Document

One of the practical applications of topic modeling is to determine the main topic of a given document. Let's explore it by using `get_document_topics` function from `gensim` and visualize the first 10 document with its dominant topics:

```{python}
import matplotlib.pyplot as plt

# Get the topic distribution for the first 10 documents
topic_dist_first_10 = [optimal_model.get_document_topics(doc, minimum_probability=0.0) for doc in corpus[:10]]

# Iterate over the first 10 documents
for i, doc_topic_dist in enumerate(topic_dist_first_10):
    # Extract topic ids and their probabilities
    doc_topics, doc_probs = zip(*doc_topic_dist)
    
    # Plot the topic probabilities for the document
    plt.figure(figsize=(10, 6))
    plt.bar(doc_topics, doc_probs, color='skyblue')
    plt.xlabel('Topic ID')
    plt.ylabel('Probability')
    plt.title(f'Topic Distribution for Post {i+1}')
    plt.xticks(doc_topics)
    plt.show()
```

### 4c. Finding the Dominant Words for Each Topic

By merely knowing the dominant topic ID for a document from last session, obviously, it's not very easy to interpret what the topics exactly are, so let's examine the top 5 terms for each topic and then look at this information visually:

```{python}
# Extract the top 5 terms for each topic from the LDA model
topics = optimal_model.show_topics(formatted=False, num_words=5)
topics_dict = {topic_num: [(word, round(weight, 4)) for word, weight in words] for topic_num, words in topics}

# Convert the dictionary to a DataFrame for easier visualization
topics_df = pd.DataFrame(topics_dict).T
topics_df.columns = [f'Term {i+1}' for i in range(5)]
topics_df['Topic'] = topics_df.index
topics_df = topics_df.melt(id_vars='Topic', var_name='Term Rank', value_name='Term and Weight')

# Split 'Term and Weight' column into separate 'Term' and 'Weight' columns
topics_df[['Term', 'Weight']] = pd.DataFrame(topics_df['Term and Weight'].tolist(), index=topics_df.index)

# Drop the 'Term and Weight' column as it's no longer needed
topics_df.drop(columns=['Term and Weight'], inplace=True)

# Display the DataFrame
print(topics_df.head(10))
```

From the data frame generated, we can visualize now:

```{python}
import seaborn as sns

# Create the FacetGrid
g = sns.FacetGrid(topics_df, col="Topic", col_wrap=4, sharex=False, sharey=False, height=4)

# Map the bar plot to each subplot
g.map_dataframe(sns.barplot, x='Weight', y='Term', palette='husl', orient='h')

# Customize the plot
g.set_titles(col_template='Topic {col_name}')
g.set_axis_labels("Weight", "Term")

plt.tight_layout()
plt.show()
```

### 4d. Finding the Dominate Documents for Each Topic

Now that we have a sense of the most common words associated with each topic, let's further explore the dominate documents for each topic:

```{python}
# Initialize a list to store topic distributions for each document
topic_distributions = [optimal_model.get_document_topics(doc, minimum_probability=0.0) for doc in corpus]

# Identify the dominant topics for each document
dominant_topics = [(sorted(topic_distribution, key=lambda x: x[1], reverse=True)[0], i)
                   for i, topic_distribution in enumerate(topic_distributions)]

# Create DataFrame from the list of dominant topics
df_dominant_topic = pd.DataFrame(dominant_topics, columns=['Dominant_Topic', 'Document_No'])
df_dominant_topic['Topic_ID'] = df_dominant_topic['Dominant_Topic'].apply(lambda x: x[0])
df_dominant_topic['Topic_Probability'] = df_dominant_topic['Dominant_Topic'].apply(lambda x: x[1])

# Ensure the original DataFrame has a unique identifier for each document
post_content_exploded['Document_No'] = post_content_exploded.index

# Merge with the original DataFrame to get the text of the dominant documents
df_dominant_topic = df_dominant_topic.merge(post_content_exploded[['Document_No', 'document']], on='Document_No', how='left')

# Create a function to get the top N unique documents for each topic
def get_top_n_documents_grouped(df, n=3):
    # Sort by Topic_Probability within each group and take the top N unique documents
    top_n_docs = df.groupby('Topic_ID').apply(lambda group: group.nlargest(n, 'Topic_Probability').drop_duplicates(subset='document'))
    return top_n_docs

# Get the top 3 unique documents for each topic
top_n_docs_unique = get_top_n_documents_grouped(df_dominant_topic, n=3)

# Display the first three dominant documents for each topic
for topic in top_n_docs_unique['Topic_ID'].unique():
    print(f"Topic {topic}:")
    top_docs = top_n_docs_unique[top_n_docs_unique['Topic_ID'] == topic].nlargest(3, 'Topic_Probability')
    for _, row in top_docs.iterrows():
        print(f"Document No: {row['Document_No']} - Probability: {row['Topic_Probability']}")
        print(row['document'])
        print()
```

#### Takeaway

In addition to some useful Python packages and functions for the actual process of topic modeling, hopefully there are two main lessons I'm hoping you take away from this walkthrough:

1.  **Topic modeling requires a lot of decisions.** Beyond deciding on a value for K, there are a number of key decisions that you have to make that can dramatically affect your results. For example, to stem or not to stem? What qualifies as a document? What flavor or topic modeling is best suited to your data and research questions? How many iterations to run?
2.  **Topic modeling is as much art as (data) science.** As Bail (2018) noted, the term "topic" is somewhat ambitious, and topic models do not produce highly nuanced classification of texts. Once you've fit your model, interpreting your model requires some mental gymnastics and ideally some knowledge of the context from which the data came to help with interpretation of your topics. Moreover, the quantitative approaches for making the decisions highlighted above are imperfect and a good deal of human judgment required.

##### Comprehension Check

Using the LDA model with a different value for K, use the approaches demonstrated in Section 4 to explore and interpret your topics and terms and revisit the following question:

1.  Now that you have a little more context, how might you revise your initial interpretation of some of the latent topics or latent themes from your model?
