<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dr.&nbsp;Shiyan Jiang">

<title>Lab 4: Topic Modeling in MOOC-Eds with Python</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="lab-4-case-study-python_files/libs/clipboard/clipboard.min.js"></script>
<script src="lab-4-case-study-python_files/libs/quarto-html/quarto.js"></script>
<script src="lab-4-case-study-python_files/libs/quarto-html/popper.min.js"></script>
<script src="lab-4-case-study-python_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="lab-4-case-study-python_files/libs/quarto-html/anchor.min.js"></script>
<link href="lab-4-case-study-python_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="lab-4-case-study-python_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="lab-4-case-study-python_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="lab-4-case-study-python_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="lab-4-case-study-python_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> INTRODUCTION</a>
  <ul class="collapse">
  <li><a href="#walkthrough-focus" id="toc-walkthrough-focus" class="nav-link" data-scroll-target="#walkthrough-focus"><span class="header-section-number">1.1</span> Walkthrough Focus</a></li>
  </ul></li>
  <li><a href="#prepare" id="toc-prepare" class="nav-link" data-scroll-target="#prepare"><span class="header-section-number">2</span> PREPARE</a>
  <ul class="collapse">
  <li><a href="#a.-context" id="toc-a.-context" class="nav-link" data-scroll-target="#a.-context"><span class="header-section-number">2.1</span> a. Context</a></li>
  <li><a href="#b.-guiding-questions" id="toc-b.-guiding-questions" class="nav-link" data-scroll-target="#b.-guiding-questions"><span class="header-section-number">2.2</span> b. Guiding Questions</a></li>
  <li><a href="#c.-set-up" id="toc-c.-set-up" class="nav-link" data-scroll-target="#c.-set-up"><span class="header-section-number">2.3</span> c.&nbsp;Set Up</a></li>
  </ul></li>
  <li><a href="#wrangle" id="toc-wrangle" class="nav-link" data-scroll-target="#wrangle"><span class="header-section-number">3</span> WRANGLE</a>
  <ul class="collapse">
  <li><a href="#a.-import-forum-data" id="toc-a.-import-forum-data" class="nav-link" data-scroll-target="#a.-import-forum-data"><span class="header-section-number">3.1</span> a. Import Forum Data</a></li>
  <li><a href="#b.-subset-columns-and-rows" id="toc-b.-subset-columns-and-rows" class="nav-link" data-scroll-target="#b.-subset-columns-and-rows"><span class="header-section-number">3.2</span> b. Subset Columns and Rows</a></li>
  <li><a href="#c.-prepare-data-for-lda-model" id="toc-c.-prepare-data-for-lda-model" class="nav-link" data-scroll-target="#c.-prepare-data-for-lda-model"><span class="header-section-number">3.3</span> c.&nbsp;Prepare Data for LDA Model</a></li>
  </ul></li>
  <li><a href="#model" id="toc-model" class="nav-link" data-scroll-target="#model"><span class="header-section-number">4</span> MODEL</a>
  <ul class="collapse">
  <li><a href="#a.-fitting-a-topic-modeling-with-lda" id="toc-a.-fitting-a-topic-modeling-with-lda" class="nav-link" data-scroll-target="#a.-fitting-a-topic-modeling-with-lda"><span class="header-section-number">4.1</span> a. Fitting a Topic Modeling with LDA</a></li>
  <li><a href="#b.-finding-optimal-k---num_topics" id="toc-b.-finding-optimal-k---num_topics" class="nav-link" data-scroll-target="#b.-finding-optimal-k---num_topics"><span class="header-section-number">4.2</span> b. Finding optimal K - num_topics</a></li>
  </ul></li>
  <li><a href="#explore" id="toc-explore" class="nav-link" data-scroll-target="#explore"><span class="header-section-number">5</span> EXPLORE</a>
  <ul class="collapse">
  <li><a href="#a.-visualizing-topics-by-pyldavis" id="toc-a.-visualizing-topics-by-pyldavis" class="nav-link" data-scroll-target="#a.-visualizing-topics-by-pyldavis"><span class="header-section-number">5.1</span> a. Visualizing Topics by pyLDAvis</a></li>
  <li><a href="#b.-finding-the-dominant-topic-for-each-document" id="toc-b.-finding-the-dominant-topic-for-each-document" class="nav-link" data-scroll-target="#b.-finding-the-dominant-topic-for-each-document"><span class="header-section-number">5.2</span> b. Finding the Dominant Topic for Each Document</a></li>
  <li><a href="#c.-finding-the-dominant-words-for-each-topic" id="toc-c.-finding-the-dominant-words-for-each-topic" class="nav-link" data-scroll-target="#c.-finding-the-dominant-words-for-each-topic"><span class="header-section-number">5.3</span> c.&nbsp;Finding the Dominant Words for Each Topic</a></li>
  <li><a href="#d.-finding-the-dominate-documents-for-each-topic" id="toc-d.-finding-the-dominate-documents-for-each-topic" class="nav-link" data-scroll-target="#d.-finding-the-dominate-documents-for-each-topic"><span class="header-section-number">5.4</span> d.&nbsp;Finding the Dominate Documents for Each Topic</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lab 4: Topic Modeling in MOOC-Eds with Python</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Dr.&nbsp;Shiyan Jiang </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> INTRODUCTION</h2>
<p>This case study walkthrough extends previous research and evaluation work at the Friday Institute for Educational Innovation at North Carolina State University. In addition to many other areas of inquiry, this work was aimed at understanding and improving peer interaction and discussion in the Friday Institute’s Massively Open Online Courses for Educators (MOOC-Ed) and Online Professional Learning programs. To learn more about these courses and programs, visit: <a href="https://place.fi.ncsu.edu" class="uri">https://place.fi.ncsu.edu</a></p>
<section id="walkthrough-focus" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="walkthrough-focus"><span class="header-section-number">1.1</span> Walkthrough Focus</h3>
<p>Our focus will be on identifying “topics” by examining how words cohere into different latent, or hidden, themes based on patterns of co-occurrence of words within documents. With a bit of tongue-in-cheek,&nbsp;<a href="http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/">Meeks and Weingart (2012)</a>&nbsp;describe topic modeling as:&nbsp;</p>
<blockquote class="blockquote">
<p><em>…focused on corpora and not individual texts, treating the works themselves as unceremonious ‘buckets of words,’ and providing seductive but obscure results in the forms of easily interpreted (and manipulated) ‘topics’…. To achieve its results, it leverages occult statistical methods like ‘dirichlet priors’ and ‘bayesian models.’</em></p>
</blockquote>
<p>That being said, <a href="http://journalofdigitalhumanities.org/2-1/dh-contribution-to-topic-modeling/">Weingart</a> also noted that “a topic model is a”clever and exceptionally versatile little algorithm that can be customized to all sorts of applications” and <a href="https://sicss.io/2020/materials/day3-text-analysis/topic-modeling/rmarkdown/Topic_Modeling.html#running-your-first-topic-model">Bail (2020)</a> add that topic modeling can be “a powerful tool for identifying general trends in a corpus that can then be analyzed in a more granular manner using other techniques.”</p>
<p>As noted by Krumm and Means (2018), the workflow for topic modelling is not always a linear process and there is often a great deal of iteration that occurs within and between wrangling, exploring, modeling. In this case study, we will primarily explore our data after the modeling process in order to gain some additional insight into the topics generated by our model. Specifically, this walkthrough covers the following concepts and skills:</p>
<ol type="1">
<li><strong>Prepare</strong>: Prior to analysis, we’ll take a quick look at some of the related MOOC-Ed research and evaluation work to gain some context for our analysis. This should aid in the interpretation of our results and help guide some decisions as we tidy, model, and visualize our data.</li>
<li><strong>Wrangle</strong>: In section 2, we again preprocess text such as tokenizing, removing stop words and lemmatizing using the <code>nltk</code> library. Other than these, there are two more essential steps to preprocess the text for topic modelling, which are creating a dictionary by using <code>corpora.Dictionary</code> and a corpus by using <code>doc2bow()</code> function, both steps will use <code>gensim</code> library.</li>
<li><strong>Model</strong>: We take a look at the most popular topic modeling algorithms: Latent Dirichlet Allocation (LDA) and using <code>gensim</code> library to implement the model. We also use coherence score approach to find the optimal number of topics for the LDA model.</li>
<li><strong>Explore</strong>: To further explore the results of our topic model, we use <code>pyLDAvis</code> for visualizing and exploring the discovered topics such as finding the dominate topics for each document, finding the dominate words for each topic, and finding the dominate documents for each topic.</li>
</ol>
<hr>
</section>
</section>
<section id="prepare" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="prepare"><span class="header-section-number">2</span> PREPARE</h2>
<p>To help us better understand the context, questions, and data sources we’ll be using, this section will focus on the following topics:</p>
<ol type="a">
<li><strong>Context</strong>. As context for our analysis, we’ll review several related papers by my colleagues relevant to the analysis of MOOC-Ed discussion forums.</li>
<li><strong>Questions.</strong> We’ll also examine what insight topic modeling can provide to a question that we asked participants answer in their professional learning teams (PLTs).</li>
<li><strong>Project Setup.</strong> We will set up a “home’ environment for the files and codes for lab 3 by installling and importing the required libraries for the topic modeling walkthrough.</li>
</ol>
<section id="a.-context" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="a.-context"><span class="header-section-number">2.1</span> a. Context</h3>
<section id="participating-in-a-mooc-and-professional-learning-team-how-a-blended-approach-to-professional-development-makes-a-difference" class="level4" data-number="2.1.1">
<h4 data-number="2.1.1" class="anchored" data-anchor-id="participating-in-a-mooc-and-professional-learning-team-how-a-blended-approach-to-professional-development-makes-a-difference"><span class="header-section-number">2.1.1</span> Participating in a MOOC and Professional Learning Team: How a Blended Approach to Professional Development Makes a Difference</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://place.fi.ncsu.edu/local/catalog/course.php?id=4&amp;ref=1"><img src="img/tsdi.png" title="Our world is rich with data sources, and technology makes data more accessible than ever before! To help ensure students are future ready to use data for making informed decisions, many countries around the world have increased the emphasis on statistics and data analysis in school curriculum–from elementary/primary grades through college. This course allows you to learn, along with colleagues from other schools, an investigation cycle to teach statistics and to help students explore data to make evidence-based claims. To learn more about engaging learners in making inferences and claims supported by data and how to emphasize inferential reasoning in teaching statistics through posing different types of investigative questions, enroll in our Teaching Statistics through Inferential Reasoning MOOC-Ed." class="img-fluid figure-img" alt="Teaching Statistics Through Data Investigations MOOC-Ed"></a></p>
<figcaption>Teaching Statistics Through Data Investigations MOOC-Ed</figcaption>
</figure>
</div>
<p>Full text: <a href="https://www.learntechlib.org/p/195234/" class="uri">https://www.learntechlib.org/p/195234/</a></p>
<p><strong>Abstract</strong></p>
<p>Massive Open Online Courses for Educators (MOOC-Eds) provide opportunities for using research-based learning and teaching practices, along with new technological tools and facilitation approaches for delivering quality online professional development. The Teaching Statistics Through Data Investigations MOOC-Ed was built for preparing teachers in pedagogy for teaching statistics, and it has been offered to participants from around the world. During 2016-2017, professional learning teams (PLTs) were formed from a subset of MOOC-Ed participants. These teams met several times to share and discuss their learning and experiences. This study focused on examining the ways that a blended approach to professional development may result in similar or different patterns of engagement to those who only participate in a large-scale online course. Results show the benefits of a blended learning environment for retention, engagement with course materials, and connectedness within the online community of learners in an online professional development on teaching statistics. The findings suggest the use of self-forming autonomous PLTs for supporting a deeper and more comprehensive experience with self-directed online professional developments such as MOOCs. Other online professional development courses, such as MOOCs, may benefit from purposely suggesting and advertising, and perhaps facilitating, the formation of small face-to-face or virtual PLTs who commit to engage in learning together.</p>
<p><strong>Data Source &amp; Analysis</strong></p>
<p>All peer interaction, including peer discussion, take place within discussion forums of MOOC-Eds, which are hosted using the Moodle Learning Management System. To build the dataset you’ll be using for this walkthrough, the research team wrote a query for Moodle’s MySQL database, which records participants’ user-logs of activity in the online forums. This sql query combines separate database tables containing postings and comments including participant IDs, timestamps, discussion text and other attributes or “metadata.”</p>
<p>For further description of the forums and data retrieval process, see also the following papers:</p>
<ul>
<li><p>Kellogg, S., &amp; Edelmann, A. (2015). <a href="https://bera-journals.onlinelibrary.wiley.com/doi/pdfdirect/10.1111/bjet.12312">Massively Open Online Course for Educators (MOOC‐Ed) network dataset</a>.&nbsp;<em>British journal of educational technology</em>,&nbsp;<em>46</em>(5), 977-983.</p></li>
<li><p>Ezen-Can, A., Boyer, K. E., Kellogg, S., &amp; Booth, S. (2015, March). <a href="https://dl.acm.org/doi/pdf/10.1145/2723576.2723589">Unsupervised modeling for understanding MOOC discussion forums: a learning analytics approach</a>. In&nbsp;<em>Proceedings of the fifth international conference on learning analytics and knowledge</em>&nbsp;(pp.&nbsp;146-150).</p></li>
<li><p>Kellogg, S., Booth, S., &amp; Oliver, K. (2014). <a href="https://www.erudit.org/en/journals/irrodl/1900-v1-n1-irrodl04945/1065545ar.pdf">A social network perspective on peer supported learning in MOOCs for educators.</a>&nbsp;<em>International Review of Research in Open and Distributed Learning</em>,&nbsp;<em>15</em>(5), 263-289.</p></li>
</ul>
<p><strong>Summary of Key Findings</strong></p>
<p>The following highlight some key findings related to the discussion forums in the papers cited above:</p>
<ol type="1">
<li>MOOCs designed specifically for K-12 teachers can provide positive self-directed learning experiences and rich engagement in discussion forums that help form online communities for educators.</li>
<li>Analysis of discussion forum data in TSDI provided a very clear picture of how enthusiastic many PLT members and leaders were to talk to others in the online community. They posed their questions and shared ideas with others about teaching statistics throughout the units, even though they were also meeting synchronously several times with their colleagues in small group PLTs.</li>
<li>Findings on knowledge construction demonstrated that over half of the discussions in both courses moved beyond sharing information and statements of agreement and entered a process of dissonance, negotiation and co-construction of knowledge, but seldom moved beyond this phase in which new knowledge was tested or applied. These findings echo similar research on difficulties in promoting knowledge construction in online settings.</li>
<li>Topic modeling provides more interpretable and cohesive models for discussion forums than other popular unsupervised modeling techniques such as k-means and k-medoids clustering algorithms.</li>
</ol>
</section>
</section>
<section id="b.-guiding-questions" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="b.-guiding-questions"><span class="header-section-number">2.2</span> b. Guiding Questions</h3>
<p>For the paper, <a href="https://www.learntechlib.org/p/195234/"><em>Participating in a MOOC and Professional Learning Team: How a Blended Approach to Professional Development Makes a Difference</em></a>, the researchers were interested in unpacking how participants who enrolled in the Teaching Statistics through Data Investigations MOOC-Ed might benefit from also being in a smaller group of professionals committed to engaging in the same professional development. The specific research question for this paper was:</p>
<blockquote class="blockquote">
<p>What are the similarities and differences between how PLT members and Non-PLT online participants engage and meet course goals in a MOOC-Ed designed for educators in secondary and collegiate settings?</p>
</blockquote>
<p>Dr.&nbsp;Hollylynne Lee and the TSDI team also developed a facilitation guide designed specifically for PLT teams to help groups synthesize the ideas in the course and make plans for how to implement new strategies in their classroom in order to impact students’ learning of statistics. One question PLT members were asked to address was:</p>
<blockquote class="blockquote">
<p>What ideas or issues emerged in the discussion forums this past week?</p>
</blockquote>
<p>For this walkthrough, we will further examine that question through the use of topic modeling.</p>
<p>One overarching question that Silge and Robinson (2018) identify as a central question to text mining and natural language processing, is:</p>
<blockquote class="blockquote">
<p>How do we to <strong>quantify</strong> what a document or collection of documents is about?</p>
</blockquote>
</section>
<section id="c.-set-up" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="c.-set-up"><span class="header-section-number">2.3</span> c.&nbsp;Set Up</h3>
<p>First, let’s load the following libraries that we’ll be needing for this walkthrough:</p>
<ul>
<li><p><a href="https://pandas.pydata.org"><code>pandas</code></a>: a powerful and widely-used Python library for data manipulation and analysis. It provides data structures and functions needed to work seamlessly with structured data, such as tables and time series.</p></li>
<li><p><a href="https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;opi=89978449&amp;url=https://www.nltk.org/&amp;ved=2ahUKEwid7Pn-97-GAxWKMVkFHSOqBuUQFnoECB4QAQ&amp;usg=AOvVaw1cuWsEY8ZUmW75nSDfXo7m"><code>nltk</code></a>: a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language. It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.</p></li>
<li><p><a href="https://pypi.org/project/gensim/"><code>gensim</code></a>: a Python library designed for natural language processing (NLP) tasks such as topic modeling, document indexing, and similarity retrieval, particularly with large text corpora. Gensim provides efficient implementations of popular topic modeling algorithms such as Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA).</p></li>
<li><p><a href="https://matplotlib.org"><code>matplotlib</code></a>: a comprehensive library for creating static, animated, and interactive visualizations in Python. It is widely used across various scientific and engineering disciplines to create high-quality graphs, charts, and plots.</p></li>
<li><p><a href="https://seaborn.pydata.org"><code>seaborn</code></a>: a Python data visualization library based on <code>matplotlib</code>. It provides a high-level interface for drawing attractive and informative statistical graphics.</p></li>
<li><p><a href="https://pypi.org/project/pyLDAvis/"><code>pyLDAvis</code></a>: a Python library used for interactive visualization of topic models. It is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.</p></li>
</ul>
<div id="5c1c3355" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install pandas nltk gensim matplotlib seaborn pyLDAvis</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)
Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)
Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.11/site-packages (4.3.0)
Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.11/site-packages (3.8.0)
Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.11/site-packages (0.12.2)
Requirement already satisfied: pyLDAvis in /opt/anaconda3/lib/python3.11/site-packages (3.4.1)
Requirement already satisfied: numpy&lt;2,&gt;=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)
Requirement already satisfied: pytz&gt;=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)
Requirement already satisfied: tzdata&gt;=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)
Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)
Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)
Requirement already satisfied: regex&gt;=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)
Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)
Requirement already satisfied: scipy&gt;=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (1.11.4)
Requirement already satisfied: smart-open&gt;=1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)
Requirement already satisfied: FuzzyTM&gt;=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (2.0.9)
Requirement already satisfied: contourpy&gt;=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)
Requirement already satisfied: cycler&gt;=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)
Requirement already satisfied: fonttools&gt;=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)
Requirement already satisfied: packaging&gt;=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.1)
Requirement already satisfied: pillow&gt;=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.2.0)
Requirement already satisfied: pyparsing&gt;=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)
Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (3.1.3)
Requirement already satisfied: numexpr in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.8.7)
Requirement already satisfied: funcy in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (2.0)
Requirement already satisfied: scikit-learn&gt;=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (1.2.2)
Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from pyLDAvis) (68.2.2)
Requirement already satisfied: pyfume in /opt/anaconda3/lib/python3.11/site-packages (from FuzzyTM&gt;=0.4.0-&gt;gensim) (0.3.1)
Requirement already satisfied: six&gt;=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.16.0)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn&gt;=1.0.0-&gt;pyLDAvis) (2.2.0)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2-&gt;pyLDAvis) (2.1.3)
Requirement already satisfied: simpful in /opt/anaconda3/lib/python3.11/site-packages (from pyfume-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (2.12.0)
Requirement already satisfied: fst-pso in /opt/anaconda3/lib/python3.11/site-packages (from pyfume-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (1.8.1)
Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.11/site-packages (from pyfume-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (4.9.0)
Requirement already satisfied: miniful in /opt/anaconda3/lib/python3.11/site-packages (from fst-pso-&gt;pyfume-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (0.0.6)</code></pre>
</div>
</div>
<div id="7426ef8c" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyLDAvis</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<hr>
</section>
</section>
<section id="wrangle" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="wrangle"><span class="header-section-number">3</span> WRANGLE</h2>
<p>Data wrangling involves some combination of cleaning, reshaping, transforming, and merging data (Wickham &amp; Grolemund, 2017). We’ll preprocess the data as follows to get it ready for topic modelling.</p>
<ol type="a">
<li><p><strong>Import Data</strong>. We’ll be working with .csv files and use <code>read_csv()</code> function to load data into our working environment.</p></li>
<li><p><strong>Subset Columns and Rows</strong>. We will filter the dataset and only includes the columns and rows of interests.</p></li>
<li><p><strong>Prepare Data for LDA Modelling.</strong></p>
<p><strong>Step 1: Tokenize Text and Remove Stop Words.</strong> Tokenization and stop words removals are often the first steps in preparing the text data for further exploring or modelling, including the LDA topic modelling.</p>
<p><strong>Step 2: Lemmatization.</strong> It is another crucial step that helps in normalizing the text data, reducing words to their base or root form. In the case of topic modelling, it enhances the model’s ability to identify the underlying topics.</p>
<p><strong>Step 3: Create a Dictionary.</strong> The dictionary assigns a unique integer ID to each unique word in the dataset. This ID is used to create a numerical representation of the text, which is important for text mining including LDA topic modelling, as they always operate on numerical representations of the text (e.g., the ID) rather than the raw text itself.</p>
<p><strong>Step 4: Create a Corpus.</strong> Build upon the dictionary, the corpus represents the text data with words count besides the IDs. In the form of a corpus, each document is represented as a list of tuples (word_id, word_count), which is necessary for LDA to perform its computations.</p></li>
</ol>
<section id="a.-import-forum-data" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="a.-import-forum-data"><span class="header-section-number">3.1</span> a. Import Forum Data</h3>
<p>To get started, we need to import, or “read”, our data into Python. The function used to import your data will depend on the file format of the data you are trying to import. Let’s read our data into our Environment using the <code>read_csv()</code> function and assign it to a variable name so we can work with it like any other object in Python.</p>
<div id="5d197678" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>ts_forum_data <span class="op">=</span> pd.read_csv(<span class="st">"data/ts_forum_data.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To get an overview, we can display the ts_forum_data data frame. You can find that the data frame includes course information (e.g., course_id, course_name), discussion forum information (e.g., forum_id, discussion_id), and more importantly, the post information (e.g., post_id, post_title, post_content).</p>
<div id="4d93532d" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>ts_forum_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">course_id</th>
<th data-quarto-table-cell-role="th">course_name</th>
<th data-quarto-table-cell-role="th">forum_id</th>
<th data-quarto-table-cell-role="th">forum_name</th>
<th data-quarto-table-cell-role="th">discussion_id</th>
<th data-quarto-table-cell-role="th">discussion_name</th>
<th data-quarto-table-cell-role="th">discussion_creator</th>
<th data-quarto-table-cell-role="th">discussion_poster</th>
<th data-quarto-table-cell-role="th">discussion_reference</th>
<th data-quarto-table-cell-role="th">parent_id</th>
<th data-quarto-table-cell-role="th">post_date</th>
<th data-quarto-table-cell-role="th">post_id</th>
<th data-quarto-table-cell-role="th">post_title</th>
<th data-quarto-table-cell-role="th">post_content</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>9.0</td>
<td>Teaching Statistics Through Data Investigatio...</td>
<td>126.0</td>
<td>Investigate: Analyze Tasks</td>
<td>6822.0</td>
<td>Not much comparison...</td>
<td>4513.0</td>
<td>4963.0</td>
<td>4513.0</td>
<td>16374.0</td>
<td>15/10/05 20:48:10</td>
<td>16597.0</td>
<td>Re: Not much comparison...</td>
<td>I also like the Coke vs. Pepsi problem becaus...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>9.0</td>
<td>Teaching Statistics Through Data Investigatio...</td>
<td>126.0</td>
<td>Investigate: Analyze Tasks</td>
<td>6822.0</td>
<td>Not much comparison...</td>
<td>4513.0</td>
<td>5009.0</td>
<td>4513.0</td>
<td>16374.0</td>
<td>15/10/05 16:13:40</td>
<td>16480.0</td>
<td>Re: Not much comparison...</td>
<td>By far the best activity as a whole was Pepsi...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>9.0</td>
<td>Teaching Statistics Through Data Investigatio...</td>
<td>126.0</td>
<td>Investigate: Analyze Tasks</td>
<td>6822.0</td>
<td>Not much comparison...</td>
<td>4513.0</td>
<td>4409.0</td>
<td>4513.0</td>
<td>16374.0</td>
<td>15/10/06 05:53:59</td>
<td>16754.0</td>
<td>Re: Not much comparison...</td>
<td>Hello Clark I felt the same way...the Pepsi...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>9.0</td>
<td>Teaching Statistics Through Data Investigatio...</td>
<td>126.0</td>
<td>Investigate: Analyze Tasks</td>
<td>6822.0</td>
<td>Not much comparison...</td>
<td>4513.0</td>
<td>4854.0</td>
<td>4513.0</td>
<td>16374.0</td>
<td>15/10/09 00:12:00</td>
<td>17414.0</td>
<td>Re: Not much comparison...</td>
<td>I definitely agree: there are not much compar...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>9.0</td>
<td>Teaching Statistics Through Data Investigatio...</td>
<td>126.0</td>
<td>Investigate: Analyze Tasks</td>
<td>6822.0</td>
<td>Not much comparison...</td>
<td>4513.0</td>
<td>4116.0</td>
<td>4513.0</td>
<td>16374.0</td>
<td>15/10/09 20:41:22</td>
<td>17568.0</td>
<td>Re: Not much comparison...</td>
<td>I agree that the Pepsi vs. Coke experiment wa...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">5784</td>
<td>76.0</td>
<td>Teaching Statistics Through Inferential Reason...</td>
<td>950.0</td>
<td>Discuss With Colleagues</td>
<td>25295.0</td>
<td>Summer assignment</td>
<td>3712.0</td>
<td>3712.0</td>
<td>NaN</td>
<td>0.0</td>
<td>17/12/03 18:06:41</td>
<td>66550.0</td>
<td>Summer assignment</td>
<td>I give a summer assignment to my students bef...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5785</td>
<td>76.0</td>
<td>Teaching Statistics Through Inferential Reason...</td>
<td>950.0</td>
<td>Discuss With Colleagues</td>
<td>25607.0</td>
<td>Strategies</td>
<td>9671.0</td>
<td>9671.0</td>
<td>NaN</td>
<td>0.0</td>
<td>17/12/07 22:53:53</td>
<td>67158.0</td>
<td>Strategies</td>
<td>One strategy that I've implemented in the use...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">5786</td>
<td>76.0</td>
<td>Teaching Statistics Through Inferential Reason...</td>
<td>950.0</td>
<td>Discuss With Colleagues</td>
<td>25607.0</td>
<td>Strategies</td>
<td>9671.0</td>
<td>16592.0</td>
<td>9671.0</td>
<td>67158.0</td>
<td>17/12/14 19:33:33</td>
<td>68019.0</td>
<td>Re: Strategies</td>
<td>Thank you this is good information.</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5787</td>
<td>76.0</td>
<td>Teaching Statistics Through Inferential Reason...</td>
<td>950.0</td>
<td>Discuss With Colleagues</td>
<td>25822.0</td>
<td>Available information</td>
<td>14611.0</td>
<td>14611.0</td>
<td>NaN</td>
<td>0.0</td>
<td>17/12/11 17:56:18</td>
<td>67531.0</td>
<td>Available information</td>
<td>I have enjoyed learning throughout this cours...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">5788</td>
<td>76.0</td>
<td>Teaching Statistics Through Inferential Reason...</td>
<td>950.0</td>
<td>Discuss With Colleagues</td>
<td>26300.0</td>
<td>Integrating inferential reasoning into everything</td>
<td>4537.0</td>
<td>4537.0</td>
<td>NaN</td>
<td>0.0</td>
<td>17/12/17 00:49:33</td>
<td>68285.0</td>
<td>Integrating inferential reasoning into everything</td>
<td>From this course I learned that inferential re...</td>
</tr>
</tbody>
</table>

<p>5789 rows × 14 columns</p>
</div>
</div>
</div>
</div>
</section>
<section id="b.-subset-columns-and-rows" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="b.-subset-columns-and-rows"><span class="header-section-number">3.2</span> b. Subset Columns and Rows</h3>
<p>The post_content will be the text from which we will discover the topics, and post_id as the identifier could be useful, so those two columns will be the columns of interests, so let’s subset column and create a <strong>post_content</strong> data frame. Also, the rows with missing values will not contribute to the topics, so let’s drop it out.</p>
<div id="8d611ab5" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select 'post_id' and 'post_content' columns</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>post_content <span class="op">=</span> ts_forum_data[[<span class="st">'post_id'</span>, <span class="st">'post_content'</span>]]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Rename the columns to 'id' and 'document'</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>post_content.columns <span class="op">=</span> [<span class="st">'id'</span>, <span class="st">'document'</span>]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop rows with missing values in the 'document' column</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>post_content.dropna(subset<span class="op">=</span>[<span class="st">'document'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the resulting DataFrame</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(post_content)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           id                                           document
0     16597.0   I also like the Coke vs. Pepsi problem becaus...
1     16480.0   By far the best activity as a whole was Pepsi...
2     16754.0   Hello Clark   I felt the same way...the Pepsi...
3     17414.0   I definitely agree: there are not much compar...
4     17568.0   I agree that the Pepsi vs. Coke experiment wa...
...       ...                                                ...
5784  66550.0   I give a summer assignment to my students bef...
5785  67158.0   One strategy that I've implemented in the use...
5786  68019.0               Thank you this is good information. 
5787  67531.0   I have enjoyed learning throughout this cours...
5788  68285.0  From this course I learned that inferential re...

[5787 rows x 2 columns]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/var/folders/_3/45612wnj07v1x0p_l6mqmjkw0000gn/T/ipykernel_73317/508170866.py:8: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
</code></pre>
</div>
</div>
</section>
<section id="c.-prepare-data-for-lda-model" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="c.-prepare-data-for-lda-model"><span class="header-section-number">3.3</span> c.&nbsp;Prepare Data for LDA Model</h3>
<p>In this session, we will processing text and transforming our data frames into new data structures required for topic modeling, in the case of using <code>gensim</code> for LDA topic modelling, the needed data structure is the bag of word (BoW) format which will be the <strong>corpus</strong> data frame that we will end with after this session.</p>
<section id="step-1-tokenize-text-remove-stop-words" class="level4" data-number="3.3.1">
<h4 data-number="3.3.1" class="anchored" data-anchor-id="step-1-tokenize-text-remove-stop-words"><span class="header-section-number">3.3.1</span> Step 1: Tokenize Text &amp; Remove Stop Words</h4>
<p>We are already familiar with the tokization process from the last two labs. Tokenization and stop words removals are always the first step for most Natural Language Processing (NLP) models, including LDA. It involves splitting the text into individual words or tokens and remove nonsense ones, which are the basic units for analysis. Let’s tokenize and remove stop wordsour forum text and by using the familiar <code>tokenize()</code> and <code>stopwords</code>:</p>
<div id="bda93fe6" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> RegexpTokenizer</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure nltk punkt tokenizer and stopwords are downloaded</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'punkt'</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'stopwords'</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a tokenizer instance with the regular expression of tweets</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>tweet_pattern <span class="op">=</span> <span class="vs">r'\b\w+\b'</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> RegexpTokenizer(tweet_pattern)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify stop words</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>stop_words <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">'english'</span>))</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize the 'document' column, remove stop words, and create a new column 'word'</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>post_content[<span class="st">'word'</span>] <span class="op">=</span> post_content[<span class="st">'document'</span>].<span class="bu">apply</span>(</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> x: [token.lower() <span class="cf">for</span> token <span class="kw">in</span> tokenizer.tokenize(<span class="bu">str</span>(x)) <span class="cf">if</span> token.lower() <span class="kw">not</span> <span class="kw">in</span> stop_words]</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Explode the 'word' column to transform each row into individual words</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>post_content_exploded <span class="op">=</span> post_content.explode(<span class="st">'word'</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the resulting DataFrame</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(post_content_exploded)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Explode the 'word' column to transform each row into individual words</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>post_content_exploded <span class="op">=</span> post_content.explode(<span class="st">'word'</span>)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the resulting DataFrame</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(post_content_exploded)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package punkt to /Users/minzhuang/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[nltk_data] Downloading package stopwords to
[nltk_data]     /Users/minzhuang/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
/var/folders/_3/45612wnj07v1x0p_l6mqmjkw0000gn/T/ipykernel_73317/3185371319.py:17: SettingWithCopyWarning:


A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>           id                                           document         word
0     16597.0   I also like the Coke vs. Pepsi problem becaus...         also
0     16597.0   I also like the Coke vs. Pepsi problem becaus...         like
0     16597.0   I also like the Coke vs. Pepsi problem becaus...         coke
0     16597.0   I also like the Coke vs. Pepsi problem becaus...           vs
0     16597.0   I also like the Coke vs. Pepsi problem becaus...        pepsi
...       ...                                                ...          ...
5788  68285.0  From this course I learned that inferential re...         work
5788  68285.0  From this course I learned that inferential re...  inferential
5788  68285.0  From this course I learned that inferential re...    reasoning
5788  68285.0  From this course I learned that inferential re...   throughout
5788  68285.0  From this course I learned that inferential re...       course

[267868 rows x 3 columns]
           id                                           document         word
0     16597.0   I also like the Coke vs. Pepsi problem becaus...         also
0     16597.0   I also like the Coke vs. Pepsi problem becaus...         like
0     16597.0   I also like the Coke vs. Pepsi problem becaus...         coke
0     16597.0   I also like the Coke vs. Pepsi problem becaus...           vs
0     16597.0   I also like the Coke vs. Pepsi problem becaus...        pepsi
...       ...                                                ...          ...
5788  68285.0  From this course I learned that inferential re...         work
5788  68285.0  From this course I learned that inferential re...  inferential
5788  68285.0  From this course I learned that inferential re...    reasoning
5788  68285.0  From this course I learned that inferential re...   throughout
5788  68285.0  From this course I learned that inferential re...       course

[267868 rows x 3 columns]</code></pre>
</div>
</div>
<p>Now let’s do a quick word count to see some of the most common words used throughout the forums.</p>
<div id="d99cc4ae" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Count the occurrences of each word</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>words_count <span class="op">=</span> post_content_exploded[<span class="st">'word'</span>].value_counts().reset_index()</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>words_count.columns <span class="op">=</span> [<span class="st">'word'</span>, <span class="st">'count'</span>]</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the word count of the top 20 words</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 20 words and their counts:"</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(words_count.head(<span class="dv">20</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Top 20 words and their counts:
          word  count
0     students   6843
1         data   4371
2   statistics   3107
3        would   2693
4        think   2049
5       course   1690
6         like   1526
7       school   1500
8          use   1472
9    questions   1470
10       class   1426
11        also   1411
12        font   1311
13        span   1267
14        time   1253
15         one   1219
16       style   1178
17       could   1116
18      normal   1031
19      really   1014</code></pre>
</div>
</div>
<p>Terms like “students,” “data,” and “class” are about what we would have expected from a course teaching statistics. The term “agree” and “time” however, are not so intuitive and worth a quick look as well.</p>
<section id="comprehension-check" class="level5" data-number="3.3.1.1">
<h5 data-number="3.3.1.1" class="anchored" data-anchor-id="comprehension-check"><span class="header-section-number">3.3.1.1</span> Comprehension Check</h5>
<p>Use the <code>str.contains()</code> function to filter for rows in our <code>post_content</code> data frame that contain the term “time” and another term or terms of your choosing. After filtering, select a random sample of 10 posts using the <code>sample()</code> function for your terms and answer the following questions:</p>
<ol type="1">
<li><p>What, if anything, do these posts have in common?</p></li>
<li><p>What topics or themes might be apparent, or do you anticipate emerging, from our topic modeling?</p></li>
</ol>
<p>Your output should look something like this:</p>
<div id="4b8fed65" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter to get rows where text contains the word 'time'</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>post_with_time <span class="op">=</span> post_content[post_content[<span class="st">'document'</span>].<span class="bu">str</span>.contains(<span class="st">'time'</span>)]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Select a random sample of 10 rows</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>time <span class="op">=</span> post_with_time.sample(n<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the random sample</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(time)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           id                                           document  \
3879  39597.0   I agree with you Rebecca. I had a project las...   
4402  49972.0   The Google form is a great idea.  I taught a ...   
1479  13978.0   In my experience working with teachers and st...   
5062  67502.0   Yes!  I can't begin to tell you how many time...   
1731  20891.0   I too wondered about the age  since younger s...   
559   17316.0   I'm right there with you Sonia and Ronald!  I...   
2675  34142.0   so much! 1)  I think we need to use good task...   
256   13055.0   Hello Mario:  Yes  on the Unit 1 there is lin...   
3711  40969.0   I was thinking this as well  Craig!  My AP st...   
1114  21640.0   I totally agree with you! All the tools  and ...   

                                                   word  
3879  [agree, rebecca, project, last, year, algebra,...  
4402  [google, form, great, idea, taught, class, 50,...  
1479  [experience, working, teachers, students, prob...  
5062  [yes, begin, tell, many, times, year, hear, st...  
1731  [wondered, age, since, younger, students, migh...  
559   [right, sonia, ronald, creative, comes, making...  
2675  [much, 1, think, need, use, good, tasks, allow...  
256   [hello, mario, yes, unit, 1, link, website, ga...  
3711  [thinking, well, craig, ap, students, think, a...  
1114  [totally, agree, tools, especially, videos, pr...  </code></pre>
</div>
</div>
</section>
</section>
<section id="step-2-lemmatize" class="level4" data-number="3.3.2">
<h4 data-number="3.3.2" class="anchored" data-anchor-id="step-2-lemmatize"><span class="header-section-number">3.3.2</span> Step 2: Lemmatize</h4>
<p>Lemmatization is a crucial step in preparing our text data for topic modeling. It reduces words to their base or root form. For example, the lemma of the words “running”, “runs”, and “ran” is “run”. By grouping similar words under one token, it will make the analysis more efficient, and improve the coherence of the topics identified by the model.</p>
<p>To lemmatize the word, we will introduce the <code>WordNetLemmatizer</code> class. One important notice here, the lemmatizer expects input that is composed of characters (strings) and can be interpreted as words. If you pass non-string data, such as floats, to the lemmatizer, it may encounter errors because it doesn’t know how to interpret these data types as words.</p>
<p>If there is a post content like “A class as large as that 300 is impossibly…”, after tokenizing, the ‘300’ will be one of the tokens. Surrounded by quotation marks, the ‘300’ is still string, while after exploding, the 300 will be recognized as integer data type by Python, it is no longer the string.</p>
<p>Therefore, before applying lemmatization or any other text processing technique, it’s important to ensure that your data consists of strings.</p>
<div id="f50e94bb" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert float values to strings in the 'word' column</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>post_content_exploded[<span class="st">'word'</span>] <span class="op">=</span> post_content_exploded[<span class="st">'word'</span>].astype(<span class="bu">str</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can lemmatize using <code>WordNetLemmatizer</code> from <code>nltk</code> library.</p>
<div id="17355f0c" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.stem <span class="im">import</span> WordNetLemmatizer</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure NLTK resources are downloaded</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'wordnet'</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the WordNetLemmatizer</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>lemmatizer <span class="op">=</span> WordNetLemmatizer()</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply lemmatization to the 'word' column</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>post_content_exploded[<span class="st">'lemmatized_word'</span>] <span class="op">=</span> post_content_exploded[<span class="st">'word'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: lemmatizer.lemmatize(x))</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the resulting DataFrame</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(post_content_exploded)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package wordnet to
[nltk_data]     /Users/minzhuang/nltk_data...
[nltk_data]   Package wordnet is already up-to-date!</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>           id                                           document         word  \
0     16597.0   I also like the Coke vs. Pepsi problem becaus...         also   
0     16597.0   I also like the Coke vs. Pepsi problem becaus...         like   
0     16597.0   I also like the Coke vs. Pepsi problem becaus...         coke   
0     16597.0   I also like the Coke vs. Pepsi problem becaus...           vs   
0     16597.0   I also like the Coke vs. Pepsi problem becaus...        pepsi   
...       ...                                                ...          ...   
5788  68285.0  From this course I learned that inferential re...         work   
5788  68285.0  From this course I learned that inferential re...  inferential   
5788  68285.0  From this course I learned that inferential re...    reasoning   
5788  68285.0  From this course I learned that inferential re...   throughout   
5788  68285.0  From this course I learned that inferential re...       course   

     lemmatized_word  
0               also  
0               like  
0               coke  
0                  v  
0              pepsi  
...              ...  
5788            work  
5788     inferential  
5788       reasoning  
5788      throughout  
5788          course  

[267868 rows x 4 columns]</code></pre>
</div>
</div>
</section>
<section id="step-3-create-a-dictionary" class="level4" data-number="3.3.3">
<h4 data-number="3.3.3" class="anchored" data-anchor-id="step-3-create-a-dictionary"><span class="header-section-number">3.3.3</span> Step 3: <strong>Create a Dictionary</strong></h4>
<p>The dictionary maps each word in our dataset to a unique integer ID. This is important because LDA modeling operates on numerical representations of the text (e.g., the ID) rather than the raw text itself. And the dictionary helps to track the entire vocabulary, making it easier to manage and manipulate the text data.</p>
<p>To create the dictionary, we will use the <code>corpora.Dictionary</code> from <code>gensim</code> library. Notice that the input of <code>corpora.Dictionary</code> should be the lists of token list for each document (in our case, the document refers to the post). So let’s convert the lemmatized words back to lists for each post, then create a dictionary.</p>
<div id="a1af203f" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim <span class="im">import</span> corpora</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the lemmatized words back to lists for each document</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>lemmatized_documents <span class="op">=</span> post_content_exploded.groupby(<span class="st">'document'</span>)[<span class="st">'lemmatized_word'</span>].<span class="bu">apply</span>(<span class="bu">list</span>).tolist()</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Dictionary</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>dictionary <span class="op">=</span> corpora.Dictionary(lemmatized_documents)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the dictionary</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co"># print(dictionary.token2id)  # This will print the token to id mapping</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="step-4-create-a-corpus" class="level4" data-number="3.3.4">
<h4 data-number="3.3.4" class="anchored" data-anchor-id="step-4-create-a-corpus"><span class="header-section-number">3.3.4</span> Step 4: Create a Corpus</h4>
<p>Now we get a dictionary by mapping the words with IDs as one of the numerical representation of text data. The word counts is another important numerical representation which is necessary for LDA to perform its computations.</p>
<p>The corpus combines the IDs and word count together to represent the text data, which is a list of documents, where each document is represented as a list of tuples (word_id, word_count). This format is known as the bag-of-words (BoW) format.</p>
<p><strong>Bag-of-Words (BoW)</strong>: The BoW simplifies the text data by representing the number of occurrences of each word in the document without considering the word order. This simplification is suitable for topic modeling and lots of other text mining tasks.</p>
<p>To create a corpus, we will apply the <code>doc2bow()</code> function on the dictionary that we just created.</p>
<div id="84689e34" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a corpus: BOW representation of each document</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [dictionary.doc2bow(doc) <span class="cf">for</span> doc <span class="kw">in</span> lemmatized_documents]</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the corpus</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># print(corpus) # This will print the id to count mapping</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>LDA requires the input data in the form of a corpus, where each document is represented as a list of (word_id, word_count) tuples. This input format allows LDA to calculate the distribution of topics in each document and the distribution of words in each topic. As we already get the corpus, we are ready to perform the LDA modelling.</p>
<hr>
</section>
</section>
</section>
<section id="model" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="model"><span class="header-section-number">4</span> MODEL</h2>
<p>In very simple terms, modeling involves developing a mathematical summary of a dataset. These summaries can help us further explore trends and patterns in our data.</p>
<p>In their book, <em>Learning Analytics Goes to School,</em> Krumm and Means (2018) describe two general types of modeling approaches used in the Data-Intensive Research workflow: unsupervised and supervised machine learning. In distinguishing between the two, they note:</p>
<blockquote class="blockquote">
<p><em>Unsupervised learning algorithms can be used to understand the structure of one’s dataset. Supervised models, on the other hand, help to quantify relationships between features and a known outcome. Known outcomes are also commonly referred to as labels or dependent variables.</em></p>
</blockquote>
<p>In Section 3 we focus on Topic Modeling, an unsupervised learning approach to automatically identify topics in a collection of documents. In fact, we’ll explore the most popular approach to topic modeling - LDA, as well as strategies for identifying the “right” number of topics:</p>
<ol type="a">
<li><strong>Fitting a Topic Modeling with LDA</strong>. In this section we learn to use the <code>LdaModel</code> function for unsupervised classification of our forum discussions to find natural groupings of words, or topics.</li>
<li><strong>Choosing K.</strong> Finally, we wrap up Section 3 by learning about diagnostic properties like exclusivity, semantic coherence, and held out likelihood for helping to select an appropriate number of topics.</li>
</ol>
<section id="a.-fitting-a-topic-modeling-with-lda" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="a.-fitting-a-topic-modeling-with-lda"><span class="header-section-number">4.1</span> a. Fitting a Topic Modeling with LDA</h3>
<p>Before running our first topic model using the <code>LdaModel</code> function, let’s learn some basic principles behind LDA and why LDA is of preferred over other automatic classification or clustering approaches.</p>
<p>Unlike simple forms of cluster analysis such as k-means clustering, LDA is a <strong>“mixture” model</strong>, which in our context means that:</p>
<ol type="1">
<li><strong>Every <u>document</u> contains a mixture of topics.</strong> Unlike algorithms like k-means, LDA treats each document as a mixture of topics, which allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups. So in practice, this means that a discussion forum post could have an estimated topic proportion of 70% for Topic 1 (e.g.&nbsp;be mostly about a Topic 1), but also be partly about Topic 2.</li>
<li><strong>Every <u>topic</u> contains a mixture of words.</strong>&nbsp;For example, if we specified in our LDA model just 2 topics for our discussion posts, we might find that one topic seems to be about pedagogy while another is about learning. The most common words in the pedagogy topic might be “teacher”, “strategies”, and “instruction”, while the learning topic may be made up of words like “understanding” and “students”. However, words can be shared between topics and words like “statistics” or “assessment” might appear in both equally.</li>
</ol>
<p>Similar to k-means other other simple clustering approaches, however, LDA does require us to specify a value of <em>k</em> for the number of topics in our corpus. Selecting <em>k</em> is no trivial matter and can greatly impact your results.</p>
<p>Since so far we don’t have a have strong rationale about the number of topics that might exist in discussion forums, let’s use the <code>nunique()</code> function from the <code>pandas</code> library to find the number of unique forum names in our course data and run with that:</p>
<div id="80794305" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Find the number of unique values</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>unique_forum_count <span class="op">=</span> ts_forum_data[<span class="st">'forum_name'</span>].nunique()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print count of unique values</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(unique_forum_count)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>22</code></pre>
</div>
</div>
<p>Since it looks like there are 22 distinct discussion forums, we’ll use that as our value for the <code>num_topics =</code> argument of the <code>LdaModel</code>. Be patient while this runs, since the default setting of is to perform a large number of iterations.</p>
<div id="78e75e7e" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.ldamodel <span class="im">import</span> LdaModel</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Build the LDA model</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>lda_model <span class="op">=</span> LdaModel(corpus, num_topics<span class="op">=</span><span class="dv">22</span>, id2word<span class="op">=</span>dictionary, passes<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the topics</span></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, topic <span class="kw">in</span> lda_model.print_topics(<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Topic: </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">Words: </span><span class="sc">{</span>topic<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Topic: 0 
Words: 0.013*"one" + 0.012*"use" + 0.011*"student" + 0.010*"like" + 0.009*"example" + 0.007*"group" + 0.007*"class" + 0.007*"know" + 0.006*"course" + 0.006*"1"
Topic: 1 
Words: 0.238*"div" + 0.017*"class" + 0.014*"shoe" + 0.012*"u" + 0.011*"image" + 0.010*"http" + 0.009*"ltr" + 0.008*"dir" + 0.008*"market" + 0.007*"go"
Topic: 2 
Words: 0.051*"student" + 0.035*"question" + 0.018*"would" + 0.017*"think" + 0.014*"answer" + 0.013*"statistic" + 0.011*"need" + 0.010*"one" + 0.010*"get" + 0.009*"problem"
Topic: 3 
Words: 0.054*"statistic" + 0.038*"course" + 0.021*"stats" + 0.020*"teaching" + 0.017*"teach" + 0.016*"class" + 0.016*"ap" + 0.014*"teacher" + 0.013*"resource" + 0.013*"math"
Topic: 4 
Words: 0.075*"http" + 0.044*"www" + 0.040*"com" + 0.035*"href" + 0.027*"org" + 0.017*"target" + 0.017*"_blank" + 0.016*"amp" + 0.015*"data" + 0.015*"online"
Topic: 5 
Words: 0.075*"value" + 0.055*"mean" + 0.042*"p" + 0.028*"standard" + 0.024*"test" + 0.021*"deviation" + 0.018*"hypothesis" + 0.016*"difference" + 0.012*"probability" + 0.012*"calculate"
Topic: 6 
Words: 0.043*"locus" + 0.035*"item" + 0.028*"resource" + 0.022*"assessment" + 0.018*"website" + 0.017*"org" + 0.016*"pdf" + 0.016*"site" + 0.013*"http" + 0.013*"professional"
Topic: 7 
Words: 0.122*"thanks" + 0.068*"thank" + 0.061*"sharing" + 0.039*"plot" + 0.034*"box" + 0.031*"great" + 0.031*"idea" + 0.029*"resource" + 0.022*"link" + 0.020*"love"
Topic: 8 
Words: 0.016*"year" + 0.016*"8" + 0.015*"class" + 0.013*"time" + 0.013*"minute" + 0.013*"grade" + 0.011*"3" + 0.011*"5" + 0.010*"one" + 0.008*"standard"
Topic: 9 
Words: 0.025*"playlist" + 0.023*"dr" + 0.017*"ltr" + 0.015*"dir" + 0.014*"moodle" + 0.014*"14" + 0.011*"channel" + 0.011*"decoration" + 0.011*"list" + 0.011*"baseline"
Topic: 10 
Words: 0.032*"li" + 0.027*"course" + 0.027*"strong" + 0.021*"http" + 0.021*"href" + 0.020*"ncsu" + 0.020*"b" + 0.019*"edu" + 0.018*"target" + 0.018*"fi"
Topic: 11 
Words: 0.045*"span" + 0.044*"quot" + 0.036*"style" + 0.035*"font" + 0.021*"p" + 0.017*"family" + 0.017*"size" + 0.014*"left" + 0.014*"10" + 0.013*"25in"
Topic: 12 
Words: 0.117*"span" + 0.086*"td" + 0.070*"style" + 0.057*"0" + 0.051*"height" + 0.042*"right" + 0.040*"line" + 0.036*"1" + 0.031*"top" + 0.031*"width"
Topic: 13 
Words: 0.098*"school" + 0.088*"level" + 0.051*"high" + 0.042*"middle" + 0.037*"student" + 0.024*"grade" + 0.018*"elementary" + 0.014*"test" + 0.012*"would" + 0.011*"statistical"
Topic: 14 
Words: 0.042*"student" + 0.022*"data" + 0.018*"school" + 0.013*"statistic" + 0.013*"think" + 0.011*"would" + 0.010*"video" + 0.010*"way" + 0.009*"like" + 0.009*"also"
Topic: 15 
Words: 0.061*"sample" + 0.025*"size" + 0.024*"simulation" + 0.020*"trial" + 0.018*"dice" + 0.017*"population" + 0.017*"number" + 0.016*"result" + 0.015*"fair" + 0.014*"sampling"
Topic: 16 
Words: 0.048*"student" + 0.034*"task" + 0.019*"would" + 0.014*"technology" + 0.014*"think" + 0.012*"statistical" + 0.012*"group" + 0.011*"could" + 0.009*"different" + 0.009*"activity"
Topic: 17 
Words: 0.085*"data" + 0.049*"student" + 0.020*"would" + 0.018*"question" + 0.013*"could" + 0.011*"time" + 0.011*"set" + 0.010*"think" + 0.010*"task" + 0.009*"also"
Topic: 18 
Words: 0.039*"language" + 0.034*"english" + 0.033*"science" + 0.021*"art" + 0.020*"math" + 0.016*"conclusion" + 0.015*"stick" + 0.013*"field" + 0.013*"scientific" + 0.011*"chemistry"
Topic: 19 
Words: 0.037*"agree" + 0.021*"excellent" + 0.019*"supposed" + 0.019*"growth" + 0.016*"literacy" + 0.014*"dear" + 0.014*"dung" + 0.013*"equal" + 0.011*"onto" + 0.011*"caffeine"
Topic: 20 
Words: 0.064*"font" + 0.058*"normal" + 0.055*"text" + 0.046*"0px" + 0.037*"color" + 0.034*"style" + 0.028*"rgb" + 0.026*"255" + 0.025*"51" + 0.023*"none"
Topic: 21 
Words: 0.044*"coaster" + 0.036*"chart" + 0.023*"roller" + 0.020*"pie" + 0.020*"steel" + 0.019*"bar" + 0.019*"speed" + 0.018*"yes" + 0.013*"totally" + 0.011*"height"</code></pre>
</div>
</div>
<p>Notice that the <code>passes</code> parameter specifies the number of times the entire corpus will be used to update the LDA model’s parameters and improve the quality of the topic distributions during training. This is part of the process known as “epochs” in other machine learning contexts. If not specified, the default value for <code>passes</code> is typically 1, here we choose 15. Increasing the number of passes can lead to a more accurate and stable model because the model has more opportunities to refine its parameters. However, more passes also mean longer training times.</p>
</section>
<section id="b.-finding-optimal-k---num_topics" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="b.-finding-optimal-k---num_topics"><span class="header-section-number">4.2</span> b. Finding optimal K - num_topics</h3>
<p>As alluded to earlier, selecting the number of topics for your model is a non-trivial decision and can dramatically impact your results. Bail (2018) notes that</p>
<blockquote class="blockquote">
<p><em>The results of topic models should not be over-interpreted unless the researcher has strong theoretical apriori about the number of topics in a given corpus, or if the researcher has carefully validated the results of a topic model using both the quantitative and qualitative techniques described above.</em></p>
</blockquote>
<p>There are several approaches to estimating a value for K and we’ll take a quick look at coherence score approach. The coherence score measures the degree of semantic similarity between high scoring words in the topic. The idea is to find a number of topics that maximizes the coherence score. The <code>gensim</code> library provides <code>CoherenceModel</code> that we can use to compute the coherence scores.</p>
<section id="coherence-score-approach" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="coherence-score-approach"><span class="header-section-number">4.2.1</span> <strong>Coherence Score Approach</strong></h4>
<div id="3f5834b2" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># from gensim.models import CoherenceModel</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># def compute_coherence_values(dictionary, corpus, texts, start=2, limit=40, step=6):</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co">#     coherence_values = []</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co">#     model_list = []</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co">#     for num_topics in range(start, limit, step):</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co">#         model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=15)</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co">#         model_list.append(model)</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co">#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co">#         coherence_values.append(coherencemodel.get_coherence())</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="co">#     return model_list, coherence_values</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a><span class="co"># # Compute coherence scores</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="co"># model_list, coherence_values = compute_coherence_values(dictionary, corpus, post_content_exploded['lemmatized_word'], start=2, limit=40, step=2)</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="co"># # Plot coherence scores</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a><span class="co"># x = range(2, 40, 2)</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.plot(x, coherence_values)</span></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.xlabel("Num Topics")</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.ylabel("Coherence score")</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.legend("coherence_values", loc='best')</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.show()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Notice that the <code>CoherenceModel</code> will take a while to run. The output looks like this:</p>
<p><img src="img/coherencemodel-output.png" class="img-fluid"></p>
<p>As a general rule of thumb and overly simplistic heuristic, we’re looking for an inflection point in our plot which indicates an optimal number of topics to select for a value of K. Let’s select the model with highest coherence value, and print out the topics identified the optimal model.</p>
<div id="5f4e3d79" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # Select the model with highest coherence value</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co"># optimal_model = model_list[coherence_values.index(max(coherence_values))]</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># optimal_num_topics = x[coherence_values.index(max(coherence_values))]</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># print(f"Optimal number of topics: {optimal_num_topics}")</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co"># </span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co"># # Print the topics of the optimal model</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># topics = optimal_model.print_topics(num_words=10)</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co"># for topic in topics:</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co">#     print(topic)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The optimal number of topics is 4. In the next session, we will explore the lda model with 4 topics.</p>
<div id="1301a9fe" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.models.ldamodel <span class="im">import</span> LdaModel</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Build the optimal LDA model</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>optimal_model <span class="op">=</span> LdaModel(corpus, num_topics<span class="op">=</span><span class="dv">4</span>, id2word<span class="op">=</span>dictionary, passes<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the topics</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, topic <span class="kw">in</span> lda_model.print_topics(<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Topic: </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">Words: </span><span class="sc">{</span>topic<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Topic: 0 
Words: 0.013*"one" + 0.012*"use" + 0.011*"student" + 0.010*"like" + 0.009*"example" + 0.007*"group" + 0.007*"class" + 0.007*"know" + 0.006*"course" + 0.006*"1"
Topic: 1 
Words: 0.238*"div" + 0.017*"class" + 0.014*"shoe" + 0.012*"u" + 0.011*"image" + 0.010*"http" + 0.009*"ltr" + 0.008*"dir" + 0.008*"market" + 0.007*"go"
Topic: 2 
Words: 0.051*"student" + 0.035*"question" + 0.018*"would" + 0.017*"think" + 0.014*"answer" + 0.013*"statistic" + 0.011*"need" + 0.010*"one" + 0.010*"get" + 0.009*"problem"
Topic: 3 
Words: 0.054*"statistic" + 0.038*"course" + 0.021*"stats" + 0.020*"teaching" + 0.017*"teach" + 0.016*"class" + 0.016*"ap" + 0.014*"teacher" + 0.013*"resource" + 0.013*"math"
Topic: 4 
Words: 0.075*"http" + 0.044*"www" + 0.040*"com" + 0.035*"href" + 0.027*"org" + 0.017*"target" + 0.017*"_blank" + 0.016*"amp" + 0.015*"data" + 0.015*"online"
Topic: 5 
Words: 0.075*"value" + 0.055*"mean" + 0.042*"p" + 0.028*"standard" + 0.024*"test" + 0.021*"deviation" + 0.018*"hypothesis" + 0.016*"difference" + 0.012*"probability" + 0.012*"calculate"
Topic: 6 
Words: 0.043*"locus" + 0.035*"item" + 0.028*"resource" + 0.022*"assessment" + 0.018*"website" + 0.017*"org" + 0.016*"pdf" + 0.016*"site" + 0.013*"http" + 0.013*"professional"
Topic: 7 
Words: 0.122*"thanks" + 0.068*"thank" + 0.061*"sharing" + 0.039*"plot" + 0.034*"box" + 0.031*"great" + 0.031*"idea" + 0.029*"resource" + 0.022*"link" + 0.020*"love"
Topic: 8 
Words: 0.016*"year" + 0.016*"8" + 0.015*"class" + 0.013*"time" + 0.013*"minute" + 0.013*"grade" + 0.011*"3" + 0.011*"5" + 0.010*"one" + 0.008*"standard"
Topic: 9 
Words: 0.025*"playlist" + 0.023*"dr" + 0.017*"ltr" + 0.015*"dir" + 0.014*"moodle" + 0.014*"14" + 0.011*"channel" + 0.011*"decoration" + 0.011*"list" + 0.011*"baseline"
Topic: 10 
Words: 0.032*"li" + 0.027*"course" + 0.027*"strong" + 0.021*"http" + 0.021*"href" + 0.020*"ncsu" + 0.020*"b" + 0.019*"edu" + 0.018*"target" + 0.018*"fi"
Topic: 11 
Words: 0.045*"span" + 0.044*"quot" + 0.036*"style" + 0.035*"font" + 0.021*"p" + 0.017*"family" + 0.017*"size" + 0.014*"left" + 0.014*"10" + 0.013*"25in"
Topic: 12 
Words: 0.117*"span" + 0.086*"td" + 0.070*"style" + 0.057*"0" + 0.051*"height" + 0.042*"right" + 0.040*"line" + 0.036*"1" + 0.031*"top" + 0.031*"width"
Topic: 13 
Words: 0.098*"school" + 0.088*"level" + 0.051*"high" + 0.042*"middle" + 0.037*"student" + 0.024*"grade" + 0.018*"elementary" + 0.014*"test" + 0.012*"would" + 0.011*"statistical"
Topic: 14 
Words: 0.042*"student" + 0.022*"data" + 0.018*"school" + 0.013*"statistic" + 0.013*"think" + 0.011*"would" + 0.010*"video" + 0.010*"way" + 0.009*"like" + 0.009*"also"
Topic: 15 
Words: 0.061*"sample" + 0.025*"size" + 0.024*"simulation" + 0.020*"trial" + 0.018*"dice" + 0.017*"population" + 0.017*"number" + 0.016*"result" + 0.015*"fair" + 0.014*"sampling"
Topic: 16 
Words: 0.048*"student" + 0.034*"task" + 0.019*"would" + 0.014*"technology" + 0.014*"think" + 0.012*"statistical" + 0.012*"group" + 0.011*"could" + 0.009*"different" + 0.009*"activity"
Topic: 17 
Words: 0.085*"data" + 0.049*"student" + 0.020*"would" + 0.018*"question" + 0.013*"could" + 0.011*"time" + 0.011*"set" + 0.010*"think" + 0.010*"task" + 0.009*"also"
Topic: 18 
Words: 0.039*"language" + 0.034*"english" + 0.033*"science" + 0.021*"art" + 0.020*"math" + 0.016*"conclusion" + 0.015*"stick" + 0.013*"field" + 0.013*"scientific" + 0.011*"chemistry"
Topic: 19 
Words: 0.037*"agree" + 0.021*"excellent" + 0.019*"supposed" + 0.019*"growth" + 0.016*"literacy" + 0.014*"dear" + 0.014*"dung" + 0.013*"equal" + 0.011*"onto" + 0.011*"caffeine"
Topic: 20 
Words: 0.064*"font" + 0.058*"normal" + 0.055*"text" + 0.046*"0px" + 0.037*"color" + 0.034*"style" + 0.028*"rgb" + 0.026*"255" + 0.025*"51" + 0.023*"none"
Topic: 21 
Words: 0.044*"coaster" + 0.036*"chart" + 0.023*"roller" + 0.020*"pie" + 0.020*"steel" + 0.019*"bar" + 0.019*"speed" + 0.018*"yes" + 0.013*"totally" + 0.011*"height"</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="explore" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="explore"><span class="header-section-number">5</span> EXPLORE</h2>
<p>Silge and Robinson (2018) note that fitting at topic model is the “easy part.” The hard part is making sense of the model results and that the rest of the analysis involves exploring and interpreting the model using a variety of approaches which we’ll walkthrough in in this section.</p>
<p>Bail (2018) cautions, however, that:</p>
<blockquote class="blockquote">
<p><em>…post-hoc interpretation of topic models is rather dangerous… and can quickly come to resemble the process of “reading tea leaves,” or finding meaning in patterns that are in fact quite arbitrary or even random.</em></p>
</blockquote>
<section id="a.-visualizing-topics-by-pyldavis" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="a.-visualizing-topics-by-pyldavis"><span class="header-section-number">5.1</span> a. Visualizing Topics by pyLDAvis</h3>
<p>Visualization is always a good way to explore the modelling results. The <code>pyLDAvis</code> is primarily designed for visualizing the results of LDA topic modeling, it provides an interactive dashboard that helps users interpret the topics discovered by LDA.</p>
<p>Let’s use to <code>pyLDAvis</code> to create one.</p>
<div id="3096355f" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyLDAvis</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyLDAvis.gensim</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare the visualization</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>vis <span class="op">=</span> pyLDAvis.gensim.prepare(optimal_model, corpus, dictionary)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the visualization</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>pyLDAvis.display(vis)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">

<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css">


<div id="ldavis_el7331755316257124469072671" style="background-color:white;"></div>
<script type="text/javascript">

var ldavis_el7331755316257124469072671_data = {"mdsDat": {"x": [-0.1764303595409703, -0.09834306191078483, -0.06211710469268949, 0.33689052614444454], "y": [0.05958565050706661, 0.15977369224586582, -0.25093614384106383, 0.03157680108813146], "topics": [1, 2, 3, 4], "cluster": [1, 1, 1, 1], "Freq": [59.73295256239395, 16.431706142051876, 12.71298684498478, 11.122354450569395]}, "tinfo": {"Term": ["font", "span", "style", "normal", "text", "http", "course", "0px", "statistic", "href", "color", "li", "height", "rgb", "strong", "_blank", "target", "td", "line", "255", "resource", "51", "data", "edu", "student", "none", "div", "ncsu", "unit", "sample", "math", "concept", "struggle", "algebra", "rather", "semester", "stats", "approach", "calculus", "meaningful", "analyzing", "incorporate", "basic", "presented", "calculation", "helped", "improve", "realize", "exam", "interpreting", "faithful", "posing", "mode", "elementary", "language", "apply", "senior", "television", "meaning", "beneficial", "agree", "ap", "grade", "feel", "knowledge", "need", "problem", "teach", "question", "level", "answer", "school", "thinking", "college", "high", "student", "think", "understand", "technology", "often", "year", "middle", "topic", "way", "understanding", "process", "statistic", "really", "well", "know", "would", "statistical", "teacher", "like", "data", "use", "time", "also", "class", "help", "make", "idea", "task", "course", "get", "great", "one", "teaching", "many", "dice", "distribution", "die", "roll", "trial", "car", "null", "bus", "water", "square", "60", "brand", "boy", "city", "coke", "rolling", "pluginfile", "girl", "mpg", "pepsi", "coin", "movie", "mileage", "pie", "randomly", "fuel", "vegetarian", "distance", "responsive", "slope", "drink", "sample", "shoe", "img", "population", "regression", "cost", "fair", "v", "coaster", "hypothesis", "chi", "company", "random", "variable", "value", "average", "simulation", "compare", "group", "experiment", "number", "correlation", "result", "sampling", "difference", "plot", "data", "size", "two", "one", "mean", "could", "graph", "would", "student", "activity", "see", "class", "different", "use", "using", "time", "also", "like", "used", "task", "example", "_blank", "edu", "ncsu", "fi", "page", "php", "em", "tsdi", "forum", "mod", "hollylynne", "facebook", "certificate", "youtube", "please", "hirise", "href", "ul", "technical", "dear", "smile", "join", "ced", "li", "mailto", "strong", "http", "institute", "h6", "contact", "target", "org", "email", "com", "www", "div", "id", "place", "pdf", "link", "view", "b", "course", "education", "unit", "resource", "online", "statistic", "learning", "teaching", "data", "mooc", "font", "style", "normal", "text", "0px", "rgb", "td", "255", "51", "none", "spacing", "align", "family", "margin", "arial", "sans", "serif", "border", "indent", "quot", "letter", "variant", "webkit", "transform", "auto", "stroke", "orphan", "widow", "14px", "universroman", "span", "color", "white", "space", "width", "height", "0", "line", "background", "size", "1"], "Freq": [1207.0, 1152.0, 1078.0, 959.0, 940.0, 998.0, 1816.0, 761.0, 3000.0, 668.0, 602.0, 633.0, 579.0, 465.0, 500.0, 481.0, 509.0, 432.0, 547.0, 423.0, 806.0, 411.0, 4160.0, 422.0, 7068.0, 382.0, 474.0, 393.0, 641.0, 530.0, 694.0529641117805, 475.5611364339566, 192.18251913671895, 182.31006949112555, 169.6207198726932, 156.3242017017986, 759.7802079307003, 166.1740753170329, 121.87858122930581, 120.42022927479067, 119.7138249123136, 109.6371013088549, 162.1003312250125, 102.55094733091896, 136.30277397934478, 99.15236997521048, 110.06346603402316, 96.84383116783734, 91.82595450586656, 94.55214056133862, 89.12987913775046, 88.74478272550343, 89.1409250007892, 133.48128242937423, 84.14552192019482, 89.26542845534367, 75.08060775327237, 79.95909326080049, 81.30385723418559, 72.00788335720614, 788.7725249378611, 544.4035371548484, 377.92636186586606, 514.892120466131, 180.17417584660973, 808.9270338887806, 496.77511688220545, 786.9117533794868, 2203.529345237778, 851.8287933894146, 672.6421698543122, 1463.9081912674098, 457.9398741635428, 370.3570584857407, 516.8002793357541, 6391.434238590874, 1811.8504182115119, 442.54577826764444, 489.4417069414095, 238.6666401543963, 740.1729628180229, 353.3586115215034, 491.74755413460207, 905.3953802216524, 449.3267832939778, 263.6316876534855, 2534.507361804937, 889.454542045969, 671.1023057637811, 644.1191809502266, 2065.802148544264, 814.8672121738529, 855.7325094244073, 1251.8232994396233, 3072.5479602910254, 1131.6244903569764, 1090.0490540128499, 1055.2062667752448, 1217.8895885928137, 663.4473177799828, 781.5403718461015, 719.451445315841, 881.2049774210308, 1168.443835130441, 772.9716879537443, 750.2695149563968, 860.8270924337533, 737.7528570685412, 722.1664551186601, 157.85620763742716, 165.53504216543462, 76.84883985325047, 71.44668881597535, 150.85495742066084, 110.04904661725014, 51.3375415060321, 49.741680728222036, 47.37006902618529, 67.26384212812098, 45.373220333885335, 44.41301373922052, 52.35639801468756, 39.26479541098208, 155.67624065259852, 37.41946753658009, 35.41022119733794, 41.777592715424994, 40.95602506867977, 149.20372099891432, 35.99630785095423, 34.672149999000666, 32.437862925213885, 34.848594212115344, 29.026680729085104, 28.36497329697206, 33.17654644791966, 28.356203479761245, 27.66538117683789, 27.021984549167055, 53.94137530330349, 491.0993107368598, 54.0695586204789, 70.284456172131, 157.2216448776333, 115.96362860408264, 59.49013575339533, 106.58333168395157, 227.54434698354257, 73.12235672382246, 148.20107699487764, 61.596536705097684, 64.6105048519868, 89.27172788196485, 159.48811613015286, 251.09880012251872, 72.9893004319655, 182.53164991529303, 110.67311531590012, 290.88256477505996, 150.43179260351926, 193.19732585205693, 75.34549898169718, 242.85202577563183, 114.31157233380924, 155.0453364668558, 148.33403592972203, 847.5434200272304, 238.64427618877394, 192.0866155640728, 392.68682517810083, 211.18344978141562, 327.33473146519117, 181.34703028261197, 471.7004648184329, 574.431051575766, 227.16621234673906, 234.21192909798125, 266.2198199012539, 202.96000012511806, 247.78341466472753, 209.96466296137743, 225.9166402145171, 219.72762418142295, 220.97299458994476, 186.8827910094997, 190.72810605884357, 159.93379603746348, 480.92167182056846, 422.0602352940745, 392.9276565294341, 340.50813272444026, 227.21824720913423, 187.83738284875724, 161.8932522818892, 160.92802942134017, 133.37601339831505, 103.23034548534604, 104.17118722366179, 96.48896122343933, 94.49268250790138, 87.77414910592191, 172.02065508427017, 74.06977005644359, 661.9942645300752, 146.35330377428625, 65.09796505294713, 63.96530929542258, 60.029209028854616, 55.07380341889111, 53.1375894810696, 623.838371181259, 48.87485353595809, 493.1237455841851, 982.0002441735944, 45.45686599228233, 41.595177703652304, 41.58726474105433, 493.80851021525933, 307.0977651494656, 60.58101923776027, 329.3846005202208, 324.9576864588416, 424.55816253727954, 183.5611163719088, 236.42341721429912, 90.17438535496886, 179.68024430271183, 160.06990906457227, 386.56366825323755, 647.5671100852993, 191.96942408546812, 298.2122217457535, 331.8665115465441, 177.14982475416963, 461.54176395619896, 205.7733256865796, 200.92889580908368, 197.61253988322449, 163.40941714552775, 1206.4117695064137, 1077.9841774126908, 959.0738427499101, 939.5718446425542, 760.9497374101286, 464.5987672974575, 431.4265746252041, 422.315146369728, 410.72443093283204, 381.37223786708796, 356.9616656822876, 350.3616280101194, 338.15065376094566, 277.0273304040502, 260.9118666099495, 249.63907751603824, 240.02581509759995, 234.7470946232786, 231.33898693494305, 198.8492620870962, 193.85761317365106, 188.91756000420833, 181.5434058158131, 182.78472810644888, 178.27603145018819, 169.11732212572568, 167.60635345691526, 167.60635345691526, 143.30192782693146, 135.2278806314281, 1123.2437032288342, 584.9241488828713, 235.72623450731095, 211.9297196565499, 364.6043841117152, 488.5460737969313, 325.7845454409369, 431.0967725207604, 234.51518430955318, 315.31797626514066, 336.3733406195999], "Total": [1207.0, 1152.0, 1078.0, 959.0, 940.0, 998.0, 1816.0, 761.0, 3000.0, 668.0, 602.0, 633.0, 579.0, 465.0, 500.0, 481.0, 509.0, 432.0, 547.0, 423.0, 806.0, 411.0, 4160.0, 422.0, 7068.0, 382.0, 474.0, 393.0, 641.0, 530.0, 695.3520160077669, 476.8978727933953, 192.98186058001656, 183.12349973812064, 170.43661746092994, 157.125791187101, 763.7359575895599, 167.11243758478165, 122.70288364397626, 121.24853863014017, 120.53940632279412, 110.44640716071369, 163.3777396922001, 103.36521054058719, 137.42062791077873, 99.97308907301755, 110.9817753470497, 97.65311077382867, 92.61894589966624, 95.38442234379205, 89.9298831655813, 89.56016033782217, 89.97692799905666, 134.74109831938904, 84.96409937344607, 90.20467672082896, 75.88490806837447, 80.81703839104374, 82.17819725553248, 72.80619406705283, 801.2013330168036, 552.9510179082649, 383.0011575457577, 526.8976200395391, 182.78142102869245, 833.2561622960355, 509.4187053743166, 810.8564383211657, 2325.577046244228, 883.5229651617548, 694.5883855633957, 1543.7028558471127, 472.93523086433777, 381.38752171036634, 535.8591331527849, 7068.8229340364205, 1953.7902420175121, 458.9373196751096, 510.0704448547756, 244.58569884147371, 785.7949319527457, 366.66303737298085, 519.0850015939742, 983.571961933806, 474.5181993836059, 271.22926260996996, 3000.471343487671, 985.5318443427155, 736.4800014420899, 704.9742445155957, 2577.4106989094353, 918.4045068828358, 971.3142703924149, 1497.390036498252, 4160.917558230081, 1425.5424953441689, 1364.904347339707, 1318.9304625406546, 1655.5539236790992, 750.6349817523567, 946.863153491656, 857.5410364890282, 1164.1699236399036, 1816.9150660098348, 967.2397716506767, 938.7061515502378, 1289.4979035590802, 939.6564745338677, 909.7196206824701, 158.7290684073115, 166.8734484227497, 77.66628930879797, 72.23980850832761, 152.86840074614915, 111.74964221954369, 52.143777627732796, 50.55893759538812, 48.159871044069725, 68.44775818112473, 46.177887247650325, 45.211122479193996, 53.397631649026835, 40.09968014070316, 159.0003029471365, 38.226413326492825, 36.200356339098676, 42.73199297246888, 41.89167117796209, 152.64338830143512, 36.82881942717682, 35.47827550526558, 33.25392169635933, 35.75511279995182, 29.818389660964755, 29.160492003456355, 34.10881595332211, 29.15741288973898, 28.45464725749046, 27.831606862666916, 55.72610292942415, 530.8967790148392, 56.34823111548912, 74.18518515424336, 170.14196431573853, 126.54498859816985, 62.70110077706636, 117.10389139065316, 263.3109235481422, 79.21159089535152, 172.02892647925165, 67.08697933681849, 71.38084320583141, 104.28752025144637, 209.16761895171098, 361.6297329362915, 82.71263029780523, 294.3251033461858, 150.66389874258311, 594.2957830273267, 244.61566737318338, 359.3252238143845, 90.9901133049744, 521.7350939839484, 170.2025555784414, 270.61591890435125, 258.66763254409864, 4160.917558230081, 557.0770987520895, 393.8368176599754, 1289.4979035590802, 461.7070679228301, 1055.2285285392816, 387.96060589369955, 2577.4106989094353, 7068.8229340364205, 828.2477223439474, 938.0040856313387, 1655.5539236790992, 732.6256239936235, 1425.5424953441689, 842.3527781211845, 1364.904347339707, 1318.9304625406546, 1497.390036498252, 651.8780280181873, 1164.1699236399036, 440.7179190092923, 481.6898481109016, 422.825314335934, 393.68392649271624, 341.2649879020931, 228.00036734139135, 188.6016039343502, 162.64808591739336, 161.68263256022783, 134.15095983412112, 103.98615302398922, 104.96161081669872, 97.25127556136455, 95.24886650107479, 88.59502625313203, 173.6996881437066, 74.82419820492598, 668.8435880965001, 147.96618944184237, 65.87259279521774, 64.75813748746732, 60.784970269739375, 55.834605795245444, 53.891746031057686, 633.0988865483066, 49.6293893006407, 500.75062957471414, 998.4936004791552, 46.23380638824502, 42.3505865266859, 42.35337440937642, 509.06832759809896, 318.5779501637679, 61.82468099860012, 348.9778800255804, 349.92433622830396, 474.42217987282595, 200.9494503837815, 297.2331477646218, 97.267076567529, 223.0923746643417, 199.3127085907445, 651.1955124499243, 1816.9150660098348, 290.286631637367, 641.6881495712383, 806.6617354530854, 268.47160843514644, 3000.471343487671, 698.7290612946881, 939.6564745338677, 4160.917558230081, 300.13688912538555, 1207.1731201300552, 1078.7588496931596, 959.842587661993, 940.372485195931, 761.7101882259486, 465.3596336007182, 432.18644175001344, 423.0760320644537, 411.48598873396514, 382.15179328761303, 357.7223245853289, 351.12911974061245, 338.95344426909446, 277.8328753337422, 261.6729375328678, 250.40050971987066, 240.78695692361075, 235.5071156183135, 232.10217087559187, 199.61179274886652, 194.62972937524574, 189.67825445744214, 182.30462895244497, 183.55281465595647, 179.04009289555768, 169.88022507989857, 168.36712612588985, 168.3671264101887, 144.06530025895964, 135.98870798109826, 1152.8720481531502, 602.6574840635727, 238.5250766438529, 214.92298869630685, 393.9683194688508, 579.8263418489695, 370.3036951358304, 547.5803942722823, 284.82996879116695, 557.0770987520895, 717.6520041581527], "Category": ["Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Default", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic1", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic2", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic3", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4", "Topic4"], "logprob": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.4404, -5.8185, -6.7245, -6.7773, -6.8494, -6.931, -5.3499, -6.8699, -7.1799, -7.192, -7.1979, -7.2858, -6.8947, -7.3526, -7.0681, -7.3863, -7.2819, -7.4099, -7.4631, -7.4338, -7.4929, -7.4972, -7.4927, -7.089, -7.5504, -7.4913, -7.6644, -7.6014, -7.5848, -7.7062, -5.3125, -5.6833, -6.0483, -5.739, -6.789, -5.2873, -5.7748, -5.3148, -4.2851, -5.2356, -5.4717, -4.6941, -5.8562, -6.0685, -5.7353, -3.2202, -4.4809, -5.8904, -5.7897, -6.5079, -5.3761, -6.1155, -5.785, -5.1746, -5.8752, -6.4084, -4.1452, -5.1924, -5.474, -5.5151, -4.3497, -5.2799, -5.231, -4.8506, -3.9527, -4.9516, -4.989, -5.0215, -4.8781, -5.4855, -5.3217, -5.4045, -5.2017, -4.9195, -5.3327, -5.3625, -5.2251, -5.3794, -5.4007, -5.6306, -5.5831, -6.3505, -6.4233, -5.676, -5.9914, -6.7539, -6.7854, -6.8343, -6.4837, -6.8774, -6.8988, -6.7342, -7.022, -5.6445, -7.0701, -7.1253, -6.9599, -6.9798, -5.687, -7.1089, -7.1464, -7.213, -7.1413, -7.3241, -7.3471, -7.1904, -7.3474, -7.3721, -7.3956, -6.7044, -4.4956, -6.702, -6.4397, -5.6346, -5.939, -6.6065, -6.0234, -5.2649, -6.4002, -5.6937, -6.5717, -6.5239, -6.2006, -5.6203, -5.1664, -6.402, -5.4854, -5.9857, -5.0194, -5.6788, -5.4286, -6.3702, -5.1998, -5.9534, -5.6486, -5.6928, -3.95, -5.2173, -5.4343, -4.7193, -5.3396, -4.9013, -5.4919, -4.5359, -4.3389, -5.2666, -5.2361, -5.108, -5.3793, -5.1797, -5.3454, -5.2721, -5.2999, -5.2943, -5.4618, -5.4414, -5.6175, -4.26, -4.3906, -4.4621, -4.6053, -5.0098, -5.2001, -5.3488, -5.3547, -5.5425, -5.7987, -5.7897, -5.8663, -5.8872, -5.9609, -5.2881, -6.1307, -3.9404, -5.4497, -6.2598, -6.2774, -6.3409, -6.427, -6.4628, -3.9998, -6.5464, -4.2349, -3.5461, -6.6189, -6.7077, -6.7079, -4.2336, -4.7085, -6.3317, -4.6385, -4.652, -4.3847, -5.2232, -4.9701, -5.934, -5.2445, -5.3601, -4.4784, -3.9625, -5.1784, -4.7379, -4.631, -5.2587, -4.3011, -5.1089, -5.1328, -5.1494, -5.3394, -3.2066, -3.3192, -3.4361, -3.4566, -3.6675, -4.1609, -4.2349, -4.2563, -4.2841, -4.3583, -4.4244, -4.4431, -4.4785, -4.6779, -4.7379, -4.782, -4.8213, -4.8435, -4.8582, -5.0095, -5.0349, -5.0607, -5.1005, -5.0937, -5.1187, -5.1714, -5.1804, -5.1804, -5.3371, -5.3951, -3.2781, -3.9306, -4.8394, -4.9458, -4.4032, -4.1106, -4.5158, -4.2357, -4.8445, -4.5485, -4.4838], "loglift": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.5134, 0.5125, 0.5111, 0.5108, 0.5105, 0.5102, 0.5101, 0.5097, 0.5085, 0.5084, 0.5084, 0.5079, 0.5074, 0.5074, 0.5071, 0.507, 0.507, 0.507, 0.5067, 0.5065, 0.5064, 0.5061, 0.506, 0.5059, 0.5056, 0.5048, 0.5046, 0.5046, 0.5046, 0.5043, 0.4997, 0.4997, 0.5019, 0.4922, 0.5009, 0.4857, 0.4902, 0.4853, 0.4614, 0.4788, 0.4832, 0.4622, 0.4831, 0.4859, 0.4791, 0.4146, 0.4399, 0.4789, 0.474, 0.4908, 0.4555, 0.4783, 0.4612, 0.4325, 0.4607, 0.4869, 0.3465, 0.4127, 0.4223, 0.425, 0.294, 0.3957, 0.3886, 0.3362, 0.2121, 0.2844, 0.2904, 0.2922, 0.2083, 0.3918, 0.3234, 0.3397, 0.2368, 0.0738, 0.2911, 0.2912, 0.1112, 0.2734, 0.2844, 1.8004, 1.7979, 1.7954, 1.7949, 1.7927, 1.7906, 1.7904, 1.7897, 1.7894, 1.7885, 1.7884, 1.7881, 1.7863, 1.7849, 1.7848, 1.7846, 1.7839, 1.7834, 1.7834, 1.7832, 1.7831, 1.783, 1.7811, 1.7803, 1.779, 1.7783, 1.7782, 1.7781, 1.7778, 1.7764, 1.7734, 1.728, 1.7647, 1.7519, 1.727, 1.7186, 1.7534, 1.7118, 1.66, 1.726, 1.6569, 1.7206, 1.7063, 1.6505, 1.5348, 1.4412, 1.6809, 1.3282, 1.4975, 1.0915, 1.3198, 1.1854, 1.6173, 1.0412, 1.4079, 1.249, 1.2499, 0.2148, 0.9582, 1.088, 0.617, 1.0238, 0.6354, 1.0455, 0.1078, -0.7041, 0.5123, 0.4184, -0.0216, 0.5223, 0.0562, 0.4167, 0.0073, 0.0138, -0.1075, 0.5566, -0.003, 0.7923, 2.061, 2.0607, 2.0606, 2.0603, 2.0591, 2.0585, 2.0579, 2.0579, 2.0568, 2.0553, 2.055, 2.0547, 2.0546, 2.0532, 2.0528, 2.0524, 2.0523, 2.0516, 2.0507, 2.0502, 2.05, 2.0488, 2.0485, 2.0478, 2.0472, 2.0472, 2.0459, 2.0456, 2.0445, 2.0443, 2.0321, 2.0258, 2.0422, 2.0048, 1.9885, 1.9515, 1.972, 1.8337, 1.9868, 1.8461, 1.8433, 1.541, 1.0309, 1.649, 1.2962, 1.1744, 1.6468, 0.1906, 0.8401, 0.52, -0.9846, 1.4546, 2.1956, 2.1955, 2.1954, 2.1954, 2.1952, 2.1946, 2.1945, 2.1944, 2.1944, 2.1942, 2.1941, 2.194, 2.1938, 2.1933, 2.1933, 2.1932, 2.193, 2.193, 2.1929, 2.1924, 2.1922, 2.1922, 2.192, 2.192, 2.1919, 2.1917, 2.1917, 2.1917, 2.1909, 2.1906, 2.1702, 2.1663, 2.1844, 2.1822, 2.1188, 2.0249, 2.0681, 1.957, 2.0018, 1.6271, 1.4385]}, "token.table": {"Topic": [2, 3, 4, 4, 1, 2, 3, 4, 4, 4, 4, 2, 3, 1, 2, 3, 4, 1, 2, 4, 1, 4, 1, 2, 3, 4, 1, 1, 2, 4, 1, 2, 1, 1, 4, 4, 1, 2, 1, 2, 3, 4, 1, 4, 1, 2, 1, 4, 2, 2, 2, 1, 4, 1, 1, 2, 3, 3, 1, 2, 2, 1, 2, 3, 4, 2, 3, 2, 1, 2, 1, 2, 3, 4, 2, 4, 2, 3, 4, 1, 2, 4, 1, 2, 1, 2, 3, 1, 2, 4, 1, 2, 1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 4, 3, 2, 2, 1, 2, 3, 1, 2, 3, 4, 2, 1, 2, 3, 4, 2, 3, 3, 1, 3, 4, 1, 4, 3, 3, 4, 1, 1, 2, 3, 4, 1, 2, 4, 3, 1, 2, 4, 1, 4, 1, 3, 3, 4, 3, 2, 1, 2, 3, 2, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 4, 3, 2, 4, 1, 2, 3, 4, 1, 1, 2, 4, 3, 3, 3, 4, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 1, 1, 4, 3, 1, 3, 1, 2, 4, 1, 3, 4, 1, 1, 3, 4, 4, 1, 2, 4, 3, 4, 1, 2, 3, 1, 2, 4, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 4, 4, 1, 4, 1, 2, 4, 1, 1, 1, 3, 4, 2, 3, 1, 1, 3, 2, 2, 3, 1, 2, 3, 4, 4, 2, 1, 2, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 2, 3, 4, 4, 3, 3, 4, 1, 2, 3, 2, 1, 2, 3, 3, 4, 1, 2, 4, 2, 1, 2, 3, 4, 1, 1, 1, 2, 1, 2, 1, 2, 3, 4, 4, 1, 2, 3, 2, 1, 1, 1, 2, 4, 1, 2, 4, 1, 3, 4, 2, 1, 2, 4, 4, 2, 2, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 4, 1, 2, 3, 1, 1, 4, 1, 2, 1, 2, 1, 2, 3, 4, 2, 3, 1, 2, 4, 4, 3, 4, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 4, 4, 1, 3, 4, 1, 1, 2, 3, 4, 4, 3, 4, 1, 2, 3, 4, 4, 1, 3, 1, 2, 3, 4, 1, 3, 4, 3, 1, 2, 3, 1, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 4, 4, 1, 2, 3, 1, 2, 3, 4, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 1, 2, 4, 2, 1, 2, 3, 4, 2, 1, 2, 3, 4, 4, 1, 2, 3, 4, 2, 4, 4, 2, 4, 1, 2, 3, 4, 2, 3, 4, 1, 2, 3], "Freq": [0.09181652909925331, 0.027004861499780385, 0.8803584848928405, 0.9990676398492152, 0.23130988144417933, 0.14770390019929522, 0.1532776322822875, 0.46819349497135093, 0.9926054347782238, 0.9974566461276403, 0.9988189422063667, 0.9744923962992642, 0.9985678583146249, 0.6918220051102653, 0.2740725919023215, 0.0325989426491748, 0.0012073682462657336, 0.9847712023008482, 0.013729383048554285, 0.0012481257316867532, 0.9938647975834487, 0.9967843175711357, 0.799890540072715, 0.16680181878293582, 0.02881122324432528, 0.004549140512261886, 0.9955250623903884, 0.9689191670749218, 0.010077911098847626, 0.020155822197695253, 0.9838122770039824, 0.014467827602999741, 0.9866450746832421, 0.993343178994578, 0.9974283258360132, 0.9941907263410301, 0.108810467852318, 0.8825737948021348, 0.06603239607445333, 0.11363714673278014, 0.5942915646700799, 0.22727429346556027, 0.1755433257680102, 0.825053631109648, 0.991567151713595, 0.006120784887120956, 0.9889268478131086, 0.9978467078713011, 0.9738259618289212, 0.9732118467142383, 0.9889448310828607, 0.9896621931337624, 0.007276927890689429, 0.9942716615689677, 0.008948574511186327, 0.9843431962304959, 0.983453012813061, 0.9868883846396146, 0.07453011075214581, 0.9241733733266082, 0.972576336348705, 0.7357054231693443, 0.1606712993128453, 0.03020137205128671, 0.07369134780513957, 0.9215822984346089, 0.07574649028229663, 0.9774953571668058, 0.018867888578787316, 0.9811302060969405, 0.9701418608052566, 0.0026220050292033964, 0.02097604023362717, 0.0026220050292033964, 0.028208394402361253, 0.9707006309047842, 0.03438613358279438, 0.942753162394946, 0.022924089055196255, 0.056037443946490434, 0.9106084641304695, 0.042028082959867825, 0.25885431298066613, 0.7367391984834344, 0.9981172640000759, 0.002096885008403521, 0.9916565229027374, 0.15386287027773846, 0.8242653764878846, 0.010990205019838462, 0.047846049954792566, 0.9409723157775871, 0.6756830209916544, 0.3098854808755554, 0.012319606273340123, 0.0018953240420523266, 0.6428478809221772, 0.0005503834596936448, 0.35664848188148185, 0.7385390258265905, 0.20380120205042263, 0.047585658025924155, 0.010334259066236053, 0.9882927842448519, 0.9954068374833484, 0.9914211260158338, 0.3843085078699994, 0.5727674876908645, 0.04064801525548071, 0.6893015797716561, 0.27708558553197266, 0.03002897971282462, 0.00272990724662042, 0.960304678123679, 0.005992565081214388, 0.9947658034815884, 0.8958265823784333, 0.10328353538010172, 0.9690252352365243, 0.017944911763639337, 0.998048096204386, 0.3065935193019185, 0.6614152326513298, 0.03100383903053109, 0.9870781940988639, 0.007421640557134315, 0.9960154101184904, 0.9866609744639233, 0.016174770073179072, 0.9933172862889549, 0.6058296894308272, 0.3630440086476867, 0.02495927559452846, 0.0068070751621441255, 0.37610019418603163, 0.6132068383467908, 0.00817609117795721, 0.9871335820107058, 0.07685483285928063, 0.913718568438114, 0.008539425873253403, 0.9896599091108661, 0.997187093728608, 0.9774194841900287, 0.020876921021534593, 0.9992235127789637, 0.9990282088703825, 0.9914204129769605, 0.9602032776635318, 0.7991813639763902, 0.14164016412000704, 0.05893057923241169, 0.982870141981431, 0.9869421868649048, 0.002610958166309272, 0.010443832665237088, 0.5309817462663815, 0.46654221395250023, 0.7989720731683747, 0.06711365414614347, 0.13316201219472912, 0.34326341499653507, 0.48965516550976323, 0.16658371610125966, 0.0016826637990026228, 0.99172180233053, 0.15694354228511967, 0.843355957993665, 0.8832522012926004, 0.01865087604539428, 0.09192217479515752, 0.006661027159069385, 0.9902664898920266, 0.964805800655435, 0.02612626926339669, 0.009330810451213103, 0.9889848708746776, 0.9908384521805973, 0.9897680291501685, 0.008970707212841407, 0.0030045260165516992, 0.9834815160845894, 0.013019612738390696, 0.13369844520173776, 0.8603204299937908, 0.9156531637612805, 0.0845983901301183, 0.838443840476431, 0.0618046224551472, 0.0979544959666484, 0.0023322499039678188, 0.9435846234589608, 0.02695956067025602, 0.02695956067025602, 0.9911537246184826, 0.9959581558858306, 0.9952513547312635, 0.9733137614090386, 0.9959697575941018, 0.9850521771693691, 0.9135085501492434, 0.08085401763743302, 0.0056739661499953, 0.9847828022506958, 0.005471015568059421, 0.005471015568059421, 0.988652861849232, 0.698410910655091, 0.29482099916997695, 0.007155849494416916, 0.9967644748966812, 0.9643212837642725, 0.03169130979506999, 0.004527329970724284, 0.9856280168206988, 0.014215788704144694, 0.8361214977280648, 0.1475901365797942, 0.016027888135362264, 0.0748748502116986, 0.1369661894116438, 0.7870990351522462, 0.15240323678097656, 0.040342033265552614, 0.8068406653110524, 0.9873182138746853, 0.8258849202403684, 0.13623932827494567, 0.03802027765812437, 0.7936511245721587, 0.059358948375203, 0.14400041179910358, 0.0032977193541779445, 0.9970022434071499, 0.998055638041392, 0.0014381205159097867, 0.5284736079474145, 0.4569997183479691, 0.012995252654444619, 0.9856628972783512, 0.9897026500752413, 0.9627368019670813, 0.032727596667436194, 0.0027272997222863495, 0.9622925167200171, 0.9905164967131564, 0.9891424610643864, 0.4531265729991107, 0.5430855249915811, 0.9865192008784476, 0.9787148339302546, 0.9982627523079003, 0.9708899094977015, 0.012001111365855395, 0.016801555912197553, 0.9969860319699032, 0.9991221605783868, 0.9780649258690365, 0.46197703082973685, 0.5371178731936097, 0.9771626106189715, 0.020442732439727437, 0.6677017447051259, 0.30476978591070203, 0.020938382238139836, 0.0069794607460466115, 0.27935914876495194, 0.05587182975299039, 0.6592875910852866, 0.007449577300398719, 0.012555796777346842, 0.9636574026613701, 0.021972644360356973, 0.9978194904531699, 0.9956124310102822, 0.9252873960647544, 0.07196679747170312, 0.01965365177871772, 0.9761313716763134, 0.996810186542424, 0.9788809839818815, 0.16485375325232227, 0.03700798542399071, 0.7939895054601643, 0.9902147887433143, 0.005757062725251828, 0.41752421413447527, 0.5721628119620586, 0.007731929891379171, 0.9668413115093507, 0.03526466867906606, 0.922758830435562, 0.0058774447798443435, 0.03526466867906606, 0.9937454294888571, 0.9964667944013544, 0.9756218112069689, 0.023556261035178324, 0.9733463029010785, 0.025808424698134658, 0.9477217723486853, 0.04343008121924556, 0.003440006433207569, 0.0055900104539622995, 0.9969350871487026, 0.12465537552964975, 0.8534098786260637, 0.019177750081484576, 0.972554196579029, 0.9974382414563583, 0.9933119306834851, 0.90205101448843, 0.09030656950446599, 0.006088083337379729, 0.07902327947378411, 0.9166700418958957, 0.00790232794737841, 0.5863672208702518, 0.41157276390892944, 0.001239676999725691, 0.9840220385311303, 0.5290040926564379, 0.4657536033170812, 0.003833362990264043, 0.999227192101009, 0.9828375997399731, 0.9679171227491841, 0.026370474550588077, 0.9248502145956247, 0.030137685200672087, 0.018836053250420055, 0.3231443841314833, 0.6697901780179835, 0.9984005235439868, 0.9483690429507073, 0.027207308609241605, 0.020729377987993604, 0.0032389653106240004, 0.6588457443488054, 0.24946586436508167, 0.0916840356213548, 0.9928350961443343, 0.988338813462393, 0.9967317294355756, 0.03549357203247212, 0.9583264448767472, 0.37713398802222303, 0.6217614397123137, 0.003590167329585452, 0.4290249958854615, 0.001795083664792726, 0.5654513544097086, 0.9701200557060747, 0.987086112467342, 0.004652829397477961, 0.004652829397477961, 0.9863998322653277, 0.9979807673838466, 0.025154569448063827, 0.9740890169026095, 0.01460968228285615, 0.978848712951362, 0.8448672591064912, 0.0009998429101851967, 0.15397580816852027, 0.0006665619401234644, 0.8874085371882572, 0.03593187941989262, 0.050086862221668506, 0.02613227594174009, 0.99510831256217, 0.0013093530428449605, 0.003928059128534882, 0.9948185547818496, 0.003994003965004684, 0.9845219773736545, 0.011982011895014051, 0.994912161292955, 0.9041109191216689, 0.08120163786196806, 0.00862944235118476, 0.005941583258192785, 0.9992965529845939, 0.9704001864166353, 0.029465592704958563, 0.756762378163368, 0.1640653964009118, 0.07473135857004883, 0.0051538867979344025, 0.9972547918319481, 0.9705787150552087, 0.028365070452693517, 0.881280164507593, 0.0010295329024621414, 0.11118955346591126, 0.006177197414772848, 0.7853934070598484, 0.2139079604593896, 0.001064218708753182, 0.9867533255001754, 0.9586911081257127, 0.02940770270324272, 0.009802567567747573, 0.9898902705752419, 0.9996038961137264, 0.9274281143552557, 0.06141908042087786, 0.010236513403479643, 0.0005118256701739822, 0.9684201347464808, 0.010572272213389529, 0.01903008998410115, 0.0021144544426779056, 0.798590759949212, 0.1655793685766256, 0.02344486634713283, 0.012455085246914317, 0.9478216447965107, 0.030823468123463763, 0.02119113433488134, 0.9969882529069759, 0.006541574289513136, 0.9877777177164836, 0.9957779475171921, 0.4621203296364544, 0.48751155653955625, 0.033008594974032454, 0.015234736141861133, 0.9867119005412032, 0.006758300688638378, 0.965273428435953, 0.021789467910518126, 0.010894733955259063, 0.0021789467910518128, 0.9462229279788346, 0.010537003652325553, 0.03371841168744177, 0.010537003652325553, 0.5329691692584891, 0.0015583893837967518, 0.464400036371432, 0.992729484706659, 0.7940836584648436, 0.1739688580382343, 0.025253543908775947, 0.007014873307993319, 0.6857111005243588, 0.2868634805325617, 0.013806263768946822, 0.012272234461286065, 0.668366051164141, 0.2493017242352924, 0.07004191299943929, 0.011871510677871067, 0.05316908167480686, 0.8658964729897117, 0.0075955830964009804, 0.0759558309640098, 0.27929119428294696, 0.6940800966833632, 0.024887334144024977, 0.23426188166970663, 0.7601559017445582, 0.9964241844201792, 0.9674918075479512, 0.18062069526093297, 0.0050172415350259155, 0.8027586456041466, 0.010034483070051831, 0.97591623443077, 0.9201156956738322, 0.05998544314337691, 0.01626723881854289, 0.003050107278476792, 0.9983290114233773, 0.9110905913074699, 0.04888116436224876, 0.038018683392860145, 0.0013578101211735767, 0.00838486262384162, 0.9894137896133112, 0.9978194887682869, 0.07360997970369262, 0.9264704342016487, 0.8015796632155576, 0.18312952615573241, 0.01280354737953214, 0.0027159039895977267, 0.051439691774555846, 0.9287722125961472, 0.01714656392485195, 0.9417215228928205, 0.05726684936510395, 0.9932837510377621], "Term": ["0", "0", "0", "0px", "1", "1", "1", "1", "14px", "255", "51", "60", "_blank", "activity", "activity", "activity", "activity", "agree", "agree", "agree", "algebra", "align", "also", "also", "also", "also", "analyzing", "answer", "answer", "answer", "ap", "ap", "apply", "approach", "arial", "auto", "average", "average", "b", "b", "b", "b", "background", "background", "basic", "basic", "beneficial", "border", "boy", "brand", "bus", "calculation", "calculation", "calculus", "car", "car", "ced", "certificate", "chi", "chi", "city", "class", "class", "class", "class", "coaster", "coaster", "coin", "coke", "coke", "college", "college", "college", "college", "color", "color", "com", "com", "com", "company", "company", "company", "compare", "compare", "concept", "concept", "contact", "correlation", "correlation", "correlation", "cost", "cost", "could", "could", "could", "could", "course", "course", "course", "data", "data", "data", "data", "dear", "dice", "die", "difference", "difference", "difference", "different", "different", "different", "different", "distance", "distribution", "distribution", "div", "div", "drink", "drink", "edu", "education", "education", "education", "elementary", "elementary", "em", "email", "email", "exam", "example", "example", "example", "example", "experiment", "experiment", "experiment", "facebook", "fair", "fair", "fair", "faithful", "family", "feel", "feel", "fi", "font", "forum", "fuel", "get", "get", "get", "girl", "grade", "grade", "grade", "graph", "graph", "great", "great", "great", "group", "group", "group", "group", "h6", "height", "height", "help", "help", "help", "help", "helped", "high", "high", "high", "hirise", "hollylynne", "href", "href", "http", "http", "http", "hypothesis", "hypothesis", "id", "id", "idea", "idea", "idea", "idea", "img", "img", "img", "improve", "incorporate", "indent", "institute", "interpreting", "join", "know", "know", "know", "knowledge", "knowledge", "knowledge", "language", "learning", "learning", "learning", "letter", "level", "level", "level", "li", "li", "like", "like", "like", "line", "line", "line", "link", "link", "link", "mailto", "make", "make", "make", "many", "many", "many", "many", "margin", "math", "math", "mean", "mean", "mean", "meaning", "meaningful", "middle", "middle", "middle", "mileage", "mod", "mode", "mooc", "mooc", "movie", "mpg", "ncsu", "need", "need", "need", "none", "normal", "null", "number", "number", "often", "often", "one", "one", "one", "one", "online", "online", "online", "online", "org", "org", "org", "orphan", "page", "pdf", "pdf", "pepsi", "pepsi", "php", "pie", "place", "place", "place", "please", "please", "plot", "plot", "plot", "pluginfile", "population", "population", "population", "population", "posing", "presented", "problem", "problem", "process", "process", "question", "question", "question", "question", "quot", "random", "random", "random", "randomly", "rather", "realize", "really", "really", "really", "regression", "regression", "regression", "resource", "resource", "resource", "responsive", "result", "result", "result", "rgb", "roll", "rolling", "sample", "sample", "sample", "sample", "sampling", "sampling", "sans", "school", "school", "school", "school", "see", "see", "see", "semester", "senior", "serif", "shoe", "shoe", "simulation", "simulation", "size", "size", "size", "size", "slope", "smile", "space", "space", "space", "spacing", "span", "span", "square", "square", "statistic", "statistic", "statistic", "statistic", "statistical", "statistical", "statistical", "statistical", "stats", "stats", "stats", "stroke", "strong", "strong", "strong", "struggle", "student", "student", "student", "student", "style", "target", "target", "task", "task", "task", "task", "td", "teach", "teach", "teacher", "teacher", "teacher", "teacher", "teaching", "teaching", "teaching", "technical", "technology", "technology", "technology", "television", "text", "think", "think", "think", "think", "thinking", "thinking", "thinking", "thinking", "time", "time", "time", "time", "topic", "topic", "topic", "transform", "trial", "trial", "tsdi", "two", "two", "two", "two", "ul", "ul", "understand", "understand", "understand", "understand", "understanding", "understanding", "understanding", "understanding", "unit", "unit", "unit", "universroman", "use", "use", "use", "use", "used", "used", "used", "used", "using", "using", "using", "using", "v", "v", "v", "v", "value", "value", "value", "variable", "variable", "variant", "vegetarian", "view", "view", "view", "view", "water", "way", "way", "way", "way", "webkit", "well", "well", "well", "well", "white", "white", "widow", "width", "width", "would", "would", "would", "would", "www", "www", "www", "year", "year", "youtube"]}, "R": 30, "lambda.step": 0.01, "plot.opts": {"xlab": "PC1", "ylab": "PC2"}, "topic.order": [3, 1, 4, 2]};

function LDAvis_load_lib(url, callback){
  var s = document.createElement('script');
  s.src = url;
  s.async = true;
  s.onreadystatechange = s.onload = callback;
  s.onerror = function(){console.warn("failed to load library " + url);};
  document.getElementsByTagName("head")[0].appendChild(s);
}

if(typeof(LDAvis) !== "undefined"){
   // already loaded: just create the visualization
   !function(LDAvis){
       new LDAvis("#" + "ldavis_el7331755316257124469072671", ldavis_el7331755316257124469072671_data);
   }(LDAvis);
}else if(typeof define === "function" && define.amd){
   // require.js is available: use it to load d3/LDAvis
   require.config({paths: {d3: "https://d3js.org/d3.v5"}});
   require(["d3"], function(d3){
      window.d3 = d3;
      LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js", function(){
        new LDAvis("#" + "ldavis_el7331755316257124469072671", ldavis_el7331755316257124469072671_data);
      });
    });
}else{
    // require.js not available: dynamically load d3 & LDAvis
    LDAvis_load_lib("https://d3js.org/d3.v5.js", function(){
         LDAvis_load_lib("https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js", function(){
                 new LDAvis("#" + "ldavis_el7331755316257124469072671", ldavis_el7331755316257124469072671_data);
            })
         });
}
</script>
</div>
</div>
</section>
<section id="b.-finding-the-dominant-topic-for-each-document" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="b.-finding-the-dominant-topic-for-each-document"><span class="header-section-number">5.2</span> b. Finding the Dominant Topic for Each Document</h3>
<p>One of the practical applications of topic modeling is to determine the main topic of a given document. Let’s explore it by using <code>get_document_topics</code> function from <code>gensim</code> and visualize the first 10 document with its dominant topics:</p>
<div id="529380f9" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the topic distribution for the first 10 documents</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>topic_dist_first_10 <span class="op">=</span> [optimal_model.get_document_topics(doc, minimum_probability<span class="op">=</span><span class="fl">0.0</span>) <span class="cf">for</span> doc <span class="kw">in</span> corpus[:<span class="dv">10</span>]]</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate over the first 10 documents</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, doc_topic_dist <span class="kw">in</span> <span class="bu">enumerate</span>(topic_dist_first_10):</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract topic ids and their probabilities</span></span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    doc_topics, doc_probs <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>doc_topic_dist)</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the topic probabilities for the document</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    plt.bar(doc_topics, doc_probs, color<span class="op">=</span><span class="st">'skyblue'</span>)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Topic ID'</span>)</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f'Topic Distribution for Post </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>    plt.xticks(doc_topics)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab-4-case-study-python_files/figure-html/cell-20-output-1.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab-4-case-study-python_files/figure-html/cell-20-output-2.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab-4-case-study-python_files/figure-html/cell-20-output-3.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab-4-case-study-python_files/figure-html/cell-20-output-4.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab-4-case-study-python_files/figure-html/cell-20-output-5.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab-4-case-study-python_files/figure-html/cell-20-output-6.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab-4-case-study-python_files/figure-html/cell-20-output-7.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab-4-case-study-python_files/figure-html/cell-20-output-8.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab-4-case-study-python_files/figure-html/cell-20-output-9.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab-4-case-study-python_files/figure-html/cell-20-output-10.png" width="812" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="c.-finding-the-dominant-words-for-each-topic" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="c.-finding-the-dominant-words-for-each-topic"><span class="header-section-number">5.3</span> c.&nbsp;Finding the Dominant Words for Each Topic</h3>
<p>By merely knowing the dominant topic ID for a document from last session, obviously, it’s not very easy to interpret what the topics exactly are, so let’s examine the top 5 terms for each topic and then look at this information visually:</p>
<div id="7f394a64" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract the top 5 terms for each topic from the LDA model</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>topics <span class="op">=</span> optimal_model.show_topics(formatted<span class="op">=</span><span class="va">False</span>, num_words<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>topics_dict <span class="op">=</span> {topic_num: [(word, <span class="bu">round</span>(weight, <span class="dv">4</span>)) <span class="cf">for</span> word, weight <span class="kw">in</span> words] <span class="cf">for</span> topic_num, words <span class="kw">in</span> topics}</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the dictionary to a DataFrame for easier visualization</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>topics_df <span class="op">=</span> pd.DataFrame(topics_dict).T</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>topics_df.columns <span class="op">=</span> [<span class="ss">f'Term </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>)]</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>topics_df[<span class="st">'Topic'</span>] <span class="op">=</span> topics_df.index</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>topics_df <span class="op">=</span> topics_df.melt(id_vars<span class="op">=</span><span class="st">'Topic'</span>, var_name<span class="op">=</span><span class="st">'Term Rank'</span>, value_name<span class="op">=</span><span class="st">'Term and Weight'</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Split 'Term and Weight' column into separate 'Term' and 'Weight' columns</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>topics_df[[<span class="st">'Term'</span>, <span class="st">'Weight'</span>]] <span class="op">=</span> pd.DataFrame(topics_df[<span class="st">'Term and Weight'</span>].tolist(), index<span class="op">=</span>topics_df.index)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop the 'Term and Weight' column as it's no longer needed</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>topics_df.drop(columns<span class="op">=</span>[<span class="st">'Term and Weight'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the DataFrame</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(topics_df.head(<span class="dv">10</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   Topic Term Rank     Term  Weight
0      0    Term 1     data  0.0193
1      1    Term 1     font  0.0405
2      2    Term 1  student  0.0399
3      3    Term 1     http  0.0288
4      0    Term 2  student  0.0131
5      1    Term 2     span  0.0377
6      2    Term 2     data  0.0192
7      3    Term 2     href  0.0194
8      0    Term 3   sample  0.0112
9      1    Term 3    style  0.0362</code></pre>
</div>
</div>
<p>From the data frame generated, we can visualize now:</p>
<div id="e449e663" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the FacetGrid</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> sns.FacetGrid(topics_df, col<span class="op">=</span><span class="st">"Topic"</span>, col_wrap<span class="op">=</span><span class="dv">4</span>, sharex<span class="op">=</span><span class="va">False</span>, sharey<span class="op">=</span><span class="va">False</span>, height<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Map the bar plot to each subplot</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>g.map_dataframe(sns.barplot, x<span class="op">=</span><span class="st">'Weight'</span>, y<span class="op">=</span><span class="st">'Term'</span>, palette<span class="op">=</span><span class="st">'husl'</span>, orient<span class="op">=</span><span class="st">'h'</span>)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Customize the plot</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>g.set_titles(col_template<span class="op">=</span><span class="st">'Topic </span><span class="sc">{col_name}</span><span class="st">'</span>)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>g.set_axis_labels(<span class="st">"Weight"</span>, <span class="st">"Term"</span>)</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead

/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead

/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead

/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead

/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead

/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead

/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead

/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead

/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead

/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead

/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead

/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead

/opt/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: DeprecationWarning:

is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="lab-4-case-study-python_files/figure-html/cell-22-output-2.png" width="1525" height="371" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="d.-finding-the-dominate-documents-for-each-topic" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="d.-finding-the-dominate-documents-for-each-topic"><span class="header-section-number">5.4</span> d.&nbsp;Finding the Dominate Documents for Each Topic</h3>
<p>Now that we have a sense of the most common words associated with each topic, let’s further explore the dominate documents for each topic:</p>
<div id="86ab6112" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize a list to store topic distributions for each document</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>topic_distributions <span class="op">=</span> [optimal_model.get_document_topics(doc, minimum_probability<span class="op">=</span><span class="fl">0.0</span>) <span class="cf">for</span> doc <span class="kw">in</span> corpus]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Identify the dominant topics for each document</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>dominant_topics <span class="op">=</span> [(<span class="bu">sorted</span>(topic_distribution, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>], i)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>                   <span class="cf">for</span> i, topic_distribution <span class="kw">in</span> <span class="bu">enumerate</span>(topic_distributions)]</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame from the list of dominant topics</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>df_dominant_topic <span class="op">=</span> pd.DataFrame(dominant_topics, columns<span class="op">=</span>[<span class="st">'Dominant_Topic'</span>, <span class="st">'Document_No'</span>])</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>df_dominant_topic[<span class="st">'Topic_ID'</span>] <span class="op">=</span> df_dominant_topic[<span class="st">'Dominant_Topic'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x[<span class="dv">0</span>])</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>df_dominant_topic[<span class="st">'Topic_Probability'</span>] <span class="op">=</span> df_dominant_topic[<span class="st">'Dominant_Topic'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x[<span class="dv">1</span>])</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure the original DataFrame has a unique identifier for each document</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>post_content_exploded[<span class="st">'Document_No'</span>] <span class="op">=</span> post_content_exploded.index</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge with the original DataFrame to get the text of the dominant documents</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>df_dominant_topic <span class="op">=</span> df_dominant_topic.merge(post_content_exploded[[<span class="st">'Document_No'</span>, <span class="st">'document'</span>]], on<span class="op">=</span><span class="st">'Document_No'</span>, how<span class="op">=</span><span class="st">'left'</span>)</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a function to get the top N unique documents for each topic</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_top_n_documents_grouped(df, n<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sort by Topic_Probability within each group and take the top N unique documents</span></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    top_n_docs <span class="op">=</span> df.groupby(<span class="st">'Topic_ID'</span>).<span class="bu">apply</span>(<span class="kw">lambda</span> group: group.nlargest(n, <span class="st">'Topic_Probability'</span>).drop_duplicates(subset<span class="op">=</span><span class="st">'document'</span>))</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> top_n_docs</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the top 3 unique documents for each topic</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>top_n_docs_unique <span class="op">=</span> get_top_n_documents_grouped(df_dominant_topic, n<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first three dominant documents for each topic</span></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> topic <span class="kw">in</span> top_n_docs_unique[<span class="st">'Topic_ID'</span>].unique():</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Topic </span><span class="sc">{</span>topic<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a>    top_docs <span class="op">=</span> top_n_docs_unique[top_n_docs_unique[<span class="st">'Topic_ID'</span>] <span class="op">==</span> topic].nlargest(<span class="dv">3</span>, <span class="st">'Topic_Probability'</span>)</span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _, row <span class="kw">in</span> top_docs.iterrows():</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Document No: </span><span class="sc">{</span>row[<span class="st">'Document_No'</span>]<span class="sc">}</span><span class="ss"> - Probability: </span><span class="sc">{</span>row[<span class="st">'Topic_Probability'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(row[<span class="st">'document'</span>])</span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Topic 0:
Document No: 43 - Probability: 0.9954051971435547
 Well  Old Faithful definitely was the best using the \Written Statistics Task Guide.\"    The students make MANY of the decisions in this task even though the main question already has been posed by the teacher and the data set chosen.  I especially like that the students choose the graphical display  but then the displays of all of the groups are interpreted.  What if everyone chooses the same?  (I think it would be great for at least choose one graphical display that DOESN'T provide much insight and discuss why it doesn't.)  It would be improved by giving the students MORE choice in the \"Pose\" and \"Collect\" phases.  Do students even care about Old Faithful wait times  or is there another topic in which they have more interest?  Ask the STUDENTS ideas about how data could be collected and decide as a class the best method.  Why TELL them to do a random sampling?  They might come up with it on their own.   SO MUCH depends on the group of students.  This task at least can get a teacher's mind going on how to BEGIN planning a great statistical task. :) "

Topic 1:
Document No: 190 - Probability: 0.999692440032959
 I think Television Time is good data to work with esp with children. Though I did wonder if it would do even better with grade 5 children. I would also like to give the data in a tabular form and get the children to plot it in different ways. It will be reinforcement of the types of graphical representations as well.                               

Topic 2:
Document No: 3224 - Probability: 0.9952095746994019
 As a high school department chair  I too am concerned about the lack of statistics in the earlier grades.  Despite being one of six major domains of the CCSSM  our feeder schools and our high school district seem to have placed this topic on the back-burner.  It is my goal as a leader to begin to work closely with our feeder schools and my teachers to bring these standards to life through authentic lessons in the classroom.  I also hope to increase our enrollment in AP statistics in coming years. 

Topic 3:
Document No: 81 - Probability: 0.9994573593139648
 Lana  you are right!  The louder and brighter - the better for the students.  This teacher thinks like that at times  too!    I really liked the fact that this assignment requires the students to pay attention to detail and think \outside the box\".  "
</code></pre>
</div>
</div>
<section id="takeaway" class="level4" data-number="5.4.1">
<h4 data-number="5.4.1" class="anchored" data-anchor-id="takeaway"><span class="header-section-number">5.4.1</span> Takeaway</h4>
<p>In addition to some useful Python packages and functions for the actual process of topic modeling, hopefully there are two main lessons I’m hoping you take away from this walkthrough:</p>
<ol type="1">
<li><strong>Topic modeling requires a lot of decisions.</strong> Beyond deciding on a value for K, there are a number of key decisions that you have to make that can dramatically affect your results. For example, to stem or not to stem? What qualifies as a document? What flavor or topic modeling is best suited to your data and research questions? How many iterations to run?</li>
<li><strong>Topic modeling is as much art as (data) science.</strong> As Bail (2018) noted, the term “topic” is somewhat ambitious, and topic models do not produce highly nuanced classification of texts. Once you’ve fit your model, interpreting your model requires some mental gymnastics and ideally some knowledge of the context from which the data came to help with interpretation of your topics. Moreover, the quantitative approaches for making the decisions highlighted above are imperfect and a good deal of human judgment required.</li>
</ol>
<section id="comprehension-check-1" class="level5" data-number="5.4.1.1">
<h5 data-number="5.4.1.1" class="anchored" data-anchor-id="comprehension-check-1"><span class="header-section-number">5.4.1.1</span> Comprehension Check</h5>
<p>Using the LDA model with a different value for K, use the approaches demonstrated in Section 4 to explore and interpret your topics and terms and revisit the following question:</p>
<ol type="1">
<li>Now that you have a little more context, how might you revise your initial interpretation of some of the latent topics or latent themes from your model?</li>
</ol>
</section>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>