---
title: "TM Lab 1.1: Tidy Text, Tokens & Twitter"
subtitle: "Guided Practice"
author: "LASER Institute"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[INSERT PRESENTATION VIDEO HERE]

## 1. PREPARE

### 1c. Prepare Environment

#### Load Packages

Let's begin by loading some familiar packages from previous Learning Labs that we'll be using for data wrangling and exploration:

```{r load-libraries, message=FALSE}
library(dplyr)
library(readr)
library(tidyr)
library(ggplot2)
library(readxl)
library(writexl)
library (DT)
```

#### rtweet ðŸ“¦

Let's load the `rtweet` package which provides users a range of functions designed to extract data from Twitter's REST and streaming APIs:

```{r load-rtweet, message=FALSE}
library(rtweet)
```

#### Check Authorization

Check to make sure the app name and `api_key` match.

```{r get-token}
## check to see if the token is loaded
get_token()
```

That's it! We're ready to start wrangling some tweets!!!

------------------------------------------------------------------------

## 2. WRANGLE

### 2a. Search Tweets

#### Constructing a Query

Run the following code to try reading into R 5,000 tweets containing the NGSS hashtag and store as a new data frame `ngss_all_tweets`:

```{r}
ngss_tweets_q1 <- search_tweets(q = "#NGSSchat", 
                                n = 5000)
```

##### [Your Turn]{style="color: green;"}Â â¤µ

View your new `ngss_all_tweets` data frame using one of the methods previously introduced to help answer the following questions:

```{r}


```

1.  How many tweets did our query using the Twitter API actually return? How many variables?

    -   

2.  Why do you think our query pulled in far less than 5,000 tweets requested?

    -   

3.  How many tweets are returned if you don't include the `n =` argument?

    -   

4.  Does our query also include retweets? How do you know?

    -   

5.  Does capitalization in your query matter?

    -   

#### Using the OR Operator

Run the following code to modify our query using the `OR` operator to include "ngss" so it will return tweets containing either \#NGSSchat or "ngss" and assign to `ngss_or_tweets`:

```{r}
ngss_tweets_q2 <- search_tweets(q = "#NGSSchat OR ngss", 
                                n = 5000)
```

##### [Your Turn]{style="color: green;"}Â â¤µ

Try including both search terms but excluding the `OR` operator to answer the following questions:

```{r}


```

1.  Does excluding the `OR` operator return more tweets, the same number of tweets, or fewer tweets? Why?

    -   

2.  Does our query also include tweets containing the \#ngss hashtag?

    -   

3.  What other useful arguments does the `search_tweet()` function contain? Try adding one and see what happens.

    -   

Hint: Use the `?search_tweets` help function to learn more about the `q` argument and other arguments for composing search queries.

#### Using Multiple Queries

Use the `search_tweets2()` function to include the `'"next gen science standard"'` in our query:

```{r}
ngss_tweets_q3 <- search_tweets2(q = c("#NGSSchat OR ngss",
                                       '"next generation science standard"'),
                                 n = 5000)
```

#### Our First Dictionary

Run the following code to store your dictionary and queried tweets in your environment:

```{r}
ngss_dictionary <- c("#NGSSchat OR ngss",
                     '"next generation science standard"',
                     '"next generation science standards"',
                     '"next gen science standard"',
                     '"next gen science standards"')

ngss_tweets_q4 <- search_tweets2(ngss_dictionary,
                                 n=5000)

datatable(ngss_tweets_q4)
```

##### [Your Turn]{style="color: green;"}Â â¤µ

1.  In the following code chunk, write a new query based on a STEM area of interest.

2.  Assign your search to a new object called `my_tweets` or something appropriate.

3.  Output your new dataset using the `datatable()` function from the `DT` package and take a quick look.

```{r}


```

#### Other Useful Functions (Optional)

Run the following code to create another list containing the usernames of the LASER Institute and use the `get_timelines()` function to get the most recent tweets from each of those users:

```{r, eval=TRUE}
laser_peeps <- c("sbkellogg", "jrosenberg6432", "yanecnu", "robmoore3", "hollylynnester")

laser_tweets <- laser_peeps %>%
  get_timelines(include_rts=FALSE)
```

Run the following code to sample 10 random tweets and view just the `screenname` and `text` of their posts:

```{r, eval=TRUE}
sample_n(laser_tweets, 10) %>%
  select(screen_name, text)
```

The `rtweet` package also has handy `ts_plot` function built into `rtweet` to take a very quick look at how far back our data set goes:

```{r ts_plot, eval=TRUE}
ts_plot(ngss_tweets_q4, by = "days")
```

Try changing it to "hours" and see what happens.

#### [Your Turn]{style="color: green;"}Â â¤µ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Use the code chunk below to try one of the following search functions from the `rtweet` vignette:

1.  `get_timelines()` Get the most recent 3,200 tweets from users.
2.  `stream_tweets()` Randomly sample (approximately 1%) from the live stream of all tweets.
3.  `get_friends()` Retrieve a list of all the accounts a user follows.
4.  `get_followers()` Retrieve a list of the accounts following a user.
5.  `get_favorites()` Get the most recently favorited statuses by a user.
6.  `get_trends()` Discover what's currently trending in a city.
7.  `search_users()` Search for 1,000 users with the specific hashtag in their profile bios.

```{r}


```

Use the following function to access the package vignette:

```{r rtweet-vignette}
vignette("intro", package="rtweet")
```

### 2b. Restructure Data

#### Subset Tweets

Use the `filter()` function introduced in previous labs to subset rows containing only original tweets in the English language:

```{r filter-en, eval=TRUE}
ngss_tweets_r1 <- ngss_tweets_q4 %>% filter(is_retweet == "FALSE", 
                                            lang == "en")
```

Now let's use the `select()` function to select the just the columns needed:

```{r select-variables, eval=TRUE}
ngss_tweets_r2 <- select(ngss_tweets_r1,
                         screen_name, 
                         created_at, 
                         text)
```

#### Add & Reorder Columns

Use the `mutate()` function to create a new variable called `standards` to label each tweets as "ngss":

```{r}
ngss_tweets_r3 <- mutate(ngss_tweets_r2, standards = "ngss")
```

Move the `standards` column to the first position so we can quickly see which standards the tweet is from:

```{r}
ngss_tweets_r4 <- relocate(ngss_tweets_r3, standards)
```

Note that you could also have used the `select()` function to reorder columns like so:

```{r}
ngss_tweets_r5 <- select(ngss_tweets_r4, standards, screen_name, created_at, text)
```

Before moving on to tidying our text, let's rewrite the code from our all our wrangling so there is less redundancy and it is easier to read:

```{r}
# Search Tweets
ngss_dictionary <- c("#NGSSchat OR ngss",
                     '"next generation science standard"',
                     '"next generation science standards"',
                     '"next gen science standard"',
                     '"next gen science standards"')

ngss_query <- search_tweets2(ngss_dictionary,
                                 n = 5000,
                                 include_rts = FALSE,
                                 lang = "en") 

# Restructure Data
ngss_tweets <- 
  ngss_query %>%
  select(text, screen_name, created_at) %>%
  mutate(standards = "ngss") %>%
  relocate(standards)
```

##### [Your Turn]{style="color: green;"} â¤µ

Recall from section 1b. Define Questions that we are interested in comparing word usage and public sentiment around both the Common Core and Next Gen Science Standards.

1.  Use the code chunk below to create a new `ccss_tweets` data frame by modifying the code above.

2.  Similar to Wang and Fikis (2019), use the \#CommonCore and \#CCSS hashtags as your search terms.

3.  Try using the handyÂ `include_rts = FALSE`Â and `lang = "en"` arguments included in the `search_tweets2()` function to remove retweets and non-English language tweets.

```{r}


```

[**WARNING:**]{style="color: red;"} You will not be able to progress to the next section until you have successfully completed the task above.

#### Merge Data Frames

Run the code below to combine our `ccss_text` and `ngss_text` into a single data frame:

```{r}
ss_tweets <- bind_rows(ngss_tweets, ccss_tweets)
```

Take a quick look at both the `head()` and the `tail()` of this new `tweets` data frame to make sure it contains both "ngss" and "ccss" standards:

```{r}
head(ss_tweets)
tail(ss_tweets)
```

------------------------------------------------------------------------

### 2c. Tidy Text

Load the `tidytext` package:

```{r load-tidytext, message=FALSE}
library(tidytext)
```

#### Tokenize Text {data-link="2b. Tidy Text"}

##### Unigrams

Let's tokenize our tweets by using this function to split each tweet into a single row to make it easier to analyze:

```{r unnest-tokens}
ss_tweets_u1 <- unnest_tokens(ss_tweets, 
                               output = word, 
                               input = text)

head(ss_tweets_u1)
```

##### [Your Turn]{style="color: green;"}Â â¤µ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Rewrite the code below to include the token argument set to "tweets":

```{r token-tweet, eval=FALSE}
ss_tweets_u2 <- unnest_tokens(ss_tweets, 
                              output = word, 
                              input = text, 
                              _____ = _____)
```

##### Bigrams

Use the `unnest_tokens()` function to tokenize our tweets into bigrams by settingÂ `n` to 2:

```{r ccss-bigrams}
ss_tweets_b1 <- ss_tweets %>% 
  unnest_tokens(bigram, 
                text, 
                token = "ngrams", 
                n = 2)

head(ss_tweets_b1)
```

Before we move any further take a quick look at the most common unigrams and bigrams in our two datasets:

```{r}
ss_tweets_u2 %>%
  count(word, sort = TRUE)

ss_tweets_b1 %>% 
  count(bigram, sort = TRUE)
```

#### Remove Stop Words

Take a closer the lexicons and stop words included in each:

```{r}
datatable(stop_words)
```

#### The `anti_join` Function

Remove stop words that don't help us learn much about what people are saying about the state standards.

```{r stop-unigrams}
ss_tweets_u3 <- anti_join(ss_tweets_u2,
                           stop_words,
                           by = "word")

head(ss_tweets_u3)
```

#### Filtering Bigrams

Use the `separate()` function from the `tidyr` package, which splits a column into multiple based on a delimiter:

```{r stop-bigrams}
library(tidyr)

ss_tweets_b2 <- ss_tweets_b1 %>%
  separate(bigram, c("word1", "word2"), sep = " ")

ss_tweets_b3 <- ss_tweets_b2 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

ss_tweets_b4 <- ss_tweets_b3 %>%
  unite(bigram, word1, word2, sep = " ")
```

#### Custom Stop Words

Before wrapping up, let's take a quick count of the most common unigrams and bigrams to see if the results are a little more meaningful:

```{r}
ss_tweets_u3 %>%
  count(word, sort = TRUE)

ss_tweets_b4 %>% 
  count(bigram, sort = TRUE)
```

Notice that the nonsense word "amp" is among our high frequency words. Let's add a filter to our previous code similar to what we did with our bigrams to remove rows with "amp" in them:

```{r}
ss_tweets_u4 <-
  ss_tweets_u3 %>%
  anti_join(stop_words, by = "word") %>%
  filter(!word == "amp")
```

Note that we could extend this filter to weed out any additional words that don't carry much meaning but skew our data by being so prominent.

##### [Your Turn]{style="color: green;"}Â â¤µ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Use the code chunk below to tidy your `my_tweets` dataset from the [âœ… Comprehension Check](#comprehension-check-7) in Section 2a by tokenizing your tweets into unigrams and removing stop words.

Also, since we created some unnecessarily lengthy code to demonstrate some of the steps in the tidying process, try to use a more compact series of functions and assign your data frame to `my_tidy_tweets`.

```{r}


```
