---
title: 'Tidy Text, Tokens, and Twitter'
subtitle: 'Text Mining Learning Lab 1'
author: "YOUR NAME HERE"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: yes
  html_notebook:
bibliography: lit/references.bib
csl: lit/apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. PREPARE

The transition to digital learning has made available new sources of data, providing researchers new opportunities for understanding and improving STEM learning. Data sources such as digital learning environments and administrative data systems, as well as data produced by social media websites and the mass digitization of academic and practitioner publications, hold enormous potential to address a range of pressing problems in STEM Education, but collecting and analyzing text-based data also presents unique challenges.

**Text Mining (TM) Module 1: Public Sentiment and the State Standards** will help demonstrate how text mining can be applied in STEM education research and provide LASER Institute scholars hands-on experience with popular techniques for collecting, processing, and analyzing text-based data. Specifically, the four learning labs that make up this module address the following topics:

-   **Learning Lab 1: Tidy Text, Tokens, & Twitter.**¬†We take a closer look at the literature guiding our analysis; wrangle our data into a one-token-per-row tidy text format; and use simple word counts to explore our tweets about the common core and next generation science standards.

-   **Learning Lab 2: Twice the fun with Bigrams.**¬†For our second lab, we see what pairs of words and word correlations tell us about our tweets what insight they provide in response to our research questions.

-   **Learning Lab 3: Come to the Dark Side.**¬†We focus on the use of lexicons in our third lab and introduce the {vader} package to compare the sentiment of tweets about the NGSS and CCSS state standards in order to better understand public reaction to these two curriculum reform efforts.¬†

-   **Learning Lab 4: A Tale of Two Standards.** We wrap our look at public sentiment around STEM state curriculum standards by selecting an analysis that provides some unique insight; refining and polishing a data product; and writing a brief narrative to communicate findings in response to our research questions.

### 1a. Review the Literature

Text Mining Module 1 is guided by a recent publication by @rosenberg2020, *Advancing new methods for understanding public sentiment about educational reforms: The case of Twitter and the Next Generation Science Standards*. This study in turn builds on upon previous work by @wang2017 examining public opinion on the Common Core State Standards (CCSS) on Twitter. For Module 1, we will focus on analyzing tweets about the [Next Generation Science Standards](https://www.nextgenscience.org) (NGSS) and [Common Core State Standards](http://www.corestandards.org) (CCSS) in order to better understand key words and phrases that emerge, as well as public sentiment towards these two curriculum reform efforts.

#### Twitter and the Next Generation Science Standards

![](img/rosenberg.png){width="30%"}

[Full Paper (Preprint)](https://osf.io/xymsd/.)

**Abstract**

While the Next Generation Science Standards (NGSS) are a long-standing and widespread standards-based educational reform effort, they have received less public attention, and no studies have explored the sentiment of the views of multiple stakeholders toward them. To establish how public sentiment about this reform might be similar to or different from past efforts, we applied a suite of data science techniques to posts about the standards on Twitter from 2010-2020 (N = 571,378) from 87,719 users. Applying data science techniques to identify teachers and to estimate tweet sentiment, we found that the public sentiment towards the NGSS is overwhelmingly positive -- 33 times more so than for the CCSS. Mixed effects models indicated that sentiment became more positive over time and that teachers, in particular, showed a more positive sentiment towards the NGSS. We discuss implications for educational reform efforts and the use of data science methods for understanding their implementation.

**Data Source & Analysis**

Similar to what we'll be learning in this lab, Rosenberg et al. used publicly accessible data from Twitter collected using the Full-Archive Twitter API and the `rtweet` package in R. Specifically, the authors accessed tweets and user information from the hashtag-based \#NGSSchat online community, all tweets that included any of the following phrases, with "/" indicating an additional phrase featuring the respective plural form: "ngss", "next generation science standard/s", "next gen science standard/s".

Unlike this learning lab, however, the authors determined Tweet sentiment using the Java version of SentiStrength to assign tweets to two 5-point scales of sentiment, one for positivity and one for negativity, because SentiStrength is a validated measure for sentiment in short informal texts (Thelwall et al., 2011). In addition, they used this tool because Wang and Fikis (2019) used it to explore the sentiment of CCSS-related posts. We'll be using the AFINN sentiment lexicon which also assigns words in a tweet to two 5-point scales, in addition to exploring some other sentiment lexicons to see if they produce similar results.

The authors also used the `lme4` package in R to run a mixed effects model to determine if sentiment changes over time and differs between teachers and non-teacher. In addition to looking at the relationships between tweet sentiment, time and teachers, we'll also take a look at the correlation between words within tweets.

**Summary of Key Findings**

1.  Contrasting with sentiment about CSSS, sentiment about the NGSS science education reform effort is overwhelmingly positive, with approximately 9 positive tweets for every negative tweet.
2.  Teachers were more positive than non-teachers, and sentiment became substantially more positive over the ten years of NGSS-related posts.
3.  Differences between the context of the tweets were small, but those that did not include the \#NGSSchat hashtag became more positive over time than those posts that did not include the hashtag.
4.  Individuals posted more tweets during \#NGSSchat chats, the sentiment of their posts was more positive, suggesting that while the context of individual tweets has a small effect (with posts not including the hashtag becoming more positive over time), the effect upon individuals of being involved in the \#NGSSchat was positive.

Finally, you can watch Dr. Rosenberg provide a quick 3-minute overview of this work at [\<https://stanford.app.box.com/s/i5ixkj2b8dyy8q5j9o5ww4nafznb497x\>](https://stanford.app.box.com/s/i5ixkj2b8dyy8q5j9o5ww4nafznb497x){.uri}

### 1b. Define Questions

One overarching question that Silge and Robinson (2018) identify as a central question to text mining and natural language processing, and that we'll explore throughout the text mining labs this year, is the question:

> How do we to **quantify** what a document or collection of documents is about?

The questions guiding the Rosenberg et al. study attempt to quantify public sentiment around the NGSS and how that sentiment changes over time. Specifically, they asked:

1.  What is the public sentiment expressed toward the NGSS?
2.  How does sentiment for teachers differ from non-teachers?
3.  How do tweets posted to \#NGSSchat differ from those without the hashtag?
4.  How does participation in \#NGSSchat relate to the public sentiment individuals express?
5.  How does public sentiment vary over time?

For our first lab on text mining in STEM education, we'll use approaches similar to those used by the authors cited above to better understand public discourse surrounding these standards, particularly as they relate to STEM education. We will also try to guage public sentiment around the NGSS, by comparing how much more positive or negative NGSS tweets are relative to CSSS tweets. Specifically, in the next four learning lab we'll attempt to answer the following questions:

1.  What are the most frequent words or phrases used in reference to tweets about the CCSS and NGSS?
2.  What words and hashtags commonly occur together?
3.  How does sentiment for NGSS compare to sentiment for CCSS?

### 1c. Load Libraries

#### tidyverse üì¶

![](img/tidyverse.png){width="20%"}

As noted in our Getting Started activity, R uses "packages" and add-ons that enhance its functionality. One package that we'll be using extensively is {tidyverse}. The {tidyverse} package is actually a [collection of R packages](https://www.tidyverse.org/packages) designed for reading, wrangling, and exploring data and which all share an underlying design philosophy, grammar, and data structures. This shared features are sometimes "tidy data principles."

Click the green arrow in the right corner of the "code chunk" that follows to load the {tidyverse} library.

```{r}
library(tidyverse)
```

Again, don't worry if you saw a number of messages: those probably mean that the tidyverse loaded just fine. Any conflicts you may have seen mean that functions in these packages you loaded have the same name as functions in other packages and R will default to function from the last loaded package unless you specify otherwise.

#### tidytext üì¶

![](img/tidytext.png){width="20%"}

As we'll learn first hand in this module, using tidy data principles can also make many text mining tasks easier, more effective, and consistent with tools already in wide use. The {tidytext} package helps to convert text into data frames of individual words, making it easy to to manipulate, summarize, and visualize text using using familiar functions form the {tidyverse} collection of packages.

Let's go ahead and load the {tidytext} package:

```{r}
library(tidytext)
```

For a more comprehensive introduction to the `tidytext` package, we cannot recommend enough the free online book, *Text Mining with R: A Tidy Approach* [@silge2017text].

## 2. WRANGLE

The importance of data wrangling, particularly when working with text, is difficult to overstate. Just as a refresher, wrangling involves the initial steps of going from raw data to a dataset that can be explored and modeled [@krumm2018]. Learning Lab 2 will have a heavy emphasis on preparing text for analysis and in particular we'll learn how to:

a.  **Merge Tweets**. First we revisit the familiar `read_csv()` function for reading in our CCSS and NGSS tweets and introduce two new functions for merging data frames as we've seen in earlier learning labs.
b.  **Restructure Data**. We focus on removing extraneous data using \`
c.  **Tidy Text.** We introduce the {tidytext} package to "tidy" and tokenize our tweets in order to create our data frame for analysis revisit the concept of joins to remove "stop words" that don't add much value to our analysis.

### 2a. Import and View Data

```{r}
ccss_tweets <- read_csv("data/ccss-tweets.csv", 
          col_types = cols(author_id = col_character(), 
                           id = col_character(),
                           conversation_id = col_character(), 
                           in_reply_to_user_id = col_character()
                           )
          )
```

Note the addition of the `col_types =` argument for changing some of the column types to character strings because the numbers for those particular columns actually indicate identifiers for authors and tweets:

-   `author_id` = the author of the tweet

-   `id` = the unique id for each tweet

-   `converastion_id` = the unique id for each [conversation thread](https://developer.twitter.com/en/docs/twitter-api/conversation-id)

-   `in_reply_to_user_id` = the author of the tweet being replied to

#### [Your Turn]{style="color: green;"}¬†‚§µ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

**RStudio Tip:** Importing data and dealing with data types can be a bit tricky, especially for beginners. Fortunately, RStudio has an "Import Dataset" feature in the Environment Pane that can help you use the {readr} package and associated functions to greatly facilitate this process.

![](img/import-data.png)

Try using the "Import Dataset" feature in the upper right environment pane to import the NGSS tweets located in the data folder.

The code generated should look something like this:

```{r}
ngss_tweets <- read_csv("data/ngss-tweets.csv", 
          col_types = cols(author_id = col_character(), 
                           id = col_character(),
                           conversation_id = col_character(), 
                           in_reply_to_user_id = col_character()
                           )
          )
```

Use the following code chunk to inspect your tweets using a function you've learned so for for viewing your data:

```{r}
# your code here
```

### 2b. Restructure Data

#### Subset Tweets

As you may have noticed, we have more data than we need for our analysis and should probably pare it down to just what we'll use.

Let's start with the First, since this is a family friendly learning lab, let's use the `filter()` function introduced in previous labs to filter out rows containing "possibly sensitive" language:

```{r, eval=TRUE}
ccss_tweets_1 <- ccss_tweets %>% 
  filter(possibly_sensitive == "FALSE")
```

Now let's use the `select()` function to select the following columns from our new `ss_tweets_clean` data frame:

1.  `text` containing the tweet which is our primary data source of interest
2.  `author_id` of the user who created the tweet
3.  `created_at` timestamp for examining changes in sentiment over time
4.  `conversation_id` for examining sentiment by conversations
5.  `id` for the unique reference id for each tweet and useful for counts

```{r select-variables, eval=TRUE}
ccss_tweets_2 <- ccss_tweets_1 %>% 
  select(text,
         author_id,
         created_at, 
         conversation_id,
         id)
```

#### [Your Turn]{style="color: green;"}¬†‚§µ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

**Note:** The `select()` function will also reorder your columns based on the order in which you list them.

Use the code chunk below to reorder the columns to your liking and assign to `ccss_tweets_3`:

```{r}
# your code here
```

#### Add & Relocate Columns

Finally, since we are interested in comparing the sentiment of NGSS tweets with CSSS tweets, it would be helpful if we had a column to quickly identify the set of state standards with which each tweet is associated.

We'll use the `mutate()` function to create a new variable called `standards` to label each tweets as "ngss":

```{r}
ccss_tweets_4 <- mutate(ccss_tweets_2, standards = "ccss")

colnames(ccss_tweets_4)
```

And just because it bothers me, I'm going to use the `relocate()` function to move the `standards` column to the first position so I can quickly see which standards the tweet is from:

```{r}
ccss_tweets_5 <- relocate(ccss_tweets_4, standards)

colnames(ccss_tweets_5)
```

Again, we could also have used the `select()` function to reorder columns like so:

```{r}
ccss_tweets_5 <- ccss_tweets_4 %>% 
  select(standards,
         text,
         author_id,
         created_at, 
         conversation_id,
         id)

colnames(ccss_tweets_5)
```

Before moving on to the CCSS standards, let's use the `%>%` operator and rewrite the code from our wrangling so there is less redundancy and it is easier to read:

```{r}
# Search Tweets
ccss_tweets_clean <- ccss_tweets %>%
  filter(possibly_sensitive == "FALSE") %>%
  select(text, author_id, created_at, conversation_id, id) %>%
  mutate(standards = "ccss") %>%
  relocate(standards)

head(ccss_tweets_clean)
```

#### [Your Turn]{style="color: green;"} ‚§µ

Recall from section [1b. Define Questions] that we are interested in comparing word usage and public sentiment around both the Common Core and Next Gen Science Standards.

Create an new `ngss_tweets_clean` data frame consisting of the Next Generation Science Standards tweets we imported by use the code above as a guide.

```{r}
# your code here
```

Try not to peek at the answer below unless you are having difficulty with your code.

#### Answer

```{r 2b-answer}
ngss_tweets_clean <- ngss_tweets %>%
  filter(possibly_sensitive == "FALSE") %>%
  select(text, author_id, created_at, conversation_id, id) %>%
  mutate(standards = "ngss") %>%
  relocate(standards)

head(ngss_tweets_clean)
```

#### Merge Data Frames

Finally, let's combine our CCSS and NGSS tweets into a single data frame by using the `union()` function from `dplyr` and simply supplying the data frames that you want to combine as arguments:

```{r}
ss_tweets <- union(ccss_tweets_clean,
                   ngss_tweets_clean)
```

Note that when creating a "union" like this (i.e. stacking one data frame on top of another), you should have the same number of columns in each data frame and they should be in the exact same order.

#### [Your Turn]{style="color: green;"} ‚§µ

Finally, let's take a quick look at both the `head()` and the `tail()` of this new `ss_tweets` data frame to make sure it contains both "ngss" and "ccss" standards:

```{r}
head(ss_tweets)
tail(ss_tweets)
```

Wow, so much for a family friendly learning lab! Based on this very limited sample, which set of standards do you think Twitter users are more negative about?

-   your response here

Let's take a slightly larger sample of the CCSS tweets:

```{r}
ss_tweets %>% 
  filter(standards == "ccss") %>%
  sample_n(20) %>%
  relocate(text)
           
```

#### [Your Turn]{style="color: green;"} ‚§µ

Use the code chunk below to take a sample of the NGSS tweets:

```{r}
# your code here
```

Still of the same opinion?

-   Respond here...

### 2c. Tidy Text

Text data by it's very nature is ESPECIALLY untidy and is sometimes referred to as "unstructured" data. In this section we are introduced to the `tidytext` package and will learn some new functions to convert text to and from tidy formats. Having our text in a tidy format will allow us to switch seamlessly between tidy tools and existing text mining packages, while also making it easier to visualize text summaries in other data analysis tools like Tableau.

#### Tokenize Text {data-link="2b. Tidy Text"}

In Chapter 1 of Text Mining with R, @silge2017text define the tidy text format as a table with one-token-per-row, and explain that:

> A **token** is a meaningful unit of text, such as a word, two-word phrase (bigram), or sentence that we are interested in using for analysis. And tokenization is the process of splitting text into tokens.

This one-token-per-row structure is in contrast to the ways text is often stored for text analysis, perhaps as strings in a corpus object or in a document-term matrix. For tidy text mining, the¬†token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph.

For this part of our workflow, our goal is to transform our `ss_tweets` data from this:

```{r}
head(relocate(ss_tweets, text))
```

Into a "tidy text" one-token-per-row format that looks like this:

```{r}
tidy_tweets <- ss_tweets %>% 
  unnest_tokens(output = word, 
                input = text) %>%
  relocate(word)

head(tidy_tweets)
```

Later in the year, we'll learn about other data structures for text analysis like the document-term matrix and corpus objects. For now, however, working with the familiar tidy data frame allows us to take advantage of popular packages that use the shared tidyverse syntax and principles for wrangling, exploring, and modeling data.

#### Unigrams

As demonstrated above, the `tidytext` package provides the incredibly powerful `unnest_tokens()` function to tokenize text (including tweets!) and convert them to a one-token-per-row format.

Let's tokenize our tweets by using this function to split each tweet into a single row to make it easier to analyze and take a look:

```{r unnest-tokens}
ss_tokens <- unnest_tokens(ss_tweets, 
                             output = word, 
                             input = text)

head(relocate(ss_tokens, word))
```

There is A LOT to unpack with this function:

-   First notice that¬†`unnest_tokens()` expects a data frame as the first argument, followed by two column names.
-   The next argument is an output column name that doesn't currently exist but will be created as the text is "unnested" into it, `word` in this case).
-   This is followed by the input column that the text comes from, which we uncreatively named `text`.
-   By default, a token is an individual word or unigram.
-   Other columns, such as¬†`author_id`¬†and¬†`created_at`, are retained.
-   All punctuation has been removed.
-   Tokens have been changed to lowercase, which makes them easier to compare or combine with other datasets (use the¬†`to_lower = FALSE`¬†argument to turn off if desired).

**Note:** Since {tidytext} follows tidy data principles, we also could have used the `%>%` operator to pass our data frame to the `unnest_tokens()` function like so:

```{r}
ss_tokens <- ss_tweets %>%
  unnest_tokens(output = word, 
                input = text)
```

#### [Your Turn]{style="color: green;"}¬†‚§µ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

The `unnest_tokens()` function also has a specialized¬†`‚Äútweets‚Äù`¬†tokenizer in the `tokens =` argument that is very useful for dealing with Twitter text. It retains hashtags and mentions of usernames with the \@ symbol as illustrated by our \@catturd2 friend who featured prominently in our the first CCSS tweet.

Rewrite the code above (**you can check answer below**) to include the token argument set to "tweets", assign to `ss_tokens_1`, and answer the questions that follow:

```{r}
# your code here
```

1.  How many observations were our original `ss_tweets` data frame?

    -   

2.  How many observations are there now? Why the difference?

    -   

#### Answer

Your code should look something like this:

```{r}
ss_tokens_1 <- unnest_tokens(ss_tweets, 
                              output = word, 
                              input = text, 
                              token = "tweets")

head(ss_tokens_1)
```

Before we move any further let's take a quick look at the most common word in our two datasets:

```{r}
ss_tokens_1 %>%
  count(word, sort = TRUE)

```

Well, many of these tweets are clearly about the ccss and math at least, but beyond that it's a bit hard to tell because there are so many "stop words" like "the", "to", "and", "in" that don't carry much meaning by themselves.

#### Remove Stop Words

Often in text analysis, we will want to remove these stop words if they are not useful for an analysis. The¬†`stop_words`¬†dataset in the {tidytext} package contains stop words from three lexicons. We can use them all together, as we have here, or¬†`filter()`¬†to only use one set of stop words if that is more appropriate for a certain analysis.

Let's take a closer the lexicons and stop words included in each:

```{r, eval=FALSE}
View(stop_words)
```

#### The `anti_join` Function

In order to remove these stop words, we will use a function called¬†`anti_join()`¬†that looks for matching values in a specific column from two datasets and returns rows from the original dataset that have no matches like so:

![](img/anti-join.png)

For a good overview of the different¬†`dplyr`¬†joins see here:¬†<https://medium.com/the-codehub/beginners-guide-to-using-joins-in-r-682fc9b1f119>.

Now let's remove stop words that don't help us learn much about what people are saying about the state standards.

```{r stop-unigrams}
ss_tokens_2 <- anti_join(ss_tokens_1,
                           stop_words,
                           by = "word")

head(ss_tokens_2)
```

Notice that we've specified the `by =` argument to look for matching words in the `word` column for both data sets and remove any rows from the `tweet_tokens` dataset that match the `stop_words` dataset. Remember when we first tokenized our dataset I conveniently chose¬†`output = word`¬†as the column name because it matches the column name¬†`word`¬†in the¬†`stop_words`¬†dataset contained in the `tidytext` package. This makes our call to¬†`anti_join()`simpler because¬†`anti_join()`¬†knows to look for the column named¬†`word`¬†in each dataset. However this wasn't really necessary since `word` is the only matching column name in both datasets and it would have matched those columns by default.

#### [Your Turn]{style="color: green;"}¬†‚§µ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Use the code chunk below to take a quick count of the most common tokens in our `ss_tweets_2` data frame to see if the results are a little more meaningful

```{r}
ss_tokens_2 %>%
  count(word, sort = TRUE)
```

#### Custom Stop Words

Notice that the nonsense word "amp" is among our high frequency words as well as some. We can create our own custom stop word list to to weed out any additional words that don't carry much meaning but skew our data by being so prominent.

Let's create a custom stop word list by using the simple `c()` function to combine our words. We can the add a filter to keep rows where words in our `word` column do NOT `!` match words `%in%` `my_stopwords` list:

```{r}
my_stopwords <- c("amp", "=", "+")

ss_tokens_3 <-
  ss_tokens_2 %>%
  filter(!word %in% my_stopwords)
```

Let's take a look at our top words again and see if that did the trick:

```{r}
ss_tokens_3 %>%
  count(word, sort = TRUE)
```

Much better! Note that we could extend this stop word list indefinitely. Feel free to use the code chunk below to try adding more words to our stop list.

```{r}
# your code here
```

## 3. EXPLORE

Calculating summary statistics, data visualization, and feature engineering (the process of creating new variables from a dataset) are a key part of exploratory data analysis. One goal in this phase is explore questions that drove the original analysis and develop new questions and hypotheses to test in later stages. In Section 3, we will calculate some very basic summary statistics from our tidied text, explore key words of interest to gather additional context, and use data visualization to identify patterns and trends that may not be obvious from our tables and numerical summaries. Topics addressed in Section 3 include:

-   3Word Counts. We focus primarily on the use of word counts and calculating proportions to to help us identify common words used to describe the most valuable aspects of online professional development offerings.

-   **Word Search**. We learn about the global regular expression parser, or¬†`grep`¬†package in R, to search for key words among our data set.

Word Counts

Word Clouds

## 4. MODEL

As highlighted in¬†[Chapter 3 of Data Science in Education Using R](https://datascienceineducation.com/c03.html) , the¬†**Model**¬†step of the data science process entails "using statistical models, from simple to complex, to understand trends and patterns in the data." The authors note that while descriptive statistics and data visualization during the¬†**Explore**¬†step can help us to identify patterns and relationships in our data, statistical models can be used to help us determine if relationships, patterns and trends are actually meaningful.

We Recall from the PREPARE section that the Rosenberg et al.¬†study was guide by the following questions:

1.  What is the public sentiment expressed toward the NGSS?

2.  How does sentiment for teachers differ from non-teachers?

3.  How do tweets posted to \#NGSSchat differ from those without the hashtag?

4.  How does participation in \#NGSSchat relate to the public sentiment individuals express?

5.  How does public sentiment vary over time?

Similar to an approach we'll be using in TM Learning Lab 3, @rosenberg2020 used sentiment scores from the SentiStrength lexicon to answer RQ \#1. To address the remaining questions the authors used a mixed effects model (also known as multi-level or hierarchical linear models via the {lme4} package in R.

Collectively, the authors found that:

1.  The SentiStrength scale indicated an overall neutral sentiment for tweets about the Next Generation Science Standards.

2.  Teachers were more positive in their posts than other participants.

3.  Posts including \#NGSSchat that were posted outside of chats were slightly more positive relative to those that did not include the \#NGSSchat hashtag.

4.  The effect upon individuals of being involved in the \#NGSSchat was positive, suggesting that there is an impact on individuals---not tweets---of participating in a community focused on the NGSS.

5.  Posts about the NGSS became substantially more positive over time.

## 5. COMMUNICATE

Knit, commit and push all changed files so that your Git pane is cleared up afterwards. You should confirm that what you committed and pushed are indeed in your repo that we will see by visiting your repo on GitHub.

### Reach üéâ

## References
