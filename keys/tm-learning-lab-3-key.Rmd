---
title: "A Tale of Two Standards"
subtitle: 'Text Mining Learning Lab 3'
author: "YOUR NAME HERE"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 5
    toc_float: yes
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. PREPARE

Ultimately, the aim of our work is to share the results of analysis with wider audience, whether in a peer-reviewed journal or reporting directly to education stakeholders. The latter is among particular importance in Learning Analytics. In *Learning Analytics Goes to School*, @krumm2018 outlined the following 3-step process for communicating with education stakeholders what you have learned through analysis:

1.  **Select**. Communicating what one has learned involves selecting among those analyses that are most important and most useful to an intended audience, as well as selecting a form for displaying that information, such as a graph or table in static or interactive form, or **data product.**
2.  **Polish**. After creating initial versions of data products, research teams often spend time refining or polishing them, by adding or editing titles, labels, and notations and by working with colors and shapes to highlight key points.
3.  **Narrate**. Writing a narrative to accompany the data products involves, at a minimum, pairing a data product with its related research question, describing how best to interpret the data product, and explaining the ways in which the data product helps answer the research question.

### 1a. Questions & Analyses

Remember that the questions of interest that we want to focus on our for our selection, polishing, and narration include:

1.  What is the public sentiment expressed toward the NGSS?
2.  How does sentiment for NGSS compare to sentiment for CCSS?

To address questions 1 and 2, we're going to focus our analyses and data products on the following:

1.  **Analyses**. For RQ1, let's try and replicate as closely as possible the analysis by @rosenberg2021a. We will clean up our analysis from Learning Lab 2 and calculate a single sentiment score using the AFINN Lexicon for the entire tweet and label it positive or negative based on that score. We also want to highlight how regardless of the lexicon selected, NGSS tweets contain more positive words than negative, so we'll also polish our previous analyses and calculate the percentages of positive and negative words for the NGSS specifically.
2.  **Data Products**. I know these are shunned in the world of data viz, but I think a **pie chart** will actually be an effective way to quickly communicate the proportion of positive and negative tweets among the Next Generation Science Standards. And for our analyses with the `bing`, `nrc`, and `loughan` lexicons, we'll create 100% stacked bars showing the percentage of positive and negative words among all tweets for the NGSS and CCSS.

### 1b. Load Libraries

For this learning lab, we'll need only to packages to wrangle, analyze, and summarize our text-based data in response to

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

In the code chunk below, load the two packages we'll need for working with text as data using tidy data principles. **Hint:** They both begin with "tidy".

```{r}
library(tidytext)
library(tidyverse)
library(here)
```

## 2. WRANGLE

The majority wrangling for Learning Lab 3 will build upon functions already introduced and practiced for during Learning Labs 1 & 2. You will be expected to do the bulk of the wrangling for this section, but will be provided guidance when new approaches for wrangling are introduced.

### 2a. Import Data

For Learning Lab 3, let's start again with our raw data and import into the R environment. We'll be working with the `ccss-tweets.csv` and `ngss-tweets.csv` located in the data folder.

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Use the code chunk below to import the the NGSS and CCSS tweets and save as `ccss_tweets` and `ngss_tweets` respectively.

```{r}
ccss_tweets <- read_csv(here("data", "ccss-tweets.csv"))

ngss_tweets <- read_csv(here("data", "ngss-tweets.csv"))
```

### 2b. Combine Tweets

To prepare for our analyses, first we need to rebuild the `ss_tweets` dataset from my `ngss_tweets` and `ccss_tweets` that we just imported. To do so, we'll need to do the following for each data frame:

-   Since, we'll only be using the unique id for each tweet and the actual content of the tweet, select both the `id` and the `text` columns.

-   Next, we'll want to created a new column to indicate the standards these tweets reference, i.e. "ccss" and "ngss".

-   Finally, we need to combine these two data frames into a single data frames containing tweets from both sets of standards.

Use the code chunk below to create two data frames called `ngss_text` and `ccss_text` that contains only three columns: `id`, `standards`, and `text` and combine these into a single data frame called `ss_tweets`.

The functions have been included for you, however you will need to supply the arguments:

```{r}
ngss_text <-
  ngss_tweets %>%
  select(id, text) %>%
  mutate(standards = "ngss")

ccss_text <-
  ccss_tweets %>%
  select(id, text) %>%
  mutate(standards = "ccss")

ss_tweets <- union(ngss_text, ccss_text)
```

The unique tweet `id` is important because like Rosenberg et al., we want to calculate an overall sentiment score for each tweet, rather than for each word. Note that the VADER package will do this as well, but the AFINN lexicon that uses the -5 to 5 point scale is the most similar to the SentiStrength lexicon used by @rosenberg2021a .

Before moving on, check the head and tail of the new `ss_tweets` data frame to make sure it includes both NGSS and CSSS tweets:

```{r}
head(ss_tweets)
tail(ss_tweets)
```

Before we can add our sentiments scores to each word in our set of tweets, we'll need to tidy our `tweets` again and calculate our `sentiment` scores. What will be different from the previous lab, however, is that in order to approximate the analysis by @rosenberg2021a, we'll calculate sentiments scores for the entire tweet and assign an overall sentiment for each tweet as either positive or negative.

### 2c. Tidy Tweets

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Use `unnest_tokens` to tidy our `ss_tweets`, remove stop words, and add `afinn` scores to each word similar to what we did in Learning Lab 2. The necessary functions have been included for you, but you will need to supply the appropriate arguments:

```{r, message=F}
tidy_tweets <- ss_tweets %>%
  unnest_tokens(output = word, 
                input = text, 
                token = "tweets")  %>% 
  anti_join(stop_words, by = "word")

tidy_tweets
```

## 3. ANALYZE

### 3a. Determine Tweet Sentiment

Before we can polish our analysis of tweet sentiment, recall from Learning Lab 2 that we'll first need to assign assign each lexicon to their respective names:

```{r}
afinn <- get_sentiments("afinn")

bing <- get_sentiments("bing")

nrc <- get_sentiments("nrc")

loughran <- get_sentiments("loughran")
```

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Similar to the VADER package, we want to calculate a single score for each tweet, rather than for each word. To do that, we'll use the `group_by()` and `summarize()` functions group by `standards` and `id` and then use the `sum()` function to add the total sentiment `value` for each tweet.

Again, the functions have been provided, but you will need to supply the arguments:

```{r, message=F}
afinn_score <- tidy_tweets %>% 
  inner_join(afinn, by = "word") %>%
  group_by(standards, id) %>% 
  summarise(value = sum(value))

afinn_score
```

In the output above, we can see that our first tweet was total value of -2 so would be considered negative.

Like Rosenberg et al., we want to add a flag for whether the tweet is "positive" or "negative". To do this, we need to:

-   remove all "neutral" tweets with a value equal to 0, or in other words, keep all tweets with a value not equal to 0;

-   add the `mutate` function to create a new `sentiment` column to indicate whether that tweets was positive or negative;

-   use an `if_else` function from the `dplyr` package for our sentiment value adds "negative" to the `sentiment` column if the score in the `value` column of the corresponding row is less than 0, and a "positive" to the row for all other values.

```{r}
afinn_sentiment <- afinn_score %>%
  filter(value != 0) %>% #hint: keep tweets with value not equal to 0
  mutate(sentiment = if_else(value < 0, "negative", "positive"))

afinn_sentiment
```

### 3b. Compute Sentiment Ratios

Finally, we're ready to compute our ratio. To do so, we need to:

-   use the `group_by()` function to group by `standards`;

-   add the `count()` function to count the number of "positive" and "negative" tweets in the `sentiment` column for each standards;

-   separate our sentiment counts into columns for positive and negative tweet counts using the `spread()` function; and finally,

-   create a new column that to compute the `ratio` of `negative` to `postive` tweets.

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Supply the appropriate arguments to each of the functions to calculate our AFINN ration of negative to positive tweets.

```{r}
afinn_ratio <- afinn_sentiment %>% 
  group_by(standards) %>% 
  count(sentiment) %>% 
  spread(sentiment, n) %>%
  mutate(ratio = negative/positive)

afinn_ratio
```

How would you interprets these ratios for sentiment to an education stakeholder? Write you responses in the space below:

-   There are about two negative tweets for ever one positive tweet for the CSSS.

-   For the NGSS, only about 15 in every 100 tweets is negative.

### 3c. Compare Sentiment Lexicons

Finally, to address Question 2, we want to compare the percentage of positive and negative words contained in the corpus of tweets for the NGSS and CCSS standards using the four different lexicons to see how sentiment compares based on lexicon used.

First let's quickly assign sentiment for each of our lexicons to out tidied tweets. We've already read in our lexicons and created the `afinn_sentiment` data frame consisting of our tidied tweets with sentiment values above, so let's do the same for the remaining three:

```{r}
bing_sentiment <- get_sentiments("bing") %>%
  inner_join(tidy_tweets, bing, by = "word")

nrc_sentiment <- get_sentiments("nrc") %>%
  inner_join(tidy_tweets, nrc, by = "word")

loughran_sentiment <- get_sentiments("loughran") %>%
  inner_join(tidy_tweets, loughran, by = "word")

```

#### Create Lexicon Summaries

Next let's polish our previous summaries and create identical summaries for each lexicon that contains the following columns: `method`, `standards`, `sentiment`, and `n`, or word counts:

```{r}
summary_afinn2 <- afinn_sentiment %>% 
  group_by(standards) %>% 
  filter(value != 0) %>%
  mutate(sentiment = if_else(value < 0, "negative", "positive")) %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(method = "AFINN")

summary_bing2 <- bing_sentiment %>% 
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(method = "bing")

summary_nrc2 <- nrc_sentiment %>% 
  filter(sentiment %in% c("positive", "negative")) %>%
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(method = "nrc") 

summary_loughran2 <- loughran_sentiment %>% 
  filter(sentiment %in% c("positive", "negative")) %>%
  group_by(standards) %>% 
  count(sentiment, sort = TRUE) %>% 
  mutate(method = "loughran") 
```

#### Combine Data Frames

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Supply the missing arguments to combine those four data frames together using the `bind_rows` function again:

```{r}
summary_sentiment <- bind_rows(summary_afinn2,
                               summary_bing2,
                               summary_nrc2,
                               summary_loughran2) %>%
  arrange(method, standards) %>%
  relocate(method)

summary_sentiment
```

#### Sum Total Words

Let's create a new data frame that has the total word counts for each set of standards and each method and join that to my `summary_sentiment` data frame:

```{r}
total_counts <- summary_sentiment %>%
  group_by(method, standards) %>%
  summarise(total = sum(n))

sentiment_counts <- left_join(summary_sentiment, total_counts)

sentiment_counts
```

Finally, we'll add a new row that calculates the percentage of positive and negative words for each set of state standards:

```{r}
sentiment_percents <- sentiment_counts %>%
  mutate(percent = n/total * 100)

sentiment_percents
```

#### Create Stacked Bar Chart

#### [Your Turn]{style="color: green;"} ⤵ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Now that we have our sentiment percent summaries for each lexicon, supply the missing arguments for the functions below to create 100% stacked bar charts for each lexicon:

```{r}
sentiment_percents %>%
  ggplot(aes(x = standards, y = percent, fill=sentiment)) +
  geom_bar(width = .8, stat = "identity") +
  facet_wrap(~method, ncol = 1) +
  coord_flip() +
  labs(title = "Public Sentiment on Twitter", 
       subtitle = "The Common Core & Next Gen Science Standards",
       x = "State Standards", 
       y = "Percentage of Words")
```

And finished! The chart above clearly illustrates that regardless of sentiment lexicon used, the NGSS contains more positive words than the CCSS lexicon.

## 4. MODEL

Recall from the PREPARE section that the Rosenberg et al. study was guide by the following questions:

1.  What is the public sentiment expressed toward the NGSS?
2.  How does sentiment for teachers differ from non-teachers?
3.  How do tweets posted to \#NGSSchat differ from those without the hashtag?
4.  How does participation in \#NGSSchat relate to the public sentiment individuals express?
5.  How does public sentiment vary over time?

Similar to our sentiment summary using the AFINN lexicon, the Rosenberg et al. study used the -5 to 5 sentiment score from the SentiStrength lexicon to answer RQ \#1. To address the remaining questions the authors used a mixed effects model (also known as multi-level or hierarchical linear models via the lme4 package in R.

Collectively, the authors found that:

1.  The SentiStrength scale indicated an overall neutral sentiment for tweets about the Next Generation Science Standards.
2.  Teachers were more positive in their posts than other participants.
3.  Posts including \#NGSSchat that were posted outside of chats were slightly more positive relative to those that did not include the \#NGSSchat hashtag.
4.  The effect upon individuals of being involved in the \#NGSSchat was positive, suggesting that there is an impact on individuals---not tweets---of participating in a community focused on the NGSS.
5.  Posts about the NGSS became substantially more positive over time.

## 5. COMMUNICATE

In this learning lab, we focused we focused on the use of lexicons for text analysis and introduce the {vader} package to compare the sentiment of tweets about the NGSS and CCSS state standards. Below, add a few notes in response to the following prompts:

1.  One thing I took away from this learning lab:

    -   

2.  One thing I want to learn more about:

    -   

Congratulations - you've made it to the end! To complete your work, you can click the drop down arrow at the top of the file, then select "Knit top HTML". This will create a report in your Files pane that serves as a record of your code and its output you can open or share.

## R-each (Optional)

In our previous learning labs and above, we primarily looked at table outputs of results for our word and sentiment counts. When communication findings with education stakeholders like policy makers and school administrators, a simple chart is often more effective to illustrate findings.

In the code chunk below, supply the missing functions, operators, and arguments to make a pie chart of the proportion of positive and negative tweets for the Next Generation Science Standards.

```{r}
afinn_counts <- afinn_sentiment %>%
  filter(standards == "ngss") %>% 
  count(sentiment)

afinn_counts %>%
ggplot(aes(x="", y=n, fill=sentiment)) +
  geom_bar(width = .6, stat = "identity") +
  labs(title = "Next Gen Science Standards",
       subtitle = "Proportion of Positive & Negative Tweets") +
  coord_polar(theta = "y") +
  theme_void()

```

#### 
