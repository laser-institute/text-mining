---
title: "Large Language Model"
subtitle: "Lab 3: Code-Along"
format:
  revealjs: 
    slide-number: c/t
    progress: true
    chalkboard: 
      buttons: false
    preview-links: auto
    theme: [default, css/laser.scss]
    logo: img/LASERLogoB.png
    width: 1920
    height: 1080
    margin: 0.05
    footer: <a href=https://www.go.ncsu.edu/laser-institute>go.ncsu.edu/laser-institute
highlight-style: a11y
editor: visual
---

## Agenda

1.  Load Libraries and Import Data

2.  Access Model through API

3.  Load Model

4.  Tune Model with Prompt Refinement

    -   Zero shot

    -   One shot and chain of thought

## Load Libraries

-   `openai`

-   `backoff`

```{python}
#| echo: true

!pip install openai
!pip install backoff
```

```{python}
#| echo: true

import openai
import backoff
import time
import pandas as pd
from tqdm import tqdm
```

## Import Data

```{python}
#| echo: true

DATA_FILENAME = 'ml literacy.csv'
df = pd.read_csv(DATA_FILENAME, encoding='utf-8')
token_usage = 0 #This initializes a variable token_usage to keep track of the total number of tokens used during the process.
print(df)
```

## Access Model through API

```{python}
#| echo: false

import dotenv
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()

# Retrieve the API key from environment variables
gpt_key = os.getenv("gpt_key")
```

Find your own GPT API key: [How to get your ChatGPT API key (4 steps)](https://www.merge.dev/blog/chatgpt-api-key)

```{python}
#| echo: true
#gpt_key = "xxx" #use your own key
```

Create a client to interact with OpenAI:

```{python}
#| echo: true

from openai import OpenAI

client = OpenAI(api_key=gpt_key)
```

Implement a backoff strategy to handle rate limits or transient issues such as network timeouts or temporary server errors when interact with GPT model:

```{python}
#| echo: true

@backoff.on_exception(backoff.expo, openai.RateLimitError)
def completions_with_backoff(**kwargs):
    '''This function will automatically try the api call again if it fails.'''
    return client.chat.completions.create(**kwargs)
```

## Load Model

```{python}
#| echo: true

gpt_org = "org-pF1Od41p8zEN8oeGTSxATXei"
gpt_host = "https://api.openai.com/v1"
gpt_model = "gpt-3.5-turbo"
model = gpt_model
MAX_TOKENS = 100
```

## Tune Model with Prompt Refinement

### Zero Shot

```{python}
#| echo: true

responses = [] #This initializes an empty list to store responses generated from the OpenAI GPT model.

for i, row in tqdm(df.iterrows(), total=len(df)):
    value = str(row['response']) #Retrieves the value of the 'response' column from the current row and converts it to a string.
    
    #Constructs a prompt by combining a prefix and the 'response' value from the row.
    prefix = "Based on the student's response provided in:"
    postfix = "evaluate and return only the student's machine learning literacy level. The assessment should categorize the student into one of the following three levels: novice, intermediate, or advanced."
    prompt = ' '.join([prefix, value, postfix])

    # Creates a list of messages containing the prompt.
    messages = [{"role": "user", "content": prompt}] 
    
    #Attempts to generate a completion using the completions_with_backoff function, passing the GPT model, messages, and maximum tokens as arguments.
    try:
        completion = completions_with_backoff(
            model=model,
            messages=messages,
            max_tokens=MAX_TOKENS
        )
    except openai.APIError as e:
        print('ERROR: while getting accessing API.')
        print(f'Failed on item {i}.')
        print(e)
        print("Prompt:", prompt)
        raise e
    
    #Retrieves the response from the completion and appends it to the responses list.
    response = completion.choices[0].message.content
    responses.append(response)
    
    #Updates the token_usage counter with the total tokens used in the completion.
    token_usage += completion.usage.total_tokens

    # Need to wait to not exceed rate limit
    time.sleep(5)
```

## Check model output with zero shot

```{python}
df["zeroshort"] = responses

print(df)
```

## Tune Model with Prompt Refinement

### One shot and chain of thought

```{python}
#| echo: true

responses = [] #This initializes an empty list to store responses generated from the OpenAI GPT model.

for i, row in tqdm(df.iterrows(), total=len(df)):
    value = str(row['response']) #Retrieves the value of the 'response' column from the current row and converts it to a string.
    
    #Constructs a prompt by combining a prefix and the 'response' value from the row.
    prefix = "Based on the student's response provided in:"
    
    # Define the base instructions
    instructions = (
        "Evaluate and return only the student's machine learning literacy level. "
        "The assessment should categorize the student into one of the following three levels: "
        "novice, intermediate, or advanced."
    )
    
    # Define the example and chain of thought for novice level
    novice_example = (
        "Novice: 'Machine learning is kind of intelligence where computers learn on their own.'"
    )
    chain_of_thought = (
        "Chain of Thought: In this example, the student's description of machine learning "
        "focuses on a broad, generalized understanding without delving into specifics about how "
        "machine learning algorithms work or are applied. The emphasis on 'intelligence' and "
        "'learning on their own' suggests a lack of detailed knowledge about the processes and "
        "techniques involved in machine learning, which is characteristic of a novice level of understanding."
    )
    
    # Define the reminder
    reminder = (
        "Remember, your task is to specify the literacy level as either "
        "novice, intermediate, or advanced without adding any additional commentary or explanation."
    )
    
    # Combine all parts into the final postfix message
    postfix = f"{instructions} To guide your evaluation, consider the following example and the associated chain of thought process: {novice_example} {chain_of_thought} {reminder}"
            
    prompt = ' '.join([prefix, value, postfix])

    # Creates a list of messages containing the prompt.
    messages = [{"role": "user", "content": prompt}] 
    
    #Attempts to generate a completion using the completions_with_backoff function, passing the GPT model, messages, and maximum tokens as arguments.
    try:
        completion = completions_with_backoff(
            model=model,
            messages=messages,
            max_tokens=MAX_TOKENS
        )
    except openai.APIError as e:
        print('ERROR: while getting accessing API.')
        print(f'Failed on item {i}.')
        print(e)
        print("Prompt:", prompt)
        raise e
    
    #Retrieves the response from the completion and appends it to the responses list.
    response = completion.choices[0].message.content
    responses.append(response)
    
    #Updates the token_usage counter with the total tokens used in the completion.
    token_usage += completion.usage.total_tokens

    # Need to wait to not exceed rate limit
    time.sleep(5)
```

## Check model output with one short and chain of thoughts

```{python}
df["oneshot"] = responses

print(df)
```
