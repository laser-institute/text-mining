---
title: "Lab 1: Text Mining Basics with Python"
author: "LASER Institute"
date: today 
format:
  html:
    toc: true
    toc-depth: 4
    toc-location: right
theme:
  light: simplex
  dark: cyborg
editor: visual
#bibliography: lit/references.bib
---

## 0. INTRODUCTION

In this case study, we'll **"walk through"** a basic research workflow, or data analysis process, modeled after the Data-Intensive Research Workflow from *Learning Analytics Goes to School* (Krumm et al., 2018):

![Figure 2.2 Steps of Data-Intensive Research Workflow](img/workflow.png)

Each walkthrough will focus on a basic analysis using text mining techniques that you'll be expected to reproduce, and apply to a new research question in independent practice, using the provided dataset or dataset of your own choosing.

We will focus on analysis of open-ended survey items from an evaluation of the North Carolina Department of Public Instruction (NCDPI) online professional development offered as part of the state's [Race to the Top](https://www.fi.ncsu.edu/projects/evaluation-of-race-to-the-top/) efforts.

### Walkthrough Focus

Our focus will be on getting our text "tidy" so we can perform some basic word counts, look at words that occur at a higher rate in a group of documents, and examine words that are unique to those document groups. Specifically, the Walkthrough will cover the following workflow topics:

1.  **Prepare**: Prior to analysis, it's critical to understand the context and data sources you're working with so you can formulate useful and answerable questions. You'll also need to set up a "Project" for our walkthrough.
2.  **Wrangle**: Wrangling data entails the work of manipulating, cleaning, transforming, and merging data. In section 2 we focus on reading, reducing, and tidying our data.
3.  **Explore**: In section 3, we use simple summary statistics, more sophisticated approaches like term frequency-inverse document frequency (tf-idf), and basic data visualization to explore our data and see what insight it provides in response to our question.

------------------------------------------------------------------------

## 1. PREPARE

Prior to analysis, it's critical to understand the context and data sources available so you can formulate useful questions that can be feasibly addressed by your data. For this section, we'll focus on the following topics:

a.  **Context**. We learn a little bit about the North Carolina Race to the Top grant and the evaluation of the online professional development work, including questions and findings from the 2012 report.
b.  **Questions.** What aspects of online professional development offerings do teachers find most valuable?
c.  **Project Setup.** We visit Chapter 6 from DSIEUR to set up a Project in RStudio and Python script for analysis.

### 1a. Some Context

#### RTT Online Professional Development Evaluation

![](img/opd.png)

[link to the report](https://github.com/laser-institute/essential-readings/blob/main/text%20mining/TMbasics.pdf)

North Carolina was one of 12 recipients of the 2010 federal Race to the Top (RttT) grants, bringing nearly \$400 million to the state's public school system. Over the course of four years, NC's RttT coordinated a set of activities and policy reforms designed to collectively improve the performances of students, teachers, leaders, and schools.

The North Carolina Race to the Top (RttT) proposal (North Carolina Office of the Governor, 2010) specifies that the state's Professional Development Initiative will focus on the "use of e-learning tools to meet the professional development needs of teachers, schools, and districts" (p. 191). It points to research demonstrating that "well-designed and -implemented online professional development programs are not only valued by teachers but also positively impact classroom practices and student learning."

**Data Source & Analysis**

The evaluation used a wide range of data sources including interviews, document review, site analytics, and surveys, which we'll focus on for this walkthrough. Survey protocols were designed in cooperation with NCDPI to systematically collect information about local professional development, state-level supports, use of available RttT professional development resources, and organizational and classroom practices in the schools, which will serve as a baseline to assess changes over the period of the North Carolina RttT initiatives.

Quantitative analyses focused primarily on descriptive analysis of item-level responses. In addition, quantitative data from these surveys were analyzed to examine patterns in responses by participants' role, event type (e.g., module, webinar, resource), and region. Responses to open-ended survey items of the Online Resources Survey were manually coded by their relation to each Learning Forward professional development standard.

Note that the dataset we'll be using for analysis in this walkthrough is exported as is from Qualtrics with personal identifiers, select demographics, metadata, and closed-ended responses removed.

**Summary of Findings**

Approximately half of the state's educators completed at least one online module by the end of the 2011-12 school year. Overall, most participants agreed that the webinars and modules were relevant to their professional development needs, though some content was redundant with prior PD activities and not always content- or grade-specific, and some modules did not meet national standards. Most online modules were completed independently and not in Professional Learning Community groups.

A common theme from focus groups and open-ended survey responses was the convenience of online professional development. One teacher in a focus group stated, "I liked the format. And the way that it was given, it was at your own pace, which works well for our schedules..." Educators also frequently cited that the information and resources provided through the modules improved their understanding of the new standards and the teacher evaluation process. Webinar participants appreciated the useful, updated information presented through a combination of PowerPoint slides and video clips.

While the majority of educators have indicated their satisfaction with these resources, the findings suggest that the use of these resources at both the state and local level was not wholly consistent with national standards for online professional development. Many LEAs likely needed additional guidance, training, support, technology tools, and/or content resources to ensure that local efforts contribute to the quality of the experiences for educators and that the vision for online professional development outlined in the state's RttT proposal is realized and can be sustained beyond RttT.

### 1b. Guiding Questions

The State's progress on designing and implementing online professional development was originally guided by the following (very) general evaluation questions:

1.  State Strategies: To what extent did the state implement and support proposed RttT professional development efforts?
2.  Short-Term Outcomes: What were direct outcomes of state-level RttT professional development efforts?

For this walkthrough, we'll use text mining to complement prior qualitative analyses conducted as part of the RttT Evaluation by examining responses to open-ended questions on the RttT Online PD Survey administered to over 15,000 NC educators.

Our (very) specific questions of interest for this walkthrough are:

1.  **What aspects of online professional development offerings do *teachers* find most valuable?**
2.  **How might resources differ in the value they afford teachers?**

Finally, one overarching question we'll explore throughout this lab, and that Silge and Robinson (2018) identify as a central question to text mining and natural language processing, is:

> How do we to **quantify** what a document or collection of documents is about?

### 1c. Set Up

Python has a series of powerful libraries or packages that provide a range of functions, making it easier to implement complex tasks and saving a lot of time. Before wrangling data, we need to install and import the necessary libraries, in this case, `pandas` and `nltk`, and for others, we can install later when needed. This can be done using `!pip install package_name` to install a library and `import package_name as alias`.

-   `!pip install` command: pip is the [package installer](https://packaging.python.org/guides/tool-recommendations/) for Python, you can install libraries using `pip install package_name` in the **terminal**. While when you're executing code in a Python code chunk, you would typically use the `!pip` command to install Python packages. The exclamation mark (**`!`**) is a special character used in Jupyter and other interactive environments to denote that the command should be executed in the system shell rather than in the Python interpreter.

-   `import` command: used to actually load libraries into your Python environment and make their functionality available for use in your code after downloading and installing those libraries with `!pip install`.

-   [`pandas`](https://pandas.pydata.org): a powerful and widely-used Python library for data manipulation and analysis. It provides data structures and functions needed to work seamlessly with structured data, such as tables and time series.

-   [`nltk`](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.nltk.org/&ved=2ahUKEwid7Pn-97-GAxWKMVkFHSOqBuUQFnoECB4QAQ&usg=AOvVaw1cuWsEY8ZUmW75nSDfXo7m): a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language. It supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.

```{python}
!pip install pandas 
!pip install nltk
```

```{python}
import pandas as pd
import nltk
```

------------------------------------------------------------------------

## 2. WRANGLE

In general, data wrangling involves some combination of cleaning, reshaping, transforming, and merging data (Wickham & Grolemund, 2017). The importance of data wrangling is difficult to overstate, as it involves the initial steps of going from raw data to a dataset that can be explored and modeled (Krumm et al, 2018).

a.  **Reading Data**. Before working with data, we need to "read" it into Python. It also helps to inspect your data.
b.  **Data Reduction**. We focus on tools from the `pandas` library to view, rename, select, slice, and filter our data in preparation for analysis.
c.  **Natural Language Toolkit (NLTK)**. We'll learn how to use the `nltk` library to tokenize our text in order to create a data frame to use for analysis.

### 2a. Read, View, Write Data: **Reading Text Files**

To read text from a file, we can use the built-in `open()` function and various file reading methods, such as `read()`, `readline()`, or `readlines()`.

```{python}
#| echo: false
with open('data/opd_survey.csv', 'r') as file:
    content = file.read()
    len(content) # Number of characters in the file
```

When working with structured document in Python, a tidy way to go is the use of the `pandas` library, which is a powerful data manipulation and analysis library in Python. Ensure you have the library installed before running the following code:

```{python}
opd_sentences = ["levels of questioning and revised blooms", 
                  "None, really.",
                  "Understanding the change",
                  "overview of reasons for change",
                  "Knowing what needs to be done."]

df = pd.DataFrame(opd_sentences, columns=['text'])
df
```

```{python}
# find the number of characters for each string in df['text']
df['text'].str.len()
```

```{python}
# find the number of tokens for each string in df['text']
df['text'].str.split().str.len()
```

```{python}
# find which entries contain the word 'Understanding'
df['text'].str.contains('Understanding')
```

You can use the following Python code to read the CSV file and display the first 4 rows along with the column names:

```{python}
# Specify the file path
csv_file_path = 'data/opd_survey.csv'

# Read the CSV file into a pandas DataFrame
opd_survey = pd.read_csv(csv_file_path)

# Display the first 4 rows
print("First 4 rows:")
print(opd_survey.head(4))

# Display the column names
print("\nColumn names:")
print(opd_survey.columns)
```

### **2b. Writing to Text Files**

To write text to a file, use the `open()` function with the appropriate mode ('w' for write, 'a' for append) and then write the content using methods like `write()`.

```{python}
with open('data/output.txt', 'w') as file:
    file.write('This is a sample text.')
```

```{python}
opd_survey.to_csv("data/opd_survey_copy.csv", index=False)
print(f"DataFrame has been written to opd_survey_copy.csv")
```

### 2c. Subset Columns

To begin, let's select Role, Resources (named as Resource...6 in the data frame), and Q21 columns and store as new data frame since those respectively pertain to educator role, OPD resource they are evaluating, and, as illustrated by the second row,

```{python}
# Subset the DataFrame to select specific columns
selected_columns = ['Role', 'Resource', 'Q21']
opd_survey = opd_survey[selected_columns]

# Display the subsetted DataFrame
print(opd_survey.head())

# Create a copy of the DataFrame
opd_selected = opd_survey.copy()
```

### 2d. Rename Columns

Notice that Q21 is not a terribly informative variable name. Let's now take our `opd_selected` data frame and use the `rename()` function to change the name from Q21 to "text" and save it as `opd_renamed`.

```{python}
# Rename the 'Q21' column to 'text'
opd_selected.rename(columns={'Q21': 'text'}, inplace=True)
opd_renamed = opd_selected.copy()

# Display the DataFrame with the renamed column
print(opd_renamed.head())
```

This code will rename the 'Q21' column to 'text' in the DataFrame. The `inplace=True` parameter ensures that the modification is done on the original DataFrame without the need to assign it back to a variable.

After running this code, you will see the updated DataFrame with the column name changed to 'text'.

### 2e. Subset Rows

Now let's deal with the legacy rows that Qualtrics outputs by default, which are effectively 3 sets of headers. The `opd_renamed.iloc[2:]` line is used to select all rows starting from the third row (index 2) onward, effectively removing the first two rows from the DataFrame.

```{python}
# Remove the first two rows from the opd_renamed DataFrame
opd_sliced = opd_renamed.iloc[2:].copy()

# Display the DataFrame with the renamed column and without the first two rows
print(opd_sliced.head())
```

Now let's take our `opd_sliced` and remove any rows that are missing data, as indicated by an `NA`.

```{python}
# Remove rows with missing data from the copied DataFrame
opd_sliced.dropna(inplace=True)

# Display the DataFrame with the renamed column and without rows with missing data
print(opd_sliced.head())
```

The `opd_sliced.dropna(inplace=True)` line removes any row containing at least one NaN value in any column. The `inplace=True` parameter modifies the DataFrame in place, without the need to assign it back to a variable.

This will give you a DataFrame (`opd_sliced`) without any rows that have missing data.

```{python}
opd_complete = opd_sliced.copy()
```

Finally, since we are only interested in the feedback from teachers, let's also filter our dataset for only participants who indicated their `Role` as "Teacher".

To filter the DataFrame `opd_complete` for rows where the 'Role' column is equal to "Teacher," you can use boolean indexing. Here's how you can achieve that:

```{python}
# Filter the DataFrame for rows where 'Role' is equal to "Teacher"
opd_teacher = opd_complete[opd_complete['Role'] == 'Teacher']

# Display the filtered DataFrame
print(opd_teacher.head())
```

That was a lot of code we just wrote to end up with `opd_teacher`. Let's review:

```{python}
# Subset the DataFrame to select specific columns
selected_columns = ['Role', 'Resource', 'Q21']
opd_survey = opd_survey[selected_columns]

# Create a copy of the DataFrame
opd_selected = opd_survey.copy()

# Rename the 'Q21' column to 'text'
opd_selected.rename(columns={'Q21': 'text'}, inplace=True)
opd_renamed = opd_selected.copy()

# Remove the first two rows from the opd_renamed DataFrame
opd_sliced = opd_renamed.iloc[2:].copy()

# Remove rows with missing data from the copied DataFrame
opd_sliced.dropna(inplace=True)

opd_complete = opd_sliced.copy()

# Filter the DataFrame for rows where 'Role' is equal to "Teacher"
opd_teacher = opd_complete[opd_complete['Role'] == 'Teacher'].copy()

# Display the filtered DataFrame
print(opd_teacher.head())
```

### **2f. Tokenization**

Tokenization involves breaking text into individual units, such as words or sentences. The `nltk` library provides powerful tools for tokenization.

```{python}
from nltk.tokenize import word_tokenize

# Download the NLTK data for tokenization
nltk.download('punkt')

text = 'Tokenization is essential for natural language processing.'
tokens = word_tokenize(text)
print(tokens)
```

#### Tokenize Text

In order to tidy our text, we need to break the text into individual tokens (a process called tokenization) and transform it to a tidy data structure.

After all the work we did prepping our data, this is going to feel a little anticlimactic.

Let's go ahead and tidy our text and save it as `opd_tidy`:

```{python}
#from nltk.tokenize import word_tokenize
from nltk.tokenize import RegexpTokenizer

# Tokenize the 'text' column and create a new column 'word'
# Use RegexpTokenizer to remove punctuation and convert to lowercase
tokenizer = RegexpTokenizer(r'\w+')
#opd_teacher['word'] = opd_teacher['text'].apply(lambda x: word_tokenize(str(x)))

opd_teacher['word'] = opd_teacher['text'].apply(lambda x: [token.lower() for token in tokenizer.tokenize(str(x))])

# Explode the 'word' column to transform each row into individual words
opd_tidy = opd_teacher.explode('word')

# Drop the original 'text' column
opd_tidy.drop(columns=['text'], inplace=True)

# Display the DataFrame with each row representing individual words, without punctuation, and in lowercase
print(opd_tidy.head())
```

This code uses the `explode` function to transform the 'word' column into individual rows while maintaining the index values. The original 'text' column is then dropped from the resulting DataFrame.

You can use the `nltk` library for further processing the 'word' column to remove punctuation and convert tokens to lowercase. The `nltk` library provides a convenient method called `RegexpTokenizer` for this purpose.

In this code, the RegexpTokenizer is used to remove punctuation, and a list comprehension is used to convert the tokens to lowercase.

#### Remove Stop Words

One final step in tidying our text is to remove words that don't add much value to our analysis (at least when using this approach) such as "and", "the", "of", "to" etc. The `nltk` library provides a list of common stop words. .

Let's take a look at these common stop words so we know what we're getting rid of from our `opd_tidy` dataset.

```{python}
from nltk.corpus import stopwords

# Download the NLTK data for stop words
nltk.download('stopwords')

# Get a list of English stop words
stop_words = set(stopwords.words('english'))

# Print the stop words being removed
print("Stop words being removed:", stop_words)
```

Let's remove rows from our `opd_tidy` data frame that contain matches in the `word` column with those in the `stop_words` dataset and save it as `opd_clean` since we were done cleaning our data at this point.

```{python}
# Remove stop words from the 'word' column
opd_teacher['word'] = opd_teacher['word'].apply(lambda x: [token for token in x if token not in stop_words])

# Explode the 'word' column to transform each row into individual words
opd_tidy = opd_teacher.explode('word')

# Drop the original 'text' column
opd_tidy.drop(columns=['text'], inplace=True)

# Display the DataFrame with each row representing individual words, without punctuation, lowercase, and without stop words
print(opd_tidy.head())

opd_clean = opd_tidy.copy()
```

This code uses the **`stopwords.words('english')`** to get a list of English stop words and then removes these stop words from the 'word' column.

#### Comprehension Check

1.  Tidy your opd_benefits data by tokenizing your text and removing stop words.
2.  How would you change the code in the Subset Columns section if you wanted to analyze how educators are are using the online resources instead of the most beneficial aspects?
3.  How would you rewrite the code in the Subset Rows section if you were interested in looking at School Executive responses instead of teachers?
4.  Rewrite the piped code in the Code Reduction section to prepare a data frame for tidying that includes all Roles, not just teachers, but excludes the Resource column. Assign it to `opd_benefits` for later use.

## 3. EXPLORE

As highlighted in both DSEIUR and Learning Analytics Goes to School, calculating summary statistics, data visualization, and feature engineering (the process of creating new variables from a dataset) are a key part of exploratory data analysis. One goal in this phase is explore questions that drove the original analysis and develop new questions and hypotheses to test in later stages. In Section 6, we will calculate some very basic summary statistics from our tidied text, explore key words of interest to gather additional context, and use data visualization to identify patterns and trends that may not be obvious from our tables and numerical summaries. Topics addressed in Section 6 include:

-   **Summary Stats**. We focus primarily on the use of word counts and calculating proportions to help us identify common words used to describe the most valuable aspects of online professional development offerings.
-   **Word Search**. We learn about the global regular expression parser to search for key words among our data set.
-   **Data Visualization**. Finally, we wrap up this walkthrough by creating wordclouds, bar plots, and small multiple charts to explore patterns and trends that would be difficult to distinguish otherwise.

### 3a. Summary Stats

Prior to making any data visualization, we revisit our or overarching question guiding most of our efforts in this lab, "How do we **quantify** what a text is about?"

#### Word Counts

As highlighted in [Word Counts are Amazing](https://tedunderwood.com/2013/02/20/wordcounts-are-amazing/), one simple but powerful approach to text analysis is counting the frequency in which words occur in a given collection of documents, or corpus.

Now that we have our original survey data in a tidy text format, we can use the `count()` function from the `pandas` package to find the most common words used by teachers when asked, "What was the most beneficial/valuable aspect of this online resource?"

Let's print out the word count of the top 10 words after preprocessing the text, using the `value_counts` method in pandas. Here's the code to achieve that:

```{python}
# Count the occurrences of each word across all the resources
words_count = opd_clean['word'].value_counts().reset_index()
words_count.columns = ['word', 'count']

# Print the word count of the top 10 words
print("Top 10 words and their counts:")
print(words_count.head(10))
```

This code uses the `value_counts` method on the 'word' column to count the occurrences of each word and then prints the top 10 words and their counts.

Going back to findings from the original report, a strategy as simple basic word counts resulted in key words consistent with findings from the qualitative analysis of focus-group transcripts and open-ended survey responses:

> Educators frequently cited that the **information and resources** provided through the modules improved their understanding of the new standards and the teacher evaluation process.

See also this finding around video clips:

> Webinar participants appreciated the useful, updated information presented through a combination of PowerPoint slides and **video clips**.

One notable distinction between word counts and more traditional qualitative analysis is that broader themes like "convenience" often are not immediately apparent in words counts, but rather emerges from responses containing words like "pace", "format", "online", "ease", and "access".

> A common theme from focus groups and open-ended survey responses was the **convenience** of online professional development. One teacher in a focus group stated, "I liked the format. And the way that it was given, it was at your own pace, which works well for our schedules..."

To show the word count of the 'word' column based on each 'Resource' column, you can group the DataFrame by the 'Resource' column and then apply the `value_counts` function. Here's the code for that:

```{python}
# Group by 'Resource' and count the occurrences of each word
resource_word_counts = opd_clean.groupby(['Resource', 'word']).size().reset_index(name='word_count')

# Display the word count based on each 'Resource' column
print(resource_word_counts)
```

This code uses `groupby` on both 'Resource' and 'word' columns and then applies the `size` function to count the occurrences of each word within each 'Resource' group. The result is a DataFrame with columns 'Resource', 'word', and 'word_count', representing the word count sorted by different 'Resource'.

#### Word Frequencies

One common approach to facilitate comparison across documents or groups of text, in our case responses by Online Resource type, is by looking at the frequency that each word occurs among all words for that document group. This also helps to better gauge how prominent the same word is across different groups.

To show the proportion of each word group by resource, you can calculate the proportion by dividing the word count by the total number of words in each resource group:

```{python}
# Group by 'Resource' and count the occurrences of each word
resource_word_counts = opd_clean.groupby(['Resource', 'word']).size().reset_index(name='word_count')

# Calculate the total number of words in each resource group
total_words_per_resource = opd_clean.groupby('Resource')['word'].apply(lambda x: len(x)).reset_index(name='total_words')

# Merge the word counts and total words per resource
resource_word_counts = pd.merge(resource_word_counts, total_words_per_resource, on='Resource')

# Calculate the proportion of each word
resource_word_counts['proportion'] = resource_word_counts['word_count'] / resource_word_counts['total_words']

# Display the result
print(resource_word_counts)
```

#### Term Frequency-Inverse Document Frequency

Term frequency-inverse document frequency (tf-idf) is an approach that takes this approach one step further.

As noted in [Tidy Text Mining with R](https://www.tidytextmining.com/tfidf.html):

> The statistic tf-idf is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.

Silge and Robinson note that, "The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of document... That is, tf-idf attempts to find the words that are important (i.e., common) in a text, but not too common."

To calculate the tf-idf statistics, the `TfidfVectorizer()` within the `sklearn.feature_extraction` module from the `scikit-learn` library will be used.

-   [`scikit-learn`](https://scikit-learn.org/stable/): an open-source Python library that implements a range of machine learning, pre-processing, cross-validation, and visualization algorithms using a unified interface. It is an open-source machine-learning library that provides a plethora of tools for various machine-learning tasks such as Classification, Regression, Clustering, and many more.

-   [`sklearn.feature_extraction`](https://scikit-learn.org/stable/api/sklearn.feature_extraction.html#module-sklearn.feature_extraction): a module that can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.

-   [`TfidfVectorizer()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html): itcan directly convert a collection of raw documents to a matrix of TF-IDF features.

```{python}
!pip install scikit-learn
from sklearn.feature_extraction.text import TfidfVectorizer
```

Because tf-idf can account through weighting for "too common" words like "and" or "but", when calculating tf-idf it is not necessary to remove stop words. So in this case, we will use original text in `opd_teacher`.

Now, we're ready to use the `TfidfVectorizer()` function to calculate a tf-idf statistic for each word and assess it's relative importance to a given online resource type:

```{python}
# Initialize the TF-IDF Vectorizer
vectorizer = TfidfVectorizer()

# Fit and transform the text data
tfidf_matrix = vectorizer.fit_transform(opd_teacher['text'])

# Convert the TF-IDF matrix to a DataFrame
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=opd_teacher['text'], columns=vectorizer.get_feature_names_out())

# Display the TF-IDF DataFrame
print(tfidf_df)
```

To make the TF-IDF DataFrame easy to read, let's display the individual words in rows rather than in columns (the below code block will run a while, the output would be like below figure):

```{python}
## Reshape the DataFrame for visualization using pd.melt
#tfidf_df = pd.melt(tfidf_df, id_vars=['text'], var_name='word', value_name='tf-idf')
#print(tfidf_df)
```

![](img/tf-idf.png)

#### Comprehension Questions

1.  Looking back at the Word Counts section, what other aspects of the online professional development resources to our word counts suggest teachers find valuable or beneficial?
2.  Instead of using the print() function for `resource_word_counts` and searching in the source how, how might you use the filter function to (refer to 2e when filtering the participants's `Role` as "Teacher") find return the most common words for Recorded Webinars?
3.  How many total resources were actually evaluated and which resource received the most feedback? How do you know?
4.  What are some obvious limitations to tf-idf, at least for this dataset, based on the initial `tfidf_df` data frame we created?
5.  Calculate word counts, frequencies, and tf-idf for your `opd_benefits` data frame. For frequencies and tf-idf, group by Role instead of Resource.
6.  What differences, if any, do you notice between teachers and other roles?

### 3b. Word Search

A quick word count actually resulted in findings fairly consistent with some of the qualitative findings reported, but also lacked some nuance, unsurprisingly, and left some questions about what some of the more frequent words were in reference to.

Let's use our reduced opd_teacher survey data frame that contains the complete teacher responses and use the handy string filter function to select just our `text` column and filter out responses that contain key words of interest. For example, what aspects of "online" made it beneficial.

```{python}
# Filter rows containing the word 'online' in the 'text' column
opd_quotes = opd_teacher[opd_teacher['text'].str.contains('online')]

# Display the opd_quotes DataFrame
print(opd_quotes)
```

We can view all quotes in the source pane, or use the sample function to select any number of random quotes. In this case 20:

```{python}
# Randomly sample 20 rows from the DataFrame
sampled_opd_quotes = opd_quotes.sample(n=20, random_state=1)

# Display the sampled DataFrame
print(sampled_opd_quotes)
```

In some cases, we can see that the use of the word "online" was simply repetition of the question prompt, but in other cases we can see that it's associated with the broader theme of "convenience" as with the quote, "This online resources gave me the opportunity to study on my own time."

Note that you can also use regular express operators with `grep` like the `*` operator to search for word stems. For example using `inform*` in our search will return quotes with "inform", "informative", "information", etc.

```{python}
# Select 'text' column and filter rows containing 'inform*'
opd_quotes = opd_teacher[opd_teacher['text'].str.contains(r'\binform\w*', case=False, regex=True)][['text']]

# Randomly sample 20 rows from the filtered DataFrame
sampled_opd_quotes = opd_quotes.sample(n=20, random_state=1)

# Display the sampled DataFrame
print(sampled_opd_quotes)
```

### 3c. Data Visualization

#### Word Clouds

The `WordCloud` library in conjunction with `matplotlib.pyplot` module is commonly used to generate word clouds due to its versatility and ease of use for displaying images and plots.

-   `WordCloud` : generating the word cloud image based on input data. However, it doesn't have built-in functionality for displaying the word cloud directly.

-   [`Matplotlib`](https://matplotlib.org): a comprehensive library for creating static, animated, and interactive visualizations in Python.

-   [`matplotlib.pyplot`](https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html) : a module of collection of functions that make `matplotlib` work like MATLAB. Each `pyplot` function makes some change to a figure: e.g., creates a figure, creates a plotting area in a figure, plots some lines in a plotting area, decorates the plot with labels, etc.

```{python}
!pip install wordcloud
!pip install matplotlib

import wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt
```

```{python}
# Generate a word cloud
wordcloud = WordCloud(width=800, height=500, background_color='white').generate_from_frequencies(dict(zip(words_count['word'], words_count['count'])))

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
```

#### Basic Bar Chart

The bar chart is the workhorse for data viz and is pretty effective for comparing two or more values. Given the unique aspect of our tidy text data frame, however, we are looking at upwards of over 5,000 values (i.e. words and their counts) to compare with our `opd_counts` data frame and will need some way to limit the number of words to display.

```{python}
# Filter rows with word counts greater than 500
opd_filtered = words_count[words_count['count'] > 500]

# Sort the DataFrame by 'count' column
opd_sorted = opd_filtered.sort_values(by='count', ascending=False)

# Create a bar plot and Display
plt.figure(figsize=(10, 6))
plt.barh(opd_sorted['word'], opd_sorted['count'], color='skyblue')
plt.xlabel('Count')
plt.ylabel('Word')
plt.title('Word Counts')
plt.gca().invert_yaxis()  # Invert y-axis to display words in descending order
plt.show()
```

#### Small Multiples

Word clouds and bar charts are pretty effective for highlighting the most common words in an entire corpus, or in our case, all teacher survey responses, regardless of resource type being reviewed.

One limitation we ran into earlier when we started looking at word frequencies and tf-idf stats was that it was difficult to easily compare the most common or unique words for each resource type. That is where small multiples come. A small multiple is basically a series of similar graphs or charts using the same scale and axes that make it easier to compare across different document collections of interest, in our case, word counts by resource type.

Let's create a small multiple by using `seaborn` library for our `resource_word_counts` data set instead of the `opd_tf_idf`.

-   [`Seaborn`](https://seaborn.pydata.org): a data visualization library build on built upon `matplotlib` and integrates closely with `pandas` data structures. It includes a variety of plotting functions for visualizing univariate and bivariate data, as well as specialized plots for categorical data, timeseries data, and more. It also provides functions for customizing the appearance of plots, such as controlling colors, styles, and axes labels.

```{python}
import seaborn as sns

# Remove rows with Resource equal to "Calendar"
opd_frequencies_filtered = resource_word_counts[resource_word_counts['Resource'] != 'Calendar']

# Group by Resource and select the top 5 words by proportion
top5_words_per_resource = opd_frequencies_filtered.groupby('Resource').apply(lambda x: x.nlargest(5, 'proportion')).reset_index(drop=True)

# Create a catplot - combination of a categorical plot ('word' is a categorical variable) and a FacetGrid ('Resource' is facet)) and map a bar plot to each facet
plot = sns.catplot(data=top5_words_per_resource, kind='bar', x='proportion', y = 'word', col='Resource', col_wrap=3)

# Customizing the titles and set font size
plot.set_titles(col_template="{col_name}", size=10) 

# Display the plot
plt.show()
```

#### Comprehension Check

1.  Create a word cloud, bar chart, and/or small multiple using your `opd_benefits` data.

## 4. MODEL

As highlighted in [Chapter 3 of Data Science in Education Using R](https://datascienceineducation.com/c03.html), the **Model** step of the data science process entails "using statistical models, from simple to complex, to understand trends and patterns in the data." The authors note that while descriptive statistics and data visualization during the **Explore** step can help us to identify patterns and relationships in our data, statistical models can be used to help us determine if relationships, patterns and trends are actually meaningful.

In [Learning Analytics Goes to School](https://catalog.lib.ncsu.edu/catalog/NCSU4862134), the authors describe modeling as simply developing a mathematical summary of a dataset and note that there are two general types to modeling: unsupervised and supervised learning. Unsupervised learning algorithms, which will be the focus in this lab, are used to explore the structure of a dataset, while supervised models "help to quantify relationships between features and a known outcome."

We will explore the use of models for text mining in future labs, but if you are interested in looking ahead to see how they might be applied to text as data, I recommend taking a look at [Chapter 6 Topic Modeling](https://www.tidytextmining.com/topicmodeling.html) from Text Mining with R: A Tidy Approach. Chris Bail in his Text as Data course also provides a nice introduction to Topic Modeling, including Structural Topic Modeling, which we will explore using the `stm` package in future labs.

Finally, if you have not already done so, I ask that at minimum you read Chapter 3 of DSIEUR as well as the section on the Data-Intensive Research Workflow from Chapter 2 of Learning Analytics Goes to school.

## 5. COMMUNICATE

The final(ish) step in our workflow/process is sharing the results of analysis with wider audience. Krumm et al. (2018) have outline the following 3-step process for communicating with education stakeholders what you have learned through analysis:

1.  **Select**. Communicating what one has learned involves selecting among those analyses that are most important and most useful to an intended audience, as well as selecting a form for displaying that information, such as a graph or table in static or interactive form, i.e. a "data product."
2.  **Polish**. After creating initial versions of data products, research teams often spend time refining or polishing them, by adding or editing titles, labels, and notations and by working with colors and shapes to highlight key points.
3.  **Narrate**. Writing a narrative to accompany the data products involves, at a minimum, pairing a data product with its related research question, describing how best to interpret the data product, and explaining the ways in which the data product helps answer the research question.

In this particular walkthrough, our target audience is developers of online professional learning opportunities who are looking to receive feedback on what's working well and potential areas for improvement. This lets us assume a good deal of prior knowledge on their end about the context of the evaluation, a high level of familiarly with the online professional development resources being assessed, and fairly literate at reading and interpreting data and charts. This also lets us simplify our data products and narrative and reduce the level of detail needed to communicate useful information.

For summative evaluation, typically at the end of a school year or grant period when the emphasis is on assessing program outcomes and impact, our audience would extend to those less familiar with the program but with a vested interest in program's success, such as the NC State Board of Education or those directly impacted by the program including NC educators is general. In that case, our data product would need to include much more narrative to provide context and greater detail in charts and graphs in order to help interpret the data presented.

### 5a. Select

#### **Analyses**

For analyses to present, I'm going to focus primarily on:

1.  **Word Counts**. One of the first things I'll share with my audience is my analysis of word counts since these are very easy to interpret and will provide online PD developers with some instant insight.
2.  **Word Search**. I also think it's important to include some actual participant responses for select key words in order to provide additional information about how those words are being used on context. I may handpick some select quotes or present a random selection of quotes.
3.  **TF-IDF**. Finally, as a way to potentially help them weed through words unique to specific resources, as well as potentially identify some areas for potential follow-up and to dig deeper, I'll discuss

I've decided to exclude analyses of just term frequency because I feel like simply counts are easier to quickly interpret while tf-idf provides more nuance. I also want to be careful not to overwhelm my audience.

#### **Data Products**

In terms of "data products" and form, and because this is a simple demonstration for sharing analyses and our first experience in this lab with independently analysis, I'll prepare my data product as a basic slide show that includes the following charts:

1.  **Word Cloud**. This is really as much decorative as it is a means to communicate key findings. I'll probably include this on my title slide as a way to immediately engage my audience and provide a preview of findings I'll share. It's also a way to include common words that may not make the cut in my bar chart.
2.  **Bar Chart**. I'll definitely share with them the bar chart I created earlier as a means to focus on the most common words used by teachers across all resources. Also, this is a little less busy than a word cloud and allows them to quickly see the top words in order.
3.  **Quotes**. As noted above, I also feel it's important to provide some participants responses that help explain how the common words are being used in context.
4.  **Small Multiples**. Although at this point a little skeptical about how meaningful the tf-idf analysis was due to issues such as spelling as well as the limited number of responses for many resources, I think the small multiples chart will help to spark discussion among PD developers about specific aspects of each resource type that teachers' value.

#### Sharing Format

We'll be using [Quarto Markdown](https://quarto.org/docs/get-started/hello/vscode.html#:~:text=qmd%20files%20contain%20a%20combination,you%20see%20on%20the%20right.) to create a slide deck or short report that documents our independent analysis. Quarto Markdown files can also be used to create a wide range of outputs and formats, including polished PDF or Word documents, websites, web apps, journal articles, online books, interactive tutorials and more.

### 5b. Polish

#### Word Cloud

To make the word cloud a little less busy and a little more useful, I removed the multitude of colors from the default setting, and I've defined a function that use the color black for words that occur more than 1000 times, and gray for the rest.

```{python}
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Filter word counts greater than 1000
words_count_above_1000 = set(words_count[words_count['count'] > 1000]['word'])

# Define custom coloring function
def color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    if word in words_count_above_1000:
        return 'black'  # Use black color for words occurring more than 1000 times
    else:
        return 'grey'  # Use gray color for the rest

# Generate a word cloud with custom coloring function
wordcloud = WordCloud(width=800, height=500, background_color='white',
color_func=color_func).generate_from_frequencies(dict(zip(words_count['word'], words_count['count'])))

# Display the word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
```

#### Bar Chart

For my bar chart, I did some minor clean up, including editing the x-axis title, removing the redundant y axis, and adding a title. If this were something for a more formal report, I'd probably finesse it even more, but it gets the point across.

```{python}
# Filter rows with word counts greater than 500
opd_filtered = words_count[words_count['count'] > 500]

# Sort the DataFrame by 'count' column
opd_sorted = opd_filtered.sort_values(by='count', ascending=False)

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.barh(opd_sorted['word'], opd_sorted['count'], color='skyblue')
plt.xlabel('Count')
plt.title('20 Most Frequently Used Words to Describe the Value of Online Resources')
plt.gca().invert_yaxis()  # Invert y-axis to display words in descending order
plt.show()
```

#### Small Multiples

Finally, one related issue that I want to clean up a little with respect to tf-idf before sharing with an outside audience is too few responses for the Calendar online learning resources. Therefore, I remove any response pertaining to Calendar. Again, if this were a chart destined for a more formal report, I'd also clean up the Resource names to make them more readable and fit properly on each bar plot.

```{python}
import seaborn as sns

# Remove rows with Resource equal to "Calendar"
opd_frequencies_filtered = resource_word_counts[resource_word_counts['Resource'] != 'Calendar']

# Group by Resource and select the top 5 words by proportion
top5_words_per_resource = opd_frequencies_filtered.groupby('Resource').apply(lambda x: x.nlargest(5, 'proportion')).reset_index(drop=True)

# Create a catplot - combination of a categorical plot ('word' is a categorical variable) and a FacetGrid ('Resource' is facet)) and map a bar plot to each facet
plot = sns.catplot(data=top5_words_per_resource, kind='bar', x='proportion', y = 'word', col='Resource', col_wrap=3)

# Customizing the titles and set font size
plot.set_titles(col_template="{col_name}", size=10) 

# Display the plot
plt.show()
```
